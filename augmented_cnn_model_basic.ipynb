{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24294f88",
   "metadata": {},
   "source": [
    "# header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7446c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:57:13.385815Z",
     "start_time": "2023-01-28T04:57:12.846479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfunction ConnectButton(){\\n    console.log(\"Connect pushed\"); \\n    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \\n}\\n\\nsetInterval(ConnectButton,60000);\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "\n",
    "setInterval(ConnectButton,60000);\n",
    "'''\n",
    "\n",
    "# from google.colab import drive\n",
    "# from os import chdir\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# project_path = '/content/drive/MyDrive/Gproject/MIT_glyco'\n",
    "# chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad22c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:57:21.512722Z",
     "start_time": "2023-01-28T04:57:13.387810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Gdrive\\내 드라이브\\Gproject\\MIT_glyco\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "print(os.getcwd()) # current working directory\n",
    "\n",
    "update = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658b6b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:57:25.596108Z",
     "start_time": "2023-01-28T04:57:21.513721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of proteins:           272\n",
      "total number of augmented proteins: 89\n",
      "total number of positive sites:     185\n"
     ]
    }
   ],
   "source": [
    "load_name = \"./data/data_for_ml.csv\"\n",
    "protein_list = list(pd.read_csv(load_name).protein.unique())\n",
    "protein_augmented = [name for name in protein_list if exists(f'./data/data_for_ml(augmented)/{name}.csv')]\n",
    "\n",
    "dataset = []\n",
    "for name in protein_augmented:\n",
    "    load_path = f'./data/data_for_ml(augmented)/{name}.csv'\n",
    "    dataset.append(pd.read_csv(load_path))\n",
    "dataset = pd.concat(dataset, axis=0).reset_index(drop=True)\n",
    "\n",
    "positive = dataset[dataset['positivity']==1]\n",
    "\n",
    "print(\"total number of proteins:          \", len(protein_list))\n",
    "print(\"total number of augmented proteins:\", len(protein_augmented))\n",
    "print(\"total number of positive sites:    \", len(positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b013c15",
   "metadata": {},
   "source": [
    "## parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f71134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:58:03.451557Z",
     "start_time": "2023-01-28T04:57:25.598076Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.Data import *\n",
    "from src.Preprocessing import *\n",
    "from src.models import *\n",
    "\n",
    "variables = xy_variables()\n",
    "test_size = 0.2\n",
    "valid_size = test_size/(1-test_size)\n",
    "\n",
    "import random\n",
    "random_state = 1\n",
    "n_cv = 10\n",
    "\n",
    "hpo_counts = 30\n",
    "hpo_config = {\n",
    "    \"n_layers\" : range(1,11),\n",
    "    \"n_neurons\" : [16, 32, 64, 128, 256]\n",
    "}\n",
    "metrics = ['epoch', 'time', 'loss', 'val_loss', 'test_loss', 'accuracy', 'precision', 'recall', 'f1']\n",
    "method = \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8eccc",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e84ab53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:58:03.591941Z",
     "start_time": "2023-01-28T04:58:03.452312Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 x_cts\n",
      "2 x_cat: ['SEQ', 'SS']\n",
      "1 y_label: ['positivity']\n",
      "dummy x shape: (59027, 191)\n",
      "dummy y shape: (59027, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'ASA',\n",
       " 1: 'Phi',\n",
       " 2: 'Psi',\n",
       " 3: 'Theta(i-1=>i+1)',\n",
       " 4: 'Tau(i-2=>i+2)',\n",
       " 5: 'HSE_alpha_up',\n",
       " 6: 'HSE_alpha_down',\n",
       " 7: 'P(C)',\n",
       " 8: 'P(H)',\n",
       " 9: 'P(E)',\n",
       " 10: 'flexibility',\n",
       " 11: '#',\n",
       " 12: 'residue_SER_THR',\n",
       " 13: 'number_of_hydrophobic',\n",
       " 14: 'number_of_hydrophilic',\n",
       " 15: 'number_of_polar',\n",
       " 16: 'number_of_aromatic',\n",
       " 17: 'number_of_aliphatic',\n",
       " 18: 'number_of_charged',\n",
       " 19: 'number_of_positive',\n",
       " 20: 'number_of_negative',\n",
       " 21: 'number_of_g',\n",
       " 22: 'number_of_v',\n",
       " 23: 'number_of_s',\n",
       " 24: 'number_of_n',\n",
       " 25: 'number_of_l',\n",
       " 26: 'number_of_p',\n",
       " 27: 'number_of_A',\n",
       " 28: 'number_of_b',\n",
       " 29: 'number_of_d',\n",
       " 30: 'number_of_e',\n",
       " 31: 'number_of_f',\n",
       " 32: 'number_of_ala',\n",
       " 33: 'number_of_cys',\n",
       " 34: 'number_of_asp',\n",
       " 35: 'number_of_glu',\n",
       " 36: 'number_of_phe',\n",
       " 37: 'number_of_his',\n",
       " 38: 'number_of_ile',\n",
       " 39: 'number_of_lys',\n",
       " 40: 'number_of_leu',\n",
       " 41: 'number_of_met',\n",
       " 42: 'number_of_asn',\n",
       " 43: 'number_of_gln',\n",
       " 44: 'number_of_arg',\n",
       " 45: 'number_of_ser',\n",
       " 46: 'number_of_thr',\n",
       " 47: 'number_of_val',\n",
       " 48: 'number_of_trp',\n",
       " 49: 'number_of_tyr',\n",
       " 50: 'sasa_hydrophobic',\n",
       " 51: 'sasa_hydrophilic',\n",
       " 52: 'sasa_polar',\n",
       " 53: 'sasa_aromatic',\n",
       " 54: 'sasa_aliphatic',\n",
       " 55: 'sasa_charged',\n",
       " 56: 'sasa_positive',\n",
       " 57: 'sasa_negative',\n",
       " 58: 'sasa_g',\n",
       " 59: 'sasa_v',\n",
       " 60: 'sasa_s',\n",
       " 61: 'sasa_n',\n",
       " 62: 'sasa_l',\n",
       " 63: 'sasa_p',\n",
       " 64: 'sasa_A',\n",
       " 65: 'sasa_b',\n",
       " 66: 'sasa_d',\n",
       " 67: 'sasa_e',\n",
       " 68: 'sasa_f',\n",
       " 69: 'sasa_ala',\n",
       " 70: 'sasa_cys',\n",
       " 71: 'sasa_asp',\n",
       " 72: 'sasa_glu',\n",
       " 73: 'sasa_phe',\n",
       " 74: 'sasa_his',\n",
       " 75: 'sasa_ile',\n",
       " 76: 'sasa_lys',\n",
       " 77: 'sasa_leu',\n",
       " 78: 'sasa_met',\n",
       " 79: 'sasa_asn',\n",
       " 80: 'sasa_gln',\n",
       " 81: 'sasa_arg',\n",
       " 82: 'sasa_ser',\n",
       " 83: 'sasa_thr',\n",
       " 84: 'sasa_val',\n",
       " 85: 'sasa_trp',\n",
       " 86: 'sasa_tyr',\n",
       " 87: 'all_sasa_hydrophobic',\n",
       " 88: 'all_sasa_hydrophilic',\n",
       " 89: 'all_sasa_polar',\n",
       " 90: 'all_sasa_aromatic',\n",
       " 91: 'all_sasa_aliphatic',\n",
       " 92: 'all_sasa_charged',\n",
       " 93: 'all_sasa_positive',\n",
       " 94: 'all_sasa_negative',\n",
       " 95: 'all_sasa_g',\n",
       " 96: 'all_sasa_v',\n",
       " 97: 'all_sasa_s',\n",
       " 98: 'all_sasa_n',\n",
       " 99: 'all_sasa_l',\n",
       " 100: 'all_sasa_p',\n",
       " 101: 'all_sasa_A',\n",
       " 102: 'all_sasa_b',\n",
       " 103: 'all_sasa_d',\n",
       " 104: 'all_sasa_e',\n",
       " 105: 'all_sasa_f',\n",
       " 106: 'all_sasa_ala',\n",
       " 107: 'all_sasa_cys',\n",
       " 108: 'all_sasa_asp',\n",
       " 109: 'all_sasa_glu',\n",
       " 110: 'all_sasa_phe',\n",
       " 111: 'all_sasa_his',\n",
       " 112: 'all_sasa_ile',\n",
       " 113: 'all_sasa_lys',\n",
       " 114: 'all_sasa_leu',\n",
       " 115: 'all_sasa_met',\n",
       " 116: 'all_sasa_asn',\n",
       " 117: 'all_sasa_gln',\n",
       " 118: 'all_sasa_arg',\n",
       " 119: 'all_sasa_ser',\n",
       " 120: 'all_sasa_thr',\n",
       " 121: 'all_sasa_val',\n",
       " 122: 'all_sasa_trp',\n",
       " 123: 'all_sasa_tyr',\n",
       " 124: 'sasa_back',\n",
       " 125: 'all_sasa_back',\n",
       " 126: 'sasa_side',\n",
       " 127: 'all_sasa_side',\n",
       " 128: 'all_sasa_back_with_whole_target',\n",
       " 129: 'all_sasa_side_with_whole_target',\n",
       " 130: 'sasa_target_ser_thr',\n",
       " 131: 'sasa_all_around_target_ser_thr',\n",
       " 132: 'sasa_all_with_around_target_ser_thr',\n",
       " 133: 'net_charge_all_around_target_ser_thr',\n",
       " 134: 'net_charge_all_with_around_target_ser_thr',\n",
       " 135: 'net_charge_all_backbone_around_target_ser_thr',\n",
       " 136: 'net_charge_all_sidechain_around_target_ser_thr',\n",
       " 137: 'net_charge_all_backbone_with_around_target_ser_thr',\n",
       " 138: 'net_charge_all_sidechain_with_around_target_ser_thr',\n",
       " 139: 'all_atom_positive_charge_all_around_target_ser_thr',\n",
       " 140: 'all_atom_positive_charge_all_with_around_target_ser_thr',\n",
       " 141: 'all_atom_positive_charge_all_backbone_around_target_ser_thr',\n",
       " 142: 'all_atom_positive_charge_all_sidechain_around_target_ser_thr',\n",
       " 143: 'all_atom_positive_charge_all_backbone_with_around_target_ser_thr',\n",
       " 144: 'all_atom_positive_charge_all_sidechain_with_around_target_ser_thr',\n",
       " 145: 'all_atom_negative_charge_all_around_target_ser_thr',\n",
       " 146: 'all_atom_negative_charge_all_with_around_target_ser_thr',\n",
       " 147: 'all_atom_negative_charge_all_backbone_around_target_ser_thr',\n",
       " 148: 'all_atom_negative_charge_all_sidechain_around_target_ser_thr',\n",
       " 149: 'all_atom_negative_charge_all_backbone_with_around_target_ser_thr',\n",
       " 150: 'all_atom_negative_charge_all_sidechain_with_around_target_ser_thr',\n",
       " 151: 'exposed_charge_all_around_target_ser_thr',\n",
       " 152: 'exposed_charge_all_with_around_target_ser_thr',\n",
       " 153: 'exposed_charge_all_backbone_around_target_ser_thr',\n",
       " 154: 'exposed_charge_all_sidechain_around_target_ser_thr',\n",
       " 155: 'exposed_charge_all_backbone_with_around_target_ser_thr',\n",
       " 156: 'exposed_charge_all_sidechain_with_around_target_ser_thr',\n",
       " 157: 'exposed_positive_charge_all_around_target_ser_thr',\n",
       " 158: 'exposed_positive_charge_all_with_around_target_ser_thr',\n",
       " 159: 'exposed_positive_charge_all_backbone_around_target_ser_thr',\n",
       " 160: 'exposed_positive_charge_all_sidechain_around_target_ser_thr',\n",
       " 161: 'exposed_positive_charge_all_backbone_with_around_target_ser_thr',\n",
       " 162: 'exposed_positive_charge_all_sidechain_with_around_target_ser_thr',\n",
       " 163: 'exposed_negative_charge_all_around_target_ser_thr',\n",
       " 164: 'exposed_negative_charge_all_with_around_target_ser_thr',\n",
       " 165: 'exposed_negative_charge_all_backbone_around_target_ser_thr',\n",
       " 166: 'exposed_negative_charge_all_sidechain_around_target_ser_thr',\n",
       " 167: 'exposed_negative_charge_all_backbone_with_around_target_ser_thr',\n",
       " 168: 'SEQ_A',\n",
       " 169: 'SEQ_C',\n",
       " 170: 'SEQ_D',\n",
       " 171: 'SEQ_E',\n",
       " 172: 'SEQ_F',\n",
       " 173: 'SEQ_G',\n",
       " 174: 'SEQ_H',\n",
       " 175: 'SEQ_I',\n",
       " 176: 'SEQ_K',\n",
       " 177: 'SEQ_L',\n",
       " 178: 'SEQ_M',\n",
       " 179: 'SEQ_N',\n",
       " 180: 'SEQ_P',\n",
       " 181: 'SEQ_Q',\n",
       " 182: 'SEQ_R',\n",
       " 183: 'SEQ_S',\n",
       " 184: 'SEQ_T',\n",
       " 185: 'SEQ_V',\n",
       " 186: 'SEQ_W',\n",
       " 187: 'SEQ_Y',\n",
       " 188: 'SS_C',\n",
       " 189: 'SS_E',\n",
       " 190: 'SS_H'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'augmented_cnn_basic'\n",
    "x_charge = pd.read_csv('./data/from_Krishna/features-all-names.csv').name.to_list()[:-1]\n",
    "\n",
    "x_cts   = variables.x_cts_original + x_charge\n",
    "x_cat   = variables.x_cat_original\n",
    "y_label = variables.y_label\n",
    "\n",
    "print(f\"{len(x_cts)} x_cts\")\n",
    "print(f\"{len(x_cat)} x_cat: {x_cat}\")\n",
    "print(f\"{len(y_label)} y_label: {y_label}\")\n",
    "\n",
    "data_x, data_y = df_to_dummy(dataset, x_cts, x_cat, y_label)\n",
    "display(dict(zip(range(len(data_x.columns)), data_x.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef6095ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T04:58:09.270265Z",
     "start_time": "2023-01-28T04:58:03.593937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn input shape : (8910, 21, 191)\n",
      "rnn output shape: (8910, 1)\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "\n",
    "seq_input  = []\n",
    "seq_output = []\n",
    "\n",
    "for name in protein_augmented:\n",
    "    load_path = f'./data/data_for_ml(augmented)/{name}.csv'\n",
    "    temp = pd.read_csv(load_path)\n",
    "    temp_x, temp_y = custom_dummy(temp, x_cts, x_cat, y_label)\n",
    "    \n",
    "    temp_input, temp_output = data_to_sequence(temp_x, temp_y, window_size)\n",
    "    seq_input.append(temp_input)\n",
    "    seq_output.append(temp_output)\n",
    "    \n",
    "seq_input  = np.concatenate(seq_input, axis=0)\n",
    "seq_output = np.concatenate(seq_output, axis=0)\n",
    "\n",
    "print(f'rnn input shape : {seq_input.shape}')\n",
    "print(f'rnn output shape: {seq_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2dc24a",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cca083e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T05:24:44.628700Z",
     "start_time": "2023-01-28T05:06:03.039116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: 0.0 1.1\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been restored from ./h5/augmented_cnn_basic_cv_1of10.h5\n",
      "\n",
      "2th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: -0.08089430894308944 1.1\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_2of10.h5\n",
      "\n",
      "3th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: 0.0 1.2375476320815681\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_3of10.h5\n",
      "\n",
      "4th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: -0.015855039920292593 1.2375476320815681\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_4of10.h5\n",
      "\n",
      "5th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: 0.0 1.1340206178027525\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_5of10.h5\n",
      "\n",
      "6th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: 0.0 1.08424866676003\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_6of10.h5\n",
      "\n",
      "7th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: 0.0 1.2375476320815681\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_7of10.h5\n",
      "\n",
      "8th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: -0.04339519453794438 1.2375476320815681\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_8of10.h5\n",
      "\n",
      "9th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: -0.04339519453794438 1.2\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_9of10.h5\n",
      "\n",
      "10th iteration\n",
      "train/test dataset: <class 'numpy.ndarray'>\n",
      "\n",
      "train: (7128, 21, 191) (7128, 1)\n",
      "check scale: 0.0 1.0\n",
      "\n",
      "test: (1782, 21, 191) (1782, 1)\n",
      "check scale: -0.012412930880514383 1.09064807219032\n",
      "up-sampled train dataset: (13960, 21, 191) (13960, 1)\n",
      "model has been saved to ./h5/augmented_cnn_basic_cv_10of10.h5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>time</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>0.530656</td>\n",
       "      <td>97.42</td>\n",
       "      <td>23.53</td>\n",
       "      <td>10.81</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1.539</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.400710</td>\n",
       "      <td>96.91</td>\n",
       "      <td>17.86</td>\n",
       "      <td>13.51</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>0.385016</td>\n",
       "      <td>97.03</td>\n",
       "      <td>16.67</td>\n",
       "      <td>10.81</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>1.891</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.342904</td>\n",
       "      <td>97.14</td>\n",
       "      <td>25.00</td>\n",
       "      <td>18.92</td>\n",
       "      <td>21.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>1.903</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.017653</td>\n",
       "      <td>0.391630</td>\n",
       "      <td>97.25</td>\n",
       "      <td>28.57</td>\n",
       "      <td>21.62</td>\n",
       "      <td>24.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>2.072</td>\n",
       "      <td>0.007991</td>\n",
       "      <td>0.020849</td>\n",
       "      <td>0.433319</td>\n",
       "      <td>97.31</td>\n",
       "      <td>26.09</td>\n",
       "      <td>16.22</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>1.719</td>\n",
       "      <td>0.023107</td>\n",
       "      <td>0.032719</td>\n",
       "      <td>0.311069</td>\n",
       "      <td>96.91</td>\n",
       "      <td>17.86</td>\n",
       "      <td>13.51</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>1.992</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.428208</td>\n",
       "      <td>97.59</td>\n",
       "      <td>25.00</td>\n",
       "      <td>8.11</td>\n",
       "      <td>12.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40</td>\n",
       "      <td>2.714</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.476106</td>\n",
       "      <td>97.87</td>\n",
       "      <td>47.06</td>\n",
       "      <td>21.62</td>\n",
       "      <td>29.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>3.255</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.415293</td>\n",
       "      <td>97.08</td>\n",
       "      <td>17.39</td>\n",
       "      <td>10.81</td>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch   time      loss  val_loss  test_loss  accuracy  precision  recall  \\\n",
       "0    21  1.968  0.000403  0.020245   0.530656     97.42      23.53   10.81   \n",
       "1     9  1.539  0.009857  0.028226   0.400710     96.91      17.86   13.51   \n",
       "2     5  1.308  0.008609  0.035927   0.385016     97.03      16.67   10.81   \n",
       "3    22  1.891  0.017765  0.011632   0.342904     97.14      25.00   18.92   \n",
       "4    21  1.903  0.009809  0.017653   0.391630     97.25      28.57   21.62   \n",
       "5    27  2.072  0.007991  0.020849   0.433319     97.31      26.09   16.22   \n",
       "6    16  1.719  0.023107  0.032719   0.311069     96.91      17.86   13.51   \n",
       "7    22  1.992  0.017414  0.024198   0.428208     97.59      25.00    8.11   \n",
       "8    40  2.714  0.000926  0.015975   0.476106     97.87      47.06   21.62   \n",
       "9    50  3.255  0.001629  0.018109   0.415293     97.08      17.39   10.81   \n",
       "\n",
       "      f1  \n",
       "0  14.81  \n",
       "1  15.38  \n",
       "2  13.11  \n",
       "3  21.54  \n",
       "4  24.62  \n",
       "5  20.00  \n",
       "6  15.38  \n",
       "7  12.24  \n",
       "8  29.63  \n",
       "9  13.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select n_layers and n_neurons as the best values of HPO\n",
    "cv_path = f'./result/cv_result_{model_name}_{n_cv}.csv'\n",
    "\n",
    "cv_result = pd.DataFrame([], columns=metrics)\n",
    "if not exists(cv_path) or update:\n",
    "    for i in range(n_cv):\n",
    "        print(f\"\\n{i+1}th iteration\")\n",
    "        random.seed(i+1)\n",
    "        train_x, train_y, test_x, test_y, _, _ = stratified_split(seq_input, seq_output, \n",
    "                                                              test_size=test_size, random_state=i+1, \n",
    "                                                              scale_x=x_cts, scale_y=[])\n",
    "        train_x, train_y = up_sampling(train_x, train_y)\n",
    "\n",
    "        history_size = train_x.shape[1]\n",
    "        x_dim = train_x.shape[2]\n",
    "        y_dim = train_y.shape[1]\n",
    "        save_path  = f'./h5/{model_name}_cv_{i+1}of{n_cv}.h5'\n",
    "        \n",
    "        model = CNN1D(history_size, x_dim, y_dim)\n",
    "        model.build()\n",
    "        if not exists(save_path) or update:\n",
    "            model.train(train_x, train_y, valid_size, save_path=save_path)\n",
    "        else:\n",
    "            model.load_model(save_path)\n",
    "\n",
    "        epoch = model.epoch\n",
    "        time = model.time\n",
    "        loss = model.loss\n",
    "        val_loss = model.val_loss\n",
    "        test_loss, accuracy, precision, recall, f1 = model.evaluate(test_x, test_y)\n",
    "        cv = pd.DataFrame([[epoch, time, loss, val_loss, test_loss, accuracy, \n",
    "                             precision[1], recall[1], f1[1]]], columns=metrics)\n",
    "        cv_result = pd.concat([cv_result, cv], axis=0)\n",
    "    cv_result = cv_result.reset_index(drop=True)\n",
    "    cv_result.to_csv(cv_path, index=False)    \n",
    "    \n",
    "else:\n",
    "    cv_result = pd.read_csv(cv_path)\n",
    "\n",
    "display(cv_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec170e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-28T05:00:14.592139Z",
     "start_time": "2023-01-28T05:00:14.592139Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.graph_plot import *\n",
    "\n",
    "for i in range(n_cv):\n",
    "    print(f\"\\n{i+1}th iteration\")\n",
    "    random.seed(i+1)\n",
    "    train_x, train_y, test_x, test_y, _, _ = stratified_split(rnn_input, rnn_output, \n",
    "                                                          test_size=test_size, random_state=i+1, \n",
    "                                                          scale_x=x_cts, scale_y=[])\n",
    "    train_x, train_y = up_sampling(train_x, train_y)\n",
    "\n",
    "    history_size = train_x.shape[1]\n",
    "    x_dim = train_x.shape[2]\n",
    "    y_dim = train_y.shape[1]\n",
    "    save_path  = f'./h5/{model_name}_{rnn_layers}_{rnn_neurons}_{dnn_layers}_{dnn_neurons}_cv_{i+1}of{n_cv}.h5'\n",
    "\n",
    "    model = RNN(history_size, x_dim, y_dim)\n",
    "    model.build(rnn_layers, rnn_neurons, dnn_layers, dnn_neurons)\n",
    "    model.load_model(save_path)\n",
    "    \n",
    "    prediction = model.model.predict(test_x, verbose=0)\n",
    "    y_pred = prediction.round(0).astype(int)\n",
    "    y_real = test_y\n",
    "    \n",
    "    plot_confusion(y_real, y_pred, title=model_name+f\"_{i+1}\", label=[\"Positive\",\"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39c00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dual Attention",
   "language": "python",
   "name": "dualattn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
