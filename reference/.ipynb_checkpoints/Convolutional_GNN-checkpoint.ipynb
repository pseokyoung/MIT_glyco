{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc_fzIDFu27D"
   },
   "source": [
    "#  <center> Problem Set 4 <center>\n",
    "<center> Spring 2022 <center>\n",
    "<center> 3.C01/3.C51, 10.C01/10.C51, 20.C01/20.C51 <center>\n",
    "<center> Due: 10 pm ET on Thursday, Apr 7, 2021 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D86KaQiJu27R"
   },
   "source": [
    "## Part 1: Predicting molecular properties with Graph Convolutional Nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9egPqYGu27S"
   },
   "source": [
    "### 1.1 (5 points) Install and try out RDkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHwtPrTtf6nq"
   },
   "source": [
    "This is a hack to install RDKit, without needing to install conda (which might take minutes). If you have anaconda installed, you can install RDKit from anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaqogUbau27S",
    "outputId": "8e94b26c-1025-46a6-cd43-fb8fddbf3664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3773    0  3773    0     0  11827      0 --:--:-- --:--:-- --:--:-- 11790\n",
      "100 20.2M  100 20.2M    0     0  3900k      0  0:00:05  0:00:05 --:--:-- 4043k\n"
     ]
    }
   ],
   "source": [
    "# Note from Pedro: if you're running this code, it may be better to install RDKit via Anaconda\n",
    "    # instead of running this cell\n",
    "url = 'https://anaconda.org/rdkit/rdkit/2018.09.1.0/download/linux-64/rdkit-2018.09.1.0-py36h71b666b_1.tar.bz2'\n",
    "!curl -L $url | tar xj lib\n",
    "!mv lib/python3.6/site-packages/rdkit /usr/local/lib/python3.7/dist-packages/\n",
    "\n",
    "x86 = '/usr/lib/x86_64-linux-gnu'\n",
    "!mv lib/*.so.* $x86/\n",
    "!ln -s $x86/libboost_python3-py36.so.1.65.1 $x86/libboost_python3.so.1.65.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kh0SNwnEu27T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from rdkit import RDLogger   \n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vuVsOhYu27U"
   },
   "source": [
    "### 1.2 (10 points) Construct Molecular Graph Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5M3-Cg9u27V",
    "outputId": "fa981f02-2534-4a4c-f30f-51e6d5c3e8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-05 19:17:54--  https://raw.githubusercontent.com/wwang2/ML4MolEng/master/psets/ps4/data/qm9.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30584525 (29M) [text/plain]\n",
      "Saving to: ‘qm9.csv’\n",
      "\n",
      "qm9.csv             100%[===================>]  29.17M  87.4MB/s    in 0.3s    \n",
      "\n",
      "2022-03-05 19:17:55 (87.4 MB/s) - ‘qm9.csv’ saved [30584525/30584525]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "! wget https://raw.githubusercontent.com/vikram-sundar/ML4MolEng_Spring2022/master/psets/ps4/data/qm9.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSP7MY1FhGu9"
   },
   "source": [
    "A SMILES to graph conversion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CohxTQgEhEz_"
   },
   "outputs": [],
   "source": [
    "def smiles2graph(smiles):\n",
    "    '''\n",
    "    Transform smiles into a list of atomic numbers and an edge array\n",
    "    \n",
    "    Args: \n",
    "        smiles (str): SMILES strings\n",
    "    \n",
    "    Returns: \n",
    "        z(np.array), A (np.array): list of atomic numbers, edge array\n",
    "    '''\n",
    "    \n",
    "    mol = Chem.MolFromSmiles( smiles ) # no hydrogen \n",
    "    z = np.array( [atom.GetAtomicNum() for atom in mol.GetAtoms()] )\n",
    "    A = np.stack(Chem.GetAdjacencyMatrix(mol)).nonzero()\n",
    "    \n",
    "    return z, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLm61aNBh5Ab"
   },
   "source": [
    "Read in the DataFrame, shuffle its rows, and store its properties as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYSiMqkhu27V"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv(\"qm9.csv\")\n",
    "df = shuffle(df).reset_index()\n",
    "\n",
    "AtomicNum_list = []\n",
    "Edge_list = []\n",
    "y_list = []\n",
    "Natom_list = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "        smiles = row.smiles\n",
    "        z, a = smiles2graph(smiles)\n",
    "\n",
    "        mol = Chem.MolFromSmiles( smiles ) \n",
    "        AtomicNum_list.append(torch.LongTensor(z))\n",
    "        Edge_list.append(torch.LongTensor(a))\n",
    "        y_list.append(torch.Tensor([row.alpha]))\n",
    "        Natom_list.append(len(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdJHeJnsitQv"
   },
   "source": [
    "A GraphDataset class for you to store graphs in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVSxuShzu27W"
   },
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 AtomicNum_list, \n",
    "                 Edge_list, \n",
    "                 Natom_list, \n",
    "                 y_list):\n",
    "        \n",
    "        '''\n",
    "        GraphDataset object\n",
    "        \n",
    "        Args: \n",
    "            z_list (list of torch.LongTensor)\n",
    "            a_list (list of torch.LongTensor)\n",
    "            N_list (list of int)\n",
    "            y_list (list of torch.FloatTensor)\n",
    "\n",
    "        '''\n",
    "        self.AtomicNum_list = AtomicNum_list # atomic number\n",
    "        self.Edge_list = Edge_list # edge list \n",
    "        self.Natom_list = Natom_list # Number of atoms \n",
    "        self.y_list = y_list # properties to predict \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Natom_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        AtomicNum = torch.LongTensor(self.AtomicNum_list[idx])\n",
    "        Edge = torch.LongTensor(self.Edge_list[idx])\n",
    "        Natom = self.Natom_list[idx]\n",
    "        y = torch.Tensor(self.y_list[idx])\n",
    "        \n",
    "        return AtomicNum, Edge, Natom, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjwS12ldi3SA"
   },
   "source": [
    "Split your dataset into train and test and define the GraphDataset class for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWDxXfulu27W"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "AtomicNum_train, AtomicNum_test, Edge_train, Edge_test, Natom_train, Natom_test, y_train, y_test = train_test_split(AtomicNum_list, \n",
    "                                                                                      Edge_list, \n",
    "                                                                                      Natom_list, \n",
    "                                                                                      y_list, test_size=0.2)\n",
    "\n",
    "AtomicNum_train, AtomicNum_val, Edge_train, Edge_val, Natom_train, Natom_val, y_train, y_val = train_test_split(AtomicNum_train, \n",
    "                                                                                      Edge_train, \n",
    "                                                                                      Natom_train, \n",
    "                                                                                      y_train, test_size=0.1/0.8)\n",
    "\n",
    "train_dataset = GraphDataset(AtomicNum_train, \n",
    "                            Edge_train, \n",
    "                            Natom_train,\n",
    "                            y_train)\n",
    "val_dataset = GraphDataset(AtomicNum_val,\n",
    "                          Edge_val, \n",
    "                          Natom_val, \n",
    "                          y_val)\n",
    "test_dataset =  GraphDataset(AtomicNum_test, \n",
    "                            Edge_test, \n",
    "                            Natom_test, \n",
    "                            y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0PqG5HNjpYv"
   },
   "source": [
    "A graph collation function to batch multiple graphs into one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lPCLjWpu27X"
   },
   "outputs": [],
   "source": [
    "def collate_graphs(batch):\n",
    "    '''Batch multiple graphs into one batched graph\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        batch (tuple): tuples of AtomicNum, Edge, Natom and y obtained from GraphDataset.__getitem__() \n",
    "        \n",
    "    Return \n",
    "        (tuple): Batched AtomicNum, Edge, Natom, y\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    AtomicNum_batch = []\n",
    "    Edge_batch = []\n",
    "    Natom_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    cumulative_atoms = np.cumsum([0] + [b[2] for b in batch])[:-1]\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        z, a, N, y = batch[i]\n",
    "        index_shift = cumulative_atoms[i]\n",
    "        a = a + index_shift\n",
    "        AtomicNum_batch.append(z) \n",
    "        Edge_batch.append(a)\n",
    "        Natom_batch.append(N)\n",
    "        y_batch.append(y)\n",
    "        \n",
    "    AtomicNum_batch = torch.cat(AtomicNum_batch)\n",
    "    Edge_batch = torch.cat(Edge_batch, dim=1)\n",
    "    Natom_batch = Natom_batch\n",
    "    y_batch = torch.cat(y_batch)\n",
    "    \n",
    "    return AtomicNum_batch, Edge_batch, Natom_batch, y_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwFV5HWUjt6k"
   },
   "source": [
    "An example use of collate_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezsdn6QYxEOp",
    "outputId": "c39051f0-1e35-48e7-9c09-742040a2bd2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6, 6, 7, 6, 6, 8]), tensor([[0, 2, 2, 1, 3, 5, 5, 4],\n",
       "         [2, 0, 1, 2, 5, 3, 4, 5]]), [3, 3], tensor([74.1800, 64.3200]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define graph 1 \n",
    "AtomicNum1 = torch.LongTensor([6, 6, 7])\n",
    "Edge1 = torch.LongTensor([[0, 2, 2, 1], \n",
    "                       [2, 0, 1, 2]])\n",
    "Natom1 = 3\n",
    "y1 =  torch.Tensor([74.18])\n",
    "\n",
    "# Define graph 2 \n",
    "AtomicNum2 = torch.LongTensor([6, 6, 8])\n",
    "Edge2 = torch.LongTensor([[0, 2, 2, 1], \n",
    "                       [2, 0, 1, 2]])\n",
    "Natom2 = 3\n",
    "y2 = torch.Tensor([64.32])\n",
    "\n",
    "graph1 = (AtomicNum1, Edge1, Natom1, y1)\n",
    "graph2 = (AtomicNum2, Edge2, Natom2, y2)\n",
    "\n",
    "collate_graphs((graph1, graph2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNxzlrfMjx-G"
   },
   "source": [
    "Defining the train and test DataLoaders with the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvrFsZMnu27Y"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pND0eELru27Y"
   },
   "source": [
    "### 1.3 (15 points) Complete the definition of a GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_mdM9Lsm8h_"
   },
   "source": [
    "The scatter_add function for use in your node updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8zVb8idxEOq"
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "def scatter_add(src, index, dim_size, dim=-1, fill_value=0):\n",
    "    \n",
    "    '''\n",
    "    Sums all values from the src tensor into out at the indices specified in the index \n",
    "    tensor along a given axis dim. \n",
    "    '''\n",
    "    \n",
    "    index_size = list(repeat(1, src.dim()))\n",
    "    index_size[dim] = src.size(dim)\n",
    "    index = index.view(index_size).expand_as(src)\n",
    "    \n",
    "    dim = range(src.dim())[dim]\n",
    "    out_size = list(src.size())\n",
    "    out_size[dim] = dim_size\n",
    "\n",
    "    out = src.new_full(out_size, fill_value)\n",
    "\n",
    "    return out.scatter_add_(dim, index, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gcPQ4z9nqBf"
   },
   "source": [
    "Example usage of scatter_add()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VxMaZ_exEOq",
    "outputId": "887d6e8f-4f63-46f4-e9be-5f8b96a94c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1., 1000.,  110.,    0.])\n"
     ]
    }
   ],
   "source": [
    "# Say you have a graph with 4 nodes, and there are an edge list that describes their connectivities.\n",
    "\n",
    "Edge = torch.LongTensor([[0, 0, 1, 3], # index for i \n",
    "                         [1, 2, 2, 0]]) # index for j \n",
    "\n",
    "# It means that the 0th node is connected to 1st node and the 2nd node; the 1st node is connected to the 2nd node. \n",
    "# For now, let us assume the connections are directed, i.e. 0th node is connected the 1st node, but the 1st node is not connected to the 0th node. \n",
    "# We want pass connection messages from the nodes in the first row to the nodes in the second row in Edge.\n",
    "\n",
    "# And for each edge, we have an message we wanto broadcast from i to j.\n",
    "message_i2j = torch.Tensor([1000., 100., 10., 1.])\n",
    "\n",
    "# We can use scatter_add() function to aggregate these pairwise messages onto each node. \n",
    "\n",
    "node_message = scatter_add(src=message_i2j, # message array for all the directed edge \n",
    "            index=Edge[1], # index to all the jth node to which you want to pass your message \n",
    "            dim=0,         # feature dimension you want to sum over \n",
    "            dim_size=4     # there are 4 nodes \n",
    "            ) \n",
    "\n",
    "print(node_message)\n",
    "\n",
    "# Now you can look at your results, you can see the messages are assigned from message_i2j to all the jth nodes you specified\n",
    "\n",
    "# See the graphical representation here: \"https://github.com/vikram-sundar/ML4MolEng_Spring2022/blob/master/psets/ps4/scatter_add_demo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvTi2ZlrnjKL",
    "outputId": "2d49d186-afbb-435e-881f-07e06afdf2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1010e+03, 1.0100e+03, 1.1000e+02, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# If you want your graph to be undirected, i.e. the ith node is connected to the jth node and vice versa, you can perfrom the summation in both direction like this: \n",
    "node_message = scatter_add(src=message_i2j, index=Edge[1], dim=0, dim_size=4) +  scatter_add(src=message_i2j, index=Edge[0], dim=0, dim_size=4)\n",
    "\n",
    "print(node_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xjflAkwu27Y"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import ModuleDict\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    '''\n",
    "        A GNN model \n",
    "    '''\n",
    "    def __init__(self, n_convs=3, n_embed=64):\n",
    "        super(GNN, self).__init__()\n",
    "        \n",
    "        self.atom_embed = nn.Embedding(100, n_embed)\n",
    "        self.convolutions = nn.ModuleList(\n",
    "            [ \n",
    "                ModuleDict({\n",
    "                    'update_mlp': nn.Sequential(nn.Linear(n_embed, n_embed), \n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(n_embed, n_embed)),\n",
    "                    'message_mlp': nn.Sequential(nn.Linear(n_embed, n_embed), \n",
    "                                                 nn.ReLU(), \n",
    "                                                 nn.Linear(n_embed, n_embed)) \n",
    "                })\n",
    "                for _ in range(n_convs)\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "        self.readout = nn.Sequential(nn.Linear(n_embed, n_embed), nn.ReLU(), nn.Linear(n_embed, 1))\n",
    "        \n",
    "    def forward(self, AtomicNum, Edge, Natom):\n",
    "        \n",
    "        h = self.atom_embed(AtomicNum)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            msg = conv['message_mlp'](h[Edge[0]] * h[Edge[1]])\n",
    "            dh = conv['update_mlp'](scatter_add(msg, Edge[0], dim=0, dim_size=h.shape[0]))\n",
    "            h += dh \n",
    "        \n",
    "        # node wise output \n",
    "        node_out = self.readout(h)\n",
    "        # split nodes back to graphs \n",
    "        node_splits = torch.split(node_out, Natom)\n",
    "        output = torch.stack([i.sum() for i in node_splits])\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmypwqSlu27Z"
   },
   "source": [
    "### 1.4 (5 points) Verify that your GNN preserves permutational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sr6HL59ofJs"
   },
   "source": [
    "Run this cell as is to show that your GNN respects permutational invariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3F7-dv8su27Z",
    "outputId": "fdd17c3b-740e-40e1-f2e9-0078a7d841e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: -0.35286 for perumutation: (0, 1, 2, 3)\n",
      "model output: -0.35286 for perumutation: (0, 1, 3, 2)\n",
      "model output: -0.35286 for perumutation: (0, 2, 1, 3)\n",
      "model output: -0.35286 for perumutation: (0, 2, 3, 1)\n",
      "model output: -0.35286 for perumutation: (0, 3, 1, 2)\n",
      "model output: -0.35286 for perumutation: (0, 3, 2, 1)\n",
      "model output: -0.35286 for perumutation: (1, 0, 2, 3)\n",
      "model output: -0.35286 for perumutation: (1, 0, 3, 2)\n",
      "model output: -0.35286 for perumutation: (1, 2, 0, 3)\n",
      "model output: -0.35286 for perumutation: (1, 2, 3, 0)\n",
      "model output: -0.35286 for perumutation: (1, 3, 0, 2)\n",
      "model output: -0.35286 for perumutation: (1, 3, 2, 0)\n",
      "model output: -0.35286 for perumutation: (2, 0, 1, 3)\n",
      "model output: -0.35286 for perumutation: (2, 0, 3, 1)\n",
      "model output: -0.35286 for perumutation: (2, 1, 0, 3)\n",
      "model output: -0.35286 for perumutation: (2, 1, 3, 0)\n",
      "model output: -0.35286 for perumutation: (2, 3, 0, 1)\n",
      "model output: -0.35286 for perumutation: (2, 3, 1, 0)\n",
      "model output: -0.35286 for perumutation: (3, 0, 1, 2)\n",
      "model output: -0.35286 for perumutation: (3, 0, 2, 1)\n",
      "model output: -0.35286 for perumutation: (3, 1, 0, 2)\n",
      "model output: -0.35286 for perumutation: (3, 1, 2, 0)\n",
      "model output: -0.35286 for perumutation: (3, 2, 0, 1)\n",
      "model output: -0.35286 for perumutation: (3, 2, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "def permute_graph(z, a, perm):\n",
    "    '''\n",
    "        permute the order of nodes in a molecular graph \n",
    "        \n",
    "        Args: \n",
    "            z(np.array): atomic number array\n",
    "            a(np.array): edge index pairs \n",
    "            \n",
    "        Return: \n",
    "            (np.array, np.array): permuted atomic number, and edge list \n",
    "    '''\n",
    "    \n",
    "    z = np.array(z)\n",
    "    perm = np.array(perm)\n",
    "    assert len(perm) == len(z)\n",
    "    \n",
    "    z_perm = z[perm]\n",
    "    a_perm = np.zeros(a.shape).astype(int)\n",
    "    \n",
    "    for i, edge in enumerate(a):\n",
    "        for j in range(len(edge)):\n",
    "            a_perm[i, j] = np.where(perm==edge[j])[0]\n",
    "    return z_perm, a_perm\n",
    "\n",
    "# node input\n",
    "z_orig = np.array([6, 6, 8, 7])\n",
    "# edge input \n",
    "a_orig = np.array([[0, 0, 1, 2, 3, 0], [1, 2, 0, 0, 0, 3]] )\n",
    "\n",
    "permutation = itertools.permutations([0, 1 ,2, 3])\n",
    "device = 'cuda:0'\n",
    "model = GNN(n_convs=4, n_embed=128).to(device)\n",
    "model.eval()\n",
    "\n",
    "for perm in permutation:\n",
    "    z_perm, a_perm = permute_graph(z_orig, a_orig, perm)\n",
    "    \n",
    "    z = torch.LongTensor(z_perm).to(device)\n",
    "    a = torch.LongTensor(a_perm).to(device)\n",
    "    N = [z.shape[0]]\n",
    "\n",
    "    output = model(z, a, N).item()\n",
    "    \n",
    "    print(\"model output: {:.5f} for perumutation: {}\".format(output, perm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5KEswY8u27a"
   },
   "source": [
    "### 1.5  (10 points) Train and test your GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kJ5tFC7pDns"
   },
   "source": [
    "The optimizer and scheduler setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8DqTDDbu27a"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW1RSspwpMXo"
   },
   "source": [
    "A combined training/validation loop, with progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMgcJzzzu27a"
   },
   "outputs": [],
   "source": [
    "def loop(model, loader, epoch, evaluation=False):\n",
    "    \n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "        mode = \"eval\"\n",
    "    else:\n",
    "        model.train()\n",
    "        mode = 'train'\n",
    "    batch_losses = []\n",
    "    \n",
    "    # Define tqdm progress bar \n",
    "    tqdm_data = tqdm(loader, position=0, leave=True, desc='{} (epoch #{})'.format(mode, epoch))\n",
    "    \n",
    "    for data in tqdm_data:\n",
    "        \n",
    "        AtomicNumber, Edge, Natom, y = data \n",
    "        AtomicNumber = AtomicNumber.to(device)\n",
    "        Edge = Edge.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(AtomicNumber, Edge, Natom)\n",
    "        loss = (pred-y).pow(2).mean() # MSE loss\n",
    "        \n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        postfix = ['batch loss={:.3f}'.format(loss.item()) , \n",
    "                   'avg. loss={:.3f}'.format(np.array(batch_losses).mean())]\n",
    "        \n",
    "        tqdm_data.set_postfix_str(' '.join(postfix))\n",
    "    \n",
    "    return np.array(batch_losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8iGMx0FpWxw"
   },
   "source": [
    "Run this cell to train your model for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rjYSawwu27a",
    "outputId": "9e2427f9-1ebe-44ff-bdea-4202cacf06ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train (epoch #0): 100%|██████████| 184/184 [00:13<00:00, 13.74it/s, batch loss=6.454 avg. loss=359.354]\n",
      "eval (epoch #0): 100%|██████████| 27/27 [00:00<00:00, 33.25it/s, batch loss=9.911 avg. loss=7.897]\n",
      "train (epoch #1): 100%|██████████| 184/184 [00:13<00:00, 13.30it/s, batch loss=6.345 avg. loss=6.698]\n",
      "eval (epoch #1): 100%|██████████| 27/27 [00:00<00:00, 33.99it/s, batch loss=8.503 avg. loss=8.616]\n",
      "train (epoch #2): 100%|██████████| 184/184 [00:11<00:00, 16.52it/s, batch loss=3.688 avg. loss=5.735]\n",
      "eval (epoch #2): 100%|██████████| 27/27 [00:00<00:00, 35.42it/s, batch loss=5.503 avg. loss=6.312]\n",
      "train (epoch #3): 100%|██████████| 184/184 [00:16<00:00, 10.90it/s, batch loss=2.960 avg. loss=5.418]\n",
      "eval (epoch #3): 100%|██████████| 27/27 [00:02<00:00, 13.43it/s, batch loss=5.738 avg. loss=5.177]\n",
      "train (epoch #4): 100%|██████████| 184/184 [00:10<00:00, 17.04it/s, batch loss=2.615 avg. loss=5.326]\n",
      "eval (epoch #4): 100%|██████████| 27/27 [00:00<00:00, 33.91it/s, batch loss=5.124 avg. loss=6.428]\n",
      "train (epoch #5): 100%|██████████| 184/184 [00:11<00:00, 16.49it/s, batch loss=4.345 avg. loss=5.113]\n",
      "eval (epoch #5): 100%|██████████| 27/27 [00:00<00:00, 33.59it/s, batch loss=4.390 avg. loss=6.242]\n",
      "train (epoch #6): 100%|██████████| 184/184 [00:11<00:00, 16.55it/s, batch loss=5.192 avg. loss=4.938]\n",
      "eval (epoch #6): 100%|██████████| 27/27 [00:00<00:00, 33.95it/s, batch loss=4.156 avg. loss=4.352]\n",
      "train (epoch #7): 100%|██████████| 184/184 [00:10<00:00, 17.79it/s, batch loss=2.599 avg. loss=4.861]\n",
      "eval (epoch #7): 100%|██████████| 27/27 [00:00<00:00, 35.28it/s, batch loss=4.661 avg. loss=4.656]\n",
      "train (epoch #8): 100%|██████████| 184/184 [00:10<00:00, 16.99it/s, batch loss=6.142 avg. loss=4.913]\n",
      "eval (epoch #8): 100%|██████████| 27/27 [00:00<00:00, 36.26it/s, batch loss=8.063 avg. loss=8.910]\n",
      "train (epoch #9): 100%|██████████| 184/184 [00:10<00:00, 17.72it/s, batch loss=4.338 avg. loss=4.971]\n",
      "eval (epoch #9): 100%|██████████| 27/27 [00:00<00:00, 35.08it/s, batch loss=4.077 avg. loss=4.243]\n",
      "train (epoch #10): 100%|██████████| 184/184 [00:10<00:00, 17.50it/s, batch loss=2.874 avg. loss=4.745]\n",
      "eval (epoch #10): 100%|██████████| 27/27 [00:00<00:00, 36.21it/s, batch loss=4.029 avg. loss=4.156]\n",
      "train (epoch #11): 100%|██████████| 184/184 [00:10<00:00, 17.47it/s, batch loss=4.069 avg. loss=4.758]\n",
      "eval (epoch #11): 100%|██████████| 27/27 [00:00<00:00, 32.81it/s, batch loss=5.537 avg. loss=5.012]\n",
      "train (epoch #12): 100%|██████████| 184/184 [00:11<00:00, 15.77it/s, batch loss=3.422 avg. loss=4.689]\n",
      "eval (epoch #12): 100%|██████████| 27/27 [00:00<00:00, 34.50it/s, batch loss=2.900 avg. loss=4.287]\n",
      "train (epoch #13): 100%|██████████| 184/184 [00:10<00:00, 16.84it/s, batch loss=2.973 avg. loss=4.709]\n",
      "eval (epoch #13): 100%|██████████| 27/27 [00:00<00:00, 32.27it/s, batch loss=6.858 avg. loss=4.924]\n",
      "train (epoch #14): 100%|██████████| 184/184 [00:10<00:00, 17.05it/s, batch loss=8.460 avg. loss=4.816]\n",
      "eval (epoch #14): 100%|██████████| 27/27 [00:00<00:00, 33.24it/s, batch loss=2.711 avg. loss=4.020]\n",
      "train (epoch #15): 100%|██████████| 184/184 [00:10<00:00, 17.66it/s, batch loss=5.659 avg. loss=4.666]\n",
      "eval (epoch #15): 100%|██████████| 27/27 [00:01<00:00, 25.73it/s, batch loss=7.249 avg. loss=7.477]\n",
      "train (epoch #16): 100%|██████████| 184/184 [00:10<00:00, 17.39it/s, batch loss=4.955 avg. loss=4.694]\n",
      "eval (epoch #16): 100%|██████████| 27/27 [00:00<00:00, 33.52it/s, batch loss=4.583 avg. loss=4.198]\n",
      "train (epoch #17): 100%|██████████| 184/184 [00:10<00:00, 17.62it/s, batch loss=2.129 avg. loss=4.517]\n",
      "eval (epoch #17): 100%|██████████| 27/27 [00:00<00:00, 35.91it/s, batch loss=4.480 avg. loss=5.005]\n",
      "train (epoch #18): 100%|██████████| 184/184 [00:10<00:00, 17.60it/s, batch loss=3.815 avg. loss=4.519]\n",
      "eval (epoch #18): 100%|██████████| 27/27 [00:00<00:00, 34.57it/s, batch loss=5.738 avg. loss=5.789]\n",
      "train (epoch #19): 100%|██████████| 184/184 [00:10<00:00, 18.07it/s, batch loss=3.497 avg. loss=4.543]\n",
      "eval (epoch #19): 100%|██████████| 27/27 [00:00<00:00, 35.50it/s, batch loss=3.295 avg. loss=3.994]\n",
      "train (epoch #20): 100%|██████████| 184/184 [00:10<00:00, 18.27it/s, batch loss=5.703 avg. loss=4.651]\n",
      "eval (epoch #20): 100%|██████████| 27/27 [00:00<00:00, 34.99it/s, batch loss=5.326 avg. loss=5.060]\n",
      "train (epoch #21): 100%|██████████| 184/184 [00:11<00:00, 16.22it/s, batch loss=3.830 avg. loss=4.524]\n",
      "eval (epoch #21): 100%|██████████| 27/27 [00:00<00:00, 35.40it/s, batch loss=4.093 avg. loss=4.332]\n",
      "train (epoch #22): 100%|██████████| 184/184 [00:10<00:00, 17.25it/s, batch loss=2.855 avg. loss=4.491]\n",
      "eval (epoch #22): 100%|██████████| 27/27 [00:00<00:00, 32.13it/s, batch loss=4.307 avg. loss=4.167]\n",
      "train (epoch #23): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=2.337 avg. loss=4.541]\n",
      "eval (epoch #23): 100%|██████████| 27/27 [00:00<00:00, 32.68it/s, batch loss=7.029 avg. loss=7.126]\n",
      "train (epoch #24): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=4.507 avg. loss=4.653]\n",
      "eval (epoch #24): 100%|██████████| 27/27 [00:00<00:00, 32.42it/s, batch loss=3.637 avg. loss=3.940]\n",
      "train (epoch #25): 100%|██████████| 184/184 [00:11<00:00, 16.16it/s, batch loss=4.519 avg. loss=4.508]\n",
      "eval (epoch #25): 100%|██████████| 27/27 [00:00<00:00, 30.81it/s, batch loss=4.341 avg. loss=4.497]\n",
      "train (epoch #26): 100%|██████████| 184/184 [00:10<00:00, 17.54it/s, batch loss=1.786 avg. loss=4.509]\n",
      "eval (epoch #26): 100%|██████████| 27/27 [00:00<00:00, 34.69it/s, batch loss=11.248 avg. loss=6.030]\n",
      "train (epoch #27): 100%|██████████| 184/184 [00:10<00:00, 17.03it/s, batch loss=2.324 avg. loss=4.463]\n",
      "eval (epoch #27): 100%|██████████| 27/27 [00:00<00:00, 34.89it/s, batch loss=6.742 avg. loss=7.252]\n",
      "train (epoch #28): 100%|██████████| 184/184 [00:10<00:00, 17.82it/s, batch loss=4.664 avg. loss=4.511]\n",
      "eval (epoch #28): 100%|██████████| 27/27 [00:00<00:00, 35.04it/s, batch loss=4.378 avg. loss=3.949]\n",
      "train (epoch #29): 100%|██████████| 184/184 [00:10<00:00, 17.64it/s, batch loss=10.841 avg. loss=4.490]\n",
      "eval (epoch #29): 100%|██████████| 27/27 [00:00<00:00, 34.84it/s, batch loss=7.919 avg. loss=7.312]\n",
      "train (epoch #30): 100%|██████████| 184/184 [00:10<00:00, 17.56it/s, batch loss=4.461 avg. loss=4.426]\n",
      "eval (epoch #30): 100%|██████████| 27/27 [00:01<00:00, 26.10it/s, batch loss=2.984 avg. loss=3.839]\n",
      "train (epoch #31): 100%|██████████| 184/184 [00:10<00:00, 17.38it/s, batch loss=2.842 avg. loss=4.387]\n",
      "eval (epoch #31): 100%|██████████| 27/27 [00:00<00:00, 29.87it/s, batch loss=5.294 avg. loss=4.486]\n",
      "train (epoch #32): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=3.949 avg. loss=4.367]\n",
      "eval (epoch #32): 100%|██████████| 27/27 [00:00<00:00, 32.34it/s, batch loss=2.947 avg. loss=3.925]\n",
      "train (epoch #33): 100%|██████████| 184/184 [00:11<00:00, 16.49it/s, batch loss=4.547 avg. loss=4.693]\n",
      "eval (epoch #33): 100%|██████████| 27/27 [00:00<00:00, 33.07it/s, batch loss=4.575 avg. loss=4.256]\n",
      "train (epoch #34): 100%|██████████| 184/184 [00:11<00:00, 16.03it/s, batch loss=3.744 avg. loss=4.342]\n",
      "eval (epoch #34): 100%|██████████| 27/27 [00:00<00:00, 32.80it/s, batch loss=5.477 avg. loss=3.980]\n",
      "train (epoch #35): 100%|██████████| 184/184 [00:11<00:00, 16.71it/s, batch loss=3.760 avg. loss=4.393]\n",
      "eval (epoch #35): 100%|██████████| 27/27 [00:01<00:00, 25.05it/s, batch loss=5.110 avg. loss=4.271]\n",
      "train (epoch #36): 100%|██████████| 184/184 [00:10<00:00, 17.00it/s, batch loss=8.292 avg. loss=4.468]\n",
      "eval (epoch #36): 100%|██████████| 27/27 [00:00<00:00, 34.77it/s, batch loss=4.721 avg. loss=4.835]\n",
      "train (epoch #37): 100%|██████████| 184/184 [00:10<00:00, 16.78it/s, batch loss=2.812 avg. loss=4.545]\n",
      "eval (epoch #37): 100%|██████████| 27/27 [00:00<00:00, 33.17it/s, batch loss=2.698 avg. loss=3.841]\n",
      "train (epoch #38): 100%|██████████| 184/184 [00:11<00:00, 16.44it/s, batch loss=4.859 avg. loss=4.356]\n",
      "eval (epoch #38): 100%|██████████| 27/27 [00:00<00:00, 34.10it/s, batch loss=3.595 avg. loss=4.402]\n",
      "train (epoch #39): 100%|██████████| 184/184 [00:10<00:00, 17.33it/s, batch loss=2.350 avg. loss=4.384]\n",
      "eval (epoch #39): 100%|██████████| 27/27 [00:00<00:00, 33.65it/s, batch loss=6.649 avg. loss=5.689]\n",
      "train (epoch #40): 100%|██████████| 184/184 [00:10<00:00, 17.03it/s, batch loss=3.582 avg. loss=4.444]\n",
      "eval (epoch #40): 100%|██████████| 27/27 [00:00<00:00, 35.02it/s, batch loss=4.781 avg. loss=3.754]\n",
      "train (epoch #41): 100%|██████████| 184/184 [00:11<00:00, 16.64it/s, batch loss=5.819 avg. loss=4.202]\n",
      "eval (epoch #41): 100%|██████████| 27/27 [00:00<00:00, 33.59it/s, batch loss=3.764 avg. loss=4.043]\n",
      "train (epoch #42): 100%|██████████| 184/184 [00:10<00:00, 16.74it/s, batch loss=10.109 avg. loss=4.368]\n",
      "eval (epoch #42): 100%|██████████| 27/27 [00:00<00:00, 34.28it/s, batch loss=13.191 avg. loss=15.150]\n",
      "train (epoch #43): 100%|██████████| 184/184 [00:10<00:00, 17.09it/s, batch loss=12.972 avg. loss=778.117]\n",
      "eval (epoch #43): 100%|██████████| 27/27 [00:00<00:00, 34.45it/s, batch loss=13.170 avg. loss=12.399]\n",
      "train (epoch #44): 100%|██████████| 184/184 [00:11<00:00, 16.37it/s, batch loss=7.087 avg. loss=10.161]\n",
      "eval (epoch #44): 100%|██████████| 27/27 [00:00<00:00, 34.03it/s, batch loss=9.598 avg. loss=8.772]\n",
      "train (epoch #45): 100%|██████████| 184/184 [00:10<00:00, 17.27it/s, batch loss=8.649 avg. loss=8.173]\n",
      "eval (epoch #45): 100%|██████████| 27/27 [00:00<00:00, 33.04it/s, batch loss=7.473 avg. loss=8.944]\n",
      "train (epoch #46): 100%|██████████| 184/184 [00:10<00:00, 17.03it/s, batch loss=5.764 avg. loss=6.854]\n",
      "eval (epoch #46): 100%|██████████| 27/27 [00:00<00:00, 34.38it/s, batch loss=4.160 avg. loss=5.872]\n",
      "train (epoch #47): 100%|██████████| 184/184 [00:10<00:00, 17.40it/s, batch loss=4.889 avg. loss=6.187]\n",
      "eval (epoch #47): 100%|██████████| 27/27 [00:00<00:00, 35.77it/s, batch loss=4.623 avg. loss=5.327]\n",
      "train (epoch #48): 100%|██████████| 184/184 [00:10<00:00, 17.65it/s, batch loss=2.404 avg. loss=5.648]\n",
      "eval (epoch #48): 100%|██████████| 27/27 [00:00<00:00, 35.24it/s, batch loss=7.359 avg. loss=6.758]\n",
      "train (epoch #49): 100%|██████████| 184/184 [00:10<00:00, 17.83it/s, batch loss=2.908 avg. loss=5.492]\n",
      "eval (epoch #49): 100%|██████████| 27/27 [00:00<00:00, 34.84it/s, batch loss=4.119 avg. loss=5.177]\n",
      "train (epoch #50): 100%|██████████| 184/184 [00:10<00:00, 17.21it/s, batch loss=1.890 avg. loss=5.263]\n",
      "eval (epoch #50): 100%|██████████| 27/27 [00:01<00:00, 25.44it/s, batch loss=9.388 avg. loss=7.095]\n",
      "train (epoch #51): 100%|██████████| 184/184 [00:10<00:00, 16.86it/s, batch loss=5.421 avg. loss=5.021]\n",
      "eval (epoch #51): 100%|██████████| 27/27 [00:00<00:00, 33.21it/s, batch loss=3.144 avg. loss=4.641]\n",
      "train (epoch #52): 100%|██████████| 184/184 [00:10<00:00, 16.76it/s, batch loss=1.875 avg. loss=4.925]\n",
      "eval (epoch #52): 100%|██████████| 27/27 [00:00<00:00, 32.27it/s, batch loss=5.159 avg. loss=4.342]\n",
      "train (epoch #53): 100%|██████████| 184/184 [00:10<00:00, 17.27it/s, batch loss=3.209 avg. loss=4.939]\n",
      "eval (epoch #53): 100%|██████████| 27/27 [00:00<00:00, 32.81it/s, batch loss=7.187 avg. loss=5.723]\n",
      "train (epoch #54): 100%|██████████| 184/184 [00:11<00:00, 16.41it/s, batch loss=2.853 avg. loss=4.744]\n",
      "eval (epoch #54): 100%|██████████| 27/27 [00:00<00:00, 34.12it/s, batch loss=3.900 avg. loss=4.402]\n",
      "train (epoch #55): 100%|██████████| 184/184 [00:10<00:00, 16.74it/s, batch loss=7.152 avg. loss=4.778]\n",
      "eval (epoch #55): 100%|██████████| 27/27 [00:00<00:00, 34.70it/s, batch loss=4.903 avg. loss=4.408]\n",
      "train (epoch #56): 100%|██████████| 184/184 [00:10<00:00, 17.53it/s, batch loss=3.695 avg. loss=4.745]\n",
      "eval (epoch #56): 100%|██████████| 27/27 [00:00<00:00, 33.60it/s, batch loss=3.250 avg. loss=4.294]\n",
      "train (epoch #57): 100%|██████████| 184/184 [00:11<00:00, 16.43it/s, batch loss=8.164 avg. loss=4.755]\n",
      "eval (epoch #57): 100%|██████████| 27/27 [00:01<00:00, 15.58it/s, batch loss=7.077 avg. loss=7.947]\n",
      "train (epoch #58): 100%|██████████| 184/184 [00:13<00:00, 13.76it/s, batch loss=3.054 avg. loss=4.817]\n",
      "eval (epoch #58): 100%|██████████| 27/27 [00:00<00:00, 32.76it/s, batch loss=3.255 avg. loss=4.371]\n",
      "train (epoch #59): 100%|██████████| 184/184 [00:15<00:00, 11.73it/s, batch loss=5.807 avg. loss=4.770]\n",
      "eval (epoch #59): 100%|██████████| 27/27 [00:01<00:00, 17.69it/s, batch loss=3.742 avg. loss=4.271]\n",
      "train (epoch #60): 100%|██████████| 184/184 [00:11<00:00, 15.82it/s, batch loss=3.169 avg. loss=4.666]\n",
      "eval (epoch #60): 100%|██████████| 27/27 [00:00<00:00, 36.03it/s, batch loss=3.497 avg. loss=4.192]\n",
      "train (epoch #61): 100%|██████████| 184/184 [00:13<00:00, 13.98it/s, batch loss=2.664 avg. loss=4.723]\n",
      "eval (epoch #61): 100%|██████████| 27/27 [00:00<00:00, 34.23it/s, batch loss=10.787 avg. loss=5.732]\n",
      "train (epoch #62): 100%|██████████| 184/184 [00:10<00:00, 17.60it/s, batch loss=3.001 avg. loss=4.654]\n",
      "eval (epoch #62): 100%|██████████| 27/27 [00:00<00:00, 34.24it/s, batch loss=6.139 avg. loss=5.634]\n",
      "train (epoch #63): 100%|██████████| 184/184 [00:10<00:00, 17.62it/s, batch loss=3.544 avg. loss=4.579]\n",
      "eval (epoch #63): 100%|██████████| 27/27 [00:00<00:00, 34.60it/s, batch loss=3.164 avg. loss=4.039]\n",
      "train (epoch #64): 100%|██████████| 184/184 [00:10<00:00, 16.79it/s, batch loss=2.902 avg. loss=4.571]\n",
      "eval (epoch #64): 100%|██████████| 27/27 [00:00<00:00, 33.59it/s, batch loss=4.373 avg. loss=4.644]\n",
      "train (epoch #65): 100%|██████████| 184/184 [00:11<00:00, 16.48it/s, batch loss=2.947 avg. loss=4.702]\n",
      "eval (epoch #65): 100%|██████████| 27/27 [00:00<00:00, 33.28it/s, batch loss=12.175 avg. loss=4.358]\n",
      "train (epoch #66): 100%|██████████| 184/184 [00:11<00:00, 15.86it/s, batch loss=3.787 avg. loss=4.676]\n",
      "eval (epoch #66): 100%|██████████| 27/27 [00:00<00:00, 32.65it/s, batch loss=3.855 avg. loss=5.598]\n",
      "train (epoch #67): 100%|██████████| 184/184 [00:10<00:00, 16.90it/s, batch loss=2.502 avg. loss=4.579]\n",
      "eval (epoch #67): 100%|██████████| 27/27 [00:00<00:00, 32.49it/s, batch loss=2.139 avg. loss=4.039]\n",
      "train (epoch #68): 100%|██████████| 184/184 [00:11<00:00, 16.43it/s, batch loss=3.836 avg. loss=4.764]\n",
      "eval (epoch #68): 100%|██████████| 27/27 [00:00<00:00, 33.61it/s, batch loss=4.128 avg. loss=5.082]\n",
      "train (epoch #69): 100%|██████████| 184/184 [00:10<00:00, 17.00it/s, batch loss=14.004 avg. loss=4.682]\n",
      "eval (epoch #69): 100%|██████████| 27/27 [00:00<00:00, 34.47it/s, batch loss=3.674 avg. loss=4.521]\n",
      "train (epoch #70): 100%|██████████| 184/184 [00:10<00:00, 17.48it/s, batch loss=17.881 avg. loss=4.622]\n",
      "eval (epoch #70): 100%|██████████| 27/27 [00:00<00:00, 32.66it/s, batch loss=12.009 avg. loss=4.942]\n",
      "train (epoch #71): 100%|██████████| 184/184 [00:12<00:00, 15.02it/s, batch loss=1.668 avg. loss=4.680]\n",
      "eval (epoch #71): 100%|██████████| 27/27 [00:00<00:00, 33.50it/s, batch loss=2.855 avg. loss=3.973]\n",
      "train (epoch #72): 100%|██████████| 184/184 [00:10<00:00, 17.38it/s, batch loss=2.604 avg. loss=4.533]\n",
      "eval (epoch #72): 100%|██████████| 27/27 [00:00<00:00, 34.08it/s, batch loss=3.624 avg. loss=4.477]\n",
      "train (epoch #73): 100%|██████████| 184/184 [00:10<00:00, 17.69it/s, batch loss=2.029 avg. loss=4.667]\n",
      "eval (epoch #73): 100%|██████████| 27/27 [00:00<00:00, 33.58it/s, batch loss=4.884 avg. loss=5.208]\n",
      "train (epoch #74): 100%|██████████| 184/184 [00:10<00:00, 17.53it/s, batch loss=3.255 avg. loss=4.650]\n",
      "eval (epoch #74): 100%|██████████| 27/27 [00:01<00:00, 24.90it/s, batch loss=6.219 avg. loss=6.953]\n",
      "train (epoch #75): 100%|██████████| 184/184 [00:11<00:00, 16.69it/s, batch loss=5.314 avg. loss=4.561]\n",
      "eval (epoch #75): 100%|██████████| 27/27 [00:00<00:00, 34.11it/s, batch loss=6.331 avg. loss=6.928]\n",
      "train (epoch #76): 100%|██████████| 184/184 [00:10<00:00, 17.49it/s, batch loss=5.099 avg. loss=4.638]\n",
      "eval (epoch #76): 100%|██████████| 27/27 [00:01<00:00, 25.01it/s, batch loss=4.430 avg. loss=4.111]\n",
      "train (epoch #77): 100%|██████████| 184/184 [00:11<00:00, 16.68it/s, batch loss=3.189 avg. loss=4.511]\n",
      "eval (epoch #77): 100%|██████████| 27/27 [00:00<00:00, 32.45it/s, batch loss=2.757 avg. loss=3.912]\n",
      "train (epoch #78): 100%|██████████| 184/184 [00:11<00:00, 16.67it/s, batch loss=3.101 avg. loss=4.428]\n",
      "eval (epoch #78): 100%|██████████| 27/27 [00:00<00:00, 35.86it/s, batch loss=4.087 avg. loss=3.983]\n",
      "train (epoch #79): 100%|██████████| 184/184 [00:10<00:00, 17.50it/s, batch loss=3.025 avg. loss=4.529]\n",
      "eval (epoch #79): 100%|██████████| 27/27 [00:00<00:00, 34.42it/s, batch loss=8.865 avg. loss=4.835]\n",
      "train (epoch #80): 100%|██████████| 184/184 [00:10<00:00, 17.12it/s, batch loss=2.820 avg. loss=4.477]\n",
      "eval (epoch #80): 100%|██████████| 27/27 [00:00<00:00, 33.67it/s, batch loss=2.761 avg. loss=3.907]\n",
      "train (epoch #81): 100%|██████████| 184/184 [00:11<00:00, 16.56it/s, batch loss=2.939 avg. loss=4.407]\n",
      "eval (epoch #81): 100%|██████████| 27/27 [00:00<00:00, 32.92it/s, batch loss=5.043 avg. loss=4.021]\n",
      "train (epoch #82): 100%|██████████| 184/184 [00:10<00:00, 16.74it/s, batch loss=3.150 avg. loss=4.533]\n",
      "eval (epoch #82): 100%|██████████| 27/27 [00:00<00:00, 32.58it/s, batch loss=5.508 avg. loss=5.394]\n",
      "train (epoch #83): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=2.693 avg. loss=4.512]\n",
      "eval (epoch #83): 100%|██████████| 27/27 [00:00<00:00, 34.11it/s, batch loss=3.765 avg. loss=4.370]\n",
      "train (epoch #84): 100%|██████████| 184/184 [00:11<00:00, 16.73it/s, batch loss=8.934 avg. loss=4.503]\n",
      "eval (epoch #84): 100%|██████████| 27/27 [00:01<00:00, 24.28it/s, batch loss=2.615 avg. loss=4.131]\n",
      "train (epoch #85): 100%|██████████| 184/184 [00:10<00:00, 16.81it/s, batch loss=2.670 avg. loss=4.630]\n",
      "eval (epoch #85): 100%|██████████| 27/27 [00:00<00:00, 33.80it/s, batch loss=3.172 avg. loss=4.329]\n",
      "train (epoch #86): 100%|██████████| 184/184 [00:11<00:00, 16.55it/s, batch loss=1.590 avg. loss=4.400]\n",
      "eval (epoch #86): 100%|██████████| 27/27 [00:00<00:00, 33.88it/s, batch loss=3.059 avg. loss=3.998]\n",
      "train (epoch #87): 100%|██████████| 184/184 [00:11<00:00, 16.61it/s, batch loss=5.615 avg. loss=4.475]\n",
      "eval (epoch #87): 100%|██████████| 27/27 [00:00<00:00, 32.01it/s, batch loss=7.069 avg. loss=6.404]\n",
      "train (epoch #88): 100%|██████████| 184/184 [00:11<00:00, 16.48it/s, batch loss=4.088 avg. loss=4.614]\n",
      "eval (epoch #88): 100%|██████████| 27/27 [00:00<00:00, 34.48it/s, batch loss=4.683 avg. loss=5.952]\n",
      "train (epoch #89): 100%|██████████| 184/184 [00:10<00:00, 16.87it/s, batch loss=3.118 avg. loss=4.482]\n",
      "eval (epoch #89): 100%|██████████| 27/27 [00:00<00:00, 30.93it/s, batch loss=3.537 avg. loss=4.180]\n",
      "train (epoch #90): 100%|██████████| 184/184 [00:11<00:00, 16.20it/s, batch loss=2.969 avg. loss=4.348]\n",
      "eval (epoch #90): 100%|██████████| 27/27 [00:00<00:00, 32.29it/s, batch loss=3.333 avg. loss=4.059]\n",
      "train (epoch #91): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=5.332 avg. loss=4.301]\n",
      "eval (epoch #91): 100%|██████████| 27/27 [00:00<00:00, 32.73it/s, batch loss=4.983 avg. loss=4.573]\n",
      "train (epoch #92): 100%|██████████| 184/184 [00:11<00:00, 16.45it/s, batch loss=3.075 avg. loss=4.537]\n",
      "eval (epoch #92): 100%|██████████| 27/27 [00:00<00:00, 31.98it/s, batch loss=3.516 avg. loss=5.368]\n",
      "train (epoch #93): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=1.614 avg. loss=4.341]\n",
      "eval (epoch #93): 100%|██████████| 27/27 [00:00<00:00, 32.24it/s, batch loss=4.732 avg. loss=4.867]\n",
      "train (epoch #94): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=6.615 avg. loss=4.350]\n",
      "eval (epoch #94): 100%|██████████| 27/27 [00:00<00:00, 30.87it/s, batch loss=3.844 avg. loss=4.001]\n",
      "train (epoch #95): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=3.485 avg. loss=4.338]\n",
      "eval (epoch #95): 100%|██████████| 27/27 [00:00<00:00, 33.14it/s, batch loss=5.120 avg. loss=4.638]\n",
      "train (epoch #96): 100%|██████████| 184/184 [00:11<00:00, 16.72it/s, batch loss=3.095 avg. loss=4.356]\n",
      "eval (epoch #96): 100%|██████████| 27/27 [00:00<00:00, 32.80it/s, batch loss=3.644 avg. loss=3.904]\n",
      "train (epoch #97): 100%|██████████| 184/184 [00:11<00:00, 16.12it/s, batch loss=3.663 avg. loss=4.268]\n",
      "eval (epoch #97): 100%|██████████| 27/27 [00:00<00:00, 33.00it/s, batch loss=5.059 avg. loss=4.391]\n",
      "train (epoch #98): 100%|██████████| 184/184 [00:10<00:00, 16.88it/s, batch loss=1.738 avg. loss=4.208]\n",
      "eval (epoch #98): 100%|██████████| 27/27 [00:00<00:00, 32.61it/s, batch loss=4.647 avg. loss=4.509]\n",
      "train (epoch #99): 100%|██████████| 184/184 [00:11<00:00, 16.68it/s, batch loss=5.188 avg. loss=4.346]\n",
      "eval (epoch #99): 100%|██████████| 27/27 [00:00<00:00, 33.02it/s, batch loss=6.070 avg. loss=4.158]\n",
      "train (epoch #100): 100%|██████████| 184/184 [00:11<00:00, 16.16it/s, batch loss=2.278 avg. loss=4.262]\n",
      "eval (epoch #100): 100%|██████████| 27/27 [00:00<00:00, 33.31it/s, batch loss=4.041 avg. loss=4.408]\n",
      "train (epoch #101): 100%|██████████| 184/184 [00:10<00:00, 16.76it/s, batch loss=4.510 avg. loss=4.344]\n",
      "eval (epoch #101): 100%|██████████| 27/27 [00:00<00:00, 33.91it/s, batch loss=3.584 avg. loss=4.050]\n",
      "train (epoch #102): 100%|██████████| 184/184 [00:11<00:00, 15.75it/s, batch loss=4.463 avg. loss=4.258]\n",
      "eval (epoch #102): 100%|██████████| 27/27 [00:00<00:00, 31.01it/s, batch loss=3.365 avg. loss=3.680]\n",
      "train (epoch #103): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=2.501 avg. loss=4.307]\n",
      "eval (epoch #103): 100%|██████████| 27/27 [00:00<00:00, 31.50it/s, batch loss=3.262 avg. loss=3.969]\n",
      "train (epoch #104): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=3.889 avg. loss=4.172]\n",
      "eval (epoch #104): 100%|██████████| 27/27 [00:00<00:00, 34.03it/s, batch loss=5.719 avg. loss=6.163]\n",
      "train (epoch #105): 100%|██████████| 184/184 [00:10<00:00, 17.17it/s, batch loss=2.621 avg. loss=4.267]\n",
      "eval (epoch #105): 100%|██████████| 27/27 [00:00<00:00, 31.71it/s, batch loss=4.634 avg. loss=4.756]\n",
      "train (epoch #106): 100%|██████████| 184/184 [00:11<00:00, 16.48it/s, batch loss=7.068 avg. loss=4.198]\n",
      "eval (epoch #106): 100%|██████████| 27/27 [00:00<00:00, 32.21it/s, batch loss=3.342 avg. loss=4.941]\n",
      "train (epoch #107): 100%|██████████| 184/184 [00:11<00:00, 15.89it/s, batch loss=6.067 avg. loss=4.314]\n",
      "eval (epoch #107): 100%|██████████| 27/27 [00:00<00:00, 31.72it/s, batch loss=3.338 avg. loss=3.750]\n",
      "train (epoch #108): 100%|██████████| 184/184 [00:11<00:00, 16.45it/s, batch loss=3.277 avg. loss=4.066]\n",
      "eval (epoch #108): 100%|██████████| 27/27 [00:00<00:00, 32.04it/s, batch loss=3.630 avg. loss=4.684]\n",
      "train (epoch #109): 100%|██████████| 184/184 [00:11<00:00, 16.25it/s, batch loss=5.191 avg. loss=4.257]\n",
      "eval (epoch #109): 100%|██████████| 27/27 [00:00<00:00, 31.23it/s, batch loss=3.004 avg. loss=3.960]\n",
      "train (epoch #110): 100%|██████████| 184/184 [00:11<00:00, 16.60it/s, batch loss=2.962 avg. loss=4.100]\n",
      "eval (epoch #110): 100%|██████████| 27/27 [00:00<00:00, 31.78it/s, batch loss=9.247 avg. loss=5.387]\n",
      "train (epoch #111): 100%|██████████| 184/184 [00:11<00:00, 16.26it/s, batch loss=2.445 avg. loss=4.133]\n",
      "eval (epoch #111): 100%|██████████| 27/27 [00:00<00:00, 31.72it/s, batch loss=2.952 avg. loss=3.835]\n",
      "train (epoch #112): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=8.721 avg. loss=4.145]\n",
      "eval (epoch #112): 100%|██████████| 27/27 [00:00<00:00, 33.11it/s, batch loss=4.729 avg. loss=3.911]\n",
      "train (epoch #113): 100%|██████████| 184/184 [00:11<00:00, 16.66it/s, batch loss=3.711 avg. loss=4.146]\n",
      "eval (epoch #113): 100%|██████████| 27/27 [00:00<00:00, 32.95it/s, batch loss=2.767 avg. loss=4.177]\n",
      "train (epoch #114): 100%|██████████| 184/184 [00:10<00:00, 16.84it/s, batch loss=7.322 avg. loss=4.400]\n",
      "eval (epoch #114): 100%|██████████| 27/27 [00:01<00:00, 24.79it/s, batch loss=10.689 avg. loss=10.626]\n",
      "train (epoch #115): 100%|██████████| 184/184 [00:11<00:00, 16.50it/s, batch loss=5.461 avg. loss=4.228]\n",
      "eval (epoch #115): 100%|██████████| 27/27 [00:00<00:00, 32.64it/s, batch loss=4.809 avg. loss=4.020]\n",
      "train (epoch #116): 100%|██████████| 184/184 [00:11<00:00, 16.53it/s, batch loss=2.966 avg. loss=4.043]\n",
      "eval (epoch #116): 100%|██████████| 27/27 [00:00<00:00, 34.04it/s, batch loss=3.983 avg. loss=4.266]\n",
      "train (epoch #117): 100%|██████████| 184/184 [00:11<00:00, 16.47it/s, batch loss=2.274 avg. loss=4.019]\n",
      "eval (epoch #117): 100%|██████████| 27/27 [00:00<00:00, 31.37it/s, batch loss=3.753 avg. loss=3.619]\n",
      "train (epoch #118): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=2.911 avg. loss=4.022]\n",
      "eval (epoch #118): 100%|██████████| 27/27 [00:00<00:00, 31.13it/s, batch loss=5.184 avg. loss=4.830]\n",
      "train (epoch #119): 100%|██████████| 184/184 [00:11<00:00, 16.42it/s, batch loss=4.443 avg. loss=4.138]\n",
      "eval (epoch #119): 100%|██████████| 27/27 [00:00<00:00, 31.98it/s, batch loss=2.982 avg. loss=4.038]\n",
      "train (epoch #120): 100%|██████████| 184/184 [00:11<00:00, 15.77it/s, batch loss=3.148 avg. loss=4.062]\n",
      "eval (epoch #120): 100%|██████████| 27/27 [00:00<00:00, 32.75it/s, batch loss=4.484 avg. loss=5.585]\n",
      "train (epoch #121): 100%|██████████| 184/184 [00:11<00:00, 16.60it/s, batch loss=3.142 avg. loss=4.052]\n",
      "eval (epoch #121): 100%|██████████| 27/27 [00:00<00:00, 31.33it/s, batch loss=8.690 avg. loss=4.620]\n",
      "train (epoch #122): 100%|██████████| 184/184 [00:11<00:00, 15.68it/s, batch loss=1.525 avg. loss=4.002]\n",
      "eval (epoch #122): 100%|██████████| 27/27 [00:00<00:00, 33.00it/s, batch loss=3.570 avg. loss=3.785]\n",
      "train (epoch #123): 100%|██████████| 184/184 [00:10<00:00, 16.86it/s, batch loss=3.854 avg. loss=3.958]\n",
      "eval (epoch #123): 100%|██████████| 27/27 [00:00<00:00, 32.67it/s, batch loss=2.424 avg. loss=3.600]\n",
      "train (epoch #124): 100%|██████████| 184/184 [00:11<00:00, 15.88it/s, batch loss=2.351 avg. loss=3.932]\n",
      "eval (epoch #124): 100%|██████████| 27/27 [00:00<00:00, 30.83it/s, batch loss=6.004 avg. loss=3.725]\n",
      "train (epoch #125): 100%|██████████| 184/184 [00:11<00:00, 16.26it/s, batch loss=4.611 avg. loss=3.966]\n",
      "eval (epoch #125): 100%|██████████| 27/27 [00:00<00:00, 31.81it/s, batch loss=7.542 avg. loss=7.723]\n",
      "train (epoch #126): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=3.014 avg. loss=4.023]\n",
      "eval (epoch #126): 100%|██████████| 27/27 [00:00<00:00, 31.12it/s, batch loss=5.374 avg. loss=5.427]\n",
      "train (epoch #127): 100%|██████████| 184/184 [00:11<00:00, 15.86it/s, batch loss=1.736 avg. loss=3.973]\n",
      "eval (epoch #127): 100%|██████████| 27/27 [00:00<00:00, 32.05it/s, batch loss=4.091 avg. loss=3.864]\n",
      "train (epoch #128): 100%|██████████| 184/184 [00:11<00:00, 16.00it/s, batch loss=1.155 avg. loss=3.922]\n",
      "eval (epoch #128): 100%|██████████| 27/27 [00:00<00:00, 29.64it/s, batch loss=2.005 avg. loss=3.426]\n",
      "train (epoch #129): 100%|██████████| 184/184 [00:11<00:00, 16.08it/s, batch loss=3.334 avg. loss=3.911]\n",
      "eval (epoch #129): 100%|██████████| 27/27 [00:01<00:00, 24.58it/s, batch loss=3.643 avg. loss=4.369]\n",
      "train (epoch #130): 100%|██████████| 184/184 [00:10<00:00, 17.32it/s, batch loss=4.344 avg. loss=3.910]\n",
      "eval (epoch #130): 100%|██████████| 27/27 [00:00<00:00, 31.79it/s, batch loss=3.189 avg. loss=3.545]\n",
      "train (epoch #131): 100%|██████████| 184/184 [00:11<00:00, 16.09it/s, batch loss=2.880 avg. loss=3.859]\n",
      "eval (epoch #131): 100%|██████████| 27/27 [00:00<00:00, 31.77it/s, batch loss=3.640 avg. loss=6.321]\n",
      "train (epoch #132): 100%|██████████| 184/184 [00:11<00:00, 15.76it/s, batch loss=5.916 avg. loss=2419.508]\n",
      "eval (epoch #132): 100%|██████████| 27/27 [00:00<00:00, 31.62it/s, batch loss=4.397 avg. loss=4.808]\n",
      "train (epoch #133): 100%|██████████| 184/184 [00:11<00:00, 16.03it/s, batch loss=1.225 avg. loss=4.802]\n",
      "eval (epoch #133): 100%|██████████| 27/27 [00:00<00:00, 32.03it/s, batch loss=3.521 avg. loss=4.416]\n",
      "train (epoch #134): 100%|██████████| 184/184 [00:11<00:00, 16.30it/s, batch loss=3.629 avg. loss=4.666]\n",
      "eval (epoch #134): 100%|██████████| 27/27 [00:00<00:00, 32.88it/s, batch loss=3.298 avg. loss=4.353]\n",
      "train (epoch #135): 100%|██████████| 184/184 [00:10<00:00, 16.92it/s, batch loss=5.254 avg. loss=4.612]\n",
      "eval (epoch #135): 100%|██████████| 27/27 [00:01<00:00, 23.97it/s, batch loss=3.157 avg. loss=4.143]\n",
      "train (epoch #136): 100%|██████████| 184/184 [00:10<00:00, 16.90it/s, batch loss=2.882 avg. loss=4.544]\n",
      "eval (epoch #136): 100%|██████████| 27/27 [00:00<00:00, 30.58it/s, batch loss=3.190 avg. loss=4.208]\n",
      "train (epoch #137): 100%|██████████| 184/184 [00:11<00:00, 16.59it/s, batch loss=2.648 avg. loss=4.456]\n",
      "eval (epoch #137): 100%|██████████| 27/27 [00:00<00:00, 31.76it/s, batch loss=2.778 avg. loss=4.118]\n",
      "train (epoch #138): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=4.763 avg. loss=4.426]\n",
      "eval (epoch #138): 100%|██████████| 27/27 [00:00<00:00, 31.90it/s, batch loss=4.398 avg. loss=4.523]\n",
      "train (epoch #139): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=4.075 avg. loss=4.428]\n",
      "eval (epoch #139): 100%|██████████| 27/27 [00:00<00:00, 31.34it/s, batch loss=4.353 avg. loss=4.191]\n",
      "train (epoch #140): 100%|██████████| 184/184 [00:11<00:00, 16.40it/s, batch loss=8.255 avg. loss=4.422]\n",
      "eval (epoch #140): 100%|██████████| 27/27 [00:00<00:00, 32.88it/s, batch loss=3.534 avg. loss=4.014]\n",
      "train (epoch #141): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=8.683 avg. loss=4.362]\n",
      "eval (epoch #141): 100%|██████████| 27/27 [00:00<00:00, 33.26it/s, batch loss=10.050 avg. loss=4.520]\n",
      "train (epoch #142): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=3.554 avg. loss=4.387]\n",
      "eval (epoch #142): 100%|██████████| 27/27 [00:01<00:00, 24.15it/s, batch loss=5.081 avg. loss=4.745]\n",
      "train (epoch #143): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=2.718 avg. loss=4.346]\n",
      "eval (epoch #143): 100%|██████████| 27/27 [00:00<00:00, 32.41it/s, batch loss=5.787 avg. loss=4.637]\n",
      "train (epoch #144): 100%|██████████| 184/184 [00:11<00:00, 16.59it/s, batch loss=4.393 avg. loss=4.363]\n",
      "eval (epoch #144): 100%|██████████| 27/27 [00:00<00:00, 32.50it/s, batch loss=3.101 avg. loss=4.069]\n",
      "train (epoch #145): 100%|██████████| 184/184 [00:11<00:00, 15.81it/s, batch loss=3.445 avg. loss=4.331]\n",
      "eval (epoch #145): 100%|██████████| 27/27 [00:00<00:00, 29.96it/s, batch loss=6.277 avg. loss=4.314]\n",
      "train (epoch #146): 100%|██████████| 184/184 [00:11<00:00, 16.27it/s, batch loss=2.878 avg. loss=4.298]\n",
      "eval (epoch #146): 100%|██████████| 27/27 [00:00<00:00, 30.32it/s, batch loss=4.491 avg. loss=4.242]\n",
      "train (epoch #147): 100%|██████████| 184/184 [00:11<00:00, 16.03it/s, batch loss=4.393 avg. loss=4.318]\n",
      "eval (epoch #147): 100%|██████████| 27/27 [00:00<00:00, 30.89it/s, batch loss=4.887 avg. loss=4.164]\n",
      "train (epoch #148): 100%|██████████| 184/184 [00:11<00:00, 16.11it/s, batch loss=2.785 avg. loss=4.256]\n",
      "eval (epoch #148): 100%|██████████| 27/27 [00:00<00:00, 32.92it/s, batch loss=4.056 avg. loss=4.165]\n",
      "train (epoch #149): 100%|██████████| 184/184 [00:11<00:00, 16.31it/s, batch loss=4.434 avg. loss=4.256]\n",
      "eval (epoch #149): 100%|██████████| 27/27 [00:00<00:00, 31.96it/s, batch loss=3.416 avg. loss=3.837]\n",
      "train (epoch #150): 100%|██████████| 184/184 [00:11<00:00, 15.67it/s, batch loss=4.035 avg. loss=4.230]\n",
      "eval (epoch #150): 100%|██████████| 27/27 [00:00<00:00, 31.76it/s, batch loss=3.242 avg. loss=3.823]\n",
      "train (epoch #151): 100%|██████████| 184/184 [00:10<00:00, 16.96it/s, batch loss=3.846 avg. loss=4.238]\n",
      "eval (epoch #151): 100%|██████████| 27/27 [00:00<00:00, 29.33it/s, batch loss=4.261 avg. loss=3.823]\n",
      "train (epoch #152): 100%|██████████| 184/184 [00:11<00:00, 15.91it/s, batch loss=3.268 avg. loss=4.287]\n",
      "eval (epoch #152): 100%|██████████| 27/27 [00:00<00:00, 32.16it/s, batch loss=6.829 avg. loss=3.990]\n",
      "train (epoch #153): 100%|██████████| 184/184 [00:11<00:00, 15.88it/s, batch loss=6.881 avg. loss=4.264]\n",
      "eval (epoch #153): 100%|██████████| 27/27 [00:00<00:00, 31.49it/s, batch loss=3.697 avg. loss=4.009]\n",
      "train (epoch #154): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=4.741 avg. loss=4.215]\n",
      "eval (epoch #154): 100%|██████████| 27/27 [00:00<00:00, 31.77it/s, batch loss=3.565 avg. loss=4.339]\n",
      "train (epoch #155): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=2.913 avg. loss=4.180]\n",
      "eval (epoch #155): 100%|██████████| 27/27 [00:01<00:00, 23.41it/s, batch loss=6.566 avg. loss=4.941]\n",
      "train (epoch #156): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=4.538 avg. loss=4.245]\n",
      "eval (epoch #156): 100%|██████████| 27/27 [00:00<00:00, 29.21it/s, batch loss=2.262 avg. loss=3.730]\n",
      "train (epoch #157): 100%|██████████| 184/184 [00:11<00:00, 15.43it/s, batch loss=3.430 avg. loss=4.166]\n",
      "eval (epoch #157): 100%|██████████| 27/27 [00:00<00:00, 29.23it/s, batch loss=3.065 avg. loss=3.756]\n",
      "train (epoch #158): 100%|██████████| 184/184 [00:12<00:00, 15.01it/s, batch loss=1.509 avg. loss=4.134]\n",
      "eval (epoch #158): 100%|██████████| 27/27 [00:00<00:00, 29.22it/s, batch loss=2.947 avg. loss=3.901]\n",
      "train (epoch #159): 100%|██████████| 184/184 [00:11<00:00, 15.43it/s, batch loss=1.912 avg. loss=4.181]\n",
      "eval (epoch #159): 100%|██████████| 27/27 [00:00<00:00, 30.43it/s, batch loss=2.435 avg. loss=3.705]\n",
      "train (epoch #160): 100%|██████████| 184/184 [00:12<00:00, 15.30it/s, batch loss=4.324 avg. loss=4.111]\n",
      "eval (epoch #160): 100%|██████████| 27/27 [00:00<00:00, 28.62it/s, batch loss=4.427 avg. loss=4.447]\n",
      "train (epoch #161): 100%|██████████| 184/184 [00:12<00:00, 15.14it/s, batch loss=2.165 avg. loss=4.211]\n",
      "eval (epoch #161): 100%|██████████| 27/27 [00:00<00:00, 29.48it/s, batch loss=3.052 avg. loss=3.958]\n",
      "train (epoch #162): 100%|██████████| 184/184 [00:12<00:00, 15.22it/s, batch loss=3.817 avg. loss=4.111]\n",
      "eval (epoch #162): 100%|██████████| 27/27 [00:00<00:00, 29.48it/s, batch loss=4.943 avg. loss=4.026]\n",
      "train (epoch #163): 100%|██████████| 184/184 [00:18<00:00,  9.91it/s, batch loss=3.482 avg. loss=4.145]\n",
      "eval (epoch #163): 100%|██████████| 27/27 [00:00<00:00, 29.33it/s, batch loss=4.671 avg. loss=4.021]\n",
      "train (epoch #164): 100%|██████████| 184/184 [00:11<00:00, 15.60it/s, batch loss=2.465 avg. loss=4.117]\n",
      "eval (epoch #164): 100%|██████████| 27/27 [00:00<00:00, 29.02it/s, batch loss=3.364 avg. loss=3.875]\n",
      "train (epoch #165): 100%|██████████| 184/184 [00:12<00:00, 15.04it/s, batch loss=3.786 avg. loss=4.109]\n",
      "eval (epoch #165): 100%|██████████| 27/27 [00:00<00:00, 28.38it/s, batch loss=7.619 avg. loss=4.412]\n",
      "train (epoch #166): 100%|██████████| 184/184 [00:11<00:00, 15.54it/s, batch loss=3.255 avg. loss=4.121]\n",
      "eval (epoch #166): 100%|██████████| 27/27 [00:00<00:00, 29.51it/s, batch loss=7.413 avg. loss=6.135]\n",
      "train (epoch #167): 100%|██████████| 184/184 [00:11<00:00, 16.57it/s, batch loss=1.835 avg. loss=4.161]\n",
      "eval (epoch #167): 100%|██████████| 27/27 [00:01<00:00, 24.64it/s, batch loss=5.680 avg. loss=4.109]\n",
      "train (epoch #168): 100%|██████████| 184/184 [00:10<00:00, 17.07it/s, batch loss=3.987 avg. loss=4.089]\n",
      "eval (epoch #168): 100%|██████████| 27/27 [00:00<00:00, 33.93it/s, batch loss=4.502 avg. loss=4.113]\n",
      "train (epoch #169): 100%|██████████| 184/184 [00:10<00:00, 17.01it/s, batch loss=4.098 avg. loss=4.098]\n",
      "eval (epoch #169): 100%|██████████| 27/27 [00:00<00:00, 31.23it/s, batch loss=3.555 avg. loss=3.809]\n",
      "train (epoch #170): 100%|██████████| 184/184 [00:11<00:00, 15.76it/s, batch loss=4.111 avg. loss=4.029]\n",
      "eval (epoch #170): 100%|██████████| 27/27 [00:00<00:00, 31.95it/s, batch loss=2.388 avg. loss=3.962]\n",
      "train (epoch #171): 100%|██████████| 184/184 [00:10<00:00, 16.92it/s, batch loss=2.946 avg. loss=4.120]\n",
      "eval (epoch #171): 100%|██████████| 27/27 [00:00<00:00, 33.37it/s, batch loss=4.548 avg. loss=3.800]\n",
      "train (epoch #172): 100%|██████████| 184/184 [00:10<00:00, 17.02it/s, batch loss=3.092 avg. loss=4.079]\n",
      "eval (epoch #172): 100%|██████████| 27/27 [00:01<00:00, 23.14it/s, batch loss=3.756 avg. loss=3.762]\n",
      "train (epoch #173): 100%|██████████| 184/184 [00:11<00:00, 16.26it/s, batch loss=2.851 avg. loss=4.044]\n",
      "eval (epoch #173): 100%|██████████| 27/27 [00:00<00:00, 32.28it/s, batch loss=3.675 avg. loss=3.827]\n",
      "train (epoch #174): 100%|██████████| 184/184 [00:11<00:00, 16.31it/s, batch loss=1.622 avg. loss=4.006]\n",
      "eval (epoch #174): 100%|██████████| 27/27 [00:00<00:00, 30.16it/s, batch loss=3.498 avg. loss=3.735]\n",
      "train (epoch #175): 100%|██████████| 184/184 [00:11<00:00, 16.38it/s, batch loss=5.424 avg. loss=4.009]\n",
      "eval (epoch #175): 100%|██████████| 27/27 [00:00<00:00, 30.99it/s, batch loss=2.728 avg. loss=3.602]\n",
      "train (epoch #176): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=2.932 avg. loss=4.047]\n",
      "eval (epoch #176): 100%|██████████| 27/27 [00:00<00:00, 32.31it/s, batch loss=5.601 avg. loss=5.673]\n",
      "train (epoch #177): 100%|██████████| 184/184 [00:11<00:00, 16.13it/s, batch loss=4.148 avg. loss=4.034]\n",
      "eval (epoch #177): 100%|██████████| 27/27 [00:01<00:00, 23.16it/s, batch loss=5.387 avg. loss=4.371]\n",
      "train (epoch #178): 100%|██████████| 184/184 [00:11<00:00, 16.23it/s, batch loss=2.405 avg. loss=4.064]\n",
      "eval (epoch #178): 100%|██████████| 27/27 [00:00<00:00, 29.16it/s, batch loss=4.236 avg. loss=3.727]\n",
      "train (epoch #179): 100%|██████████| 184/184 [00:11<00:00, 15.91it/s, batch loss=3.433 avg. loss=3.985]\n",
      "eval (epoch #179): 100%|██████████| 27/27 [00:00<00:00, 32.83it/s, batch loss=2.837 avg. loss=3.809]\n",
      "train (epoch #180): 100%|██████████| 184/184 [00:11<00:00, 16.20it/s, batch loss=9.301 avg. loss=4.056]\n",
      "eval (epoch #180): 100%|██████████| 27/27 [00:00<00:00, 32.37it/s, batch loss=4.248 avg. loss=3.661]\n",
      "train (epoch #181): 100%|██████████| 184/184 [00:11<00:00, 16.35it/s, batch loss=2.539 avg. loss=4.021]\n",
      "eval (epoch #181): 100%|██████████| 27/27 [00:00<00:00, 31.86it/s, batch loss=7.233 avg. loss=7.035]\n",
      "train (epoch #182): 100%|██████████| 184/184 [00:11<00:00, 16.09it/s, batch loss=2.958 avg. loss=3.990]\n",
      "eval (epoch #182): 100%|██████████| 27/27 [00:00<00:00, 32.64it/s, batch loss=3.559 avg. loss=3.626]\n",
      "train (epoch #183): 100%|██████████| 184/184 [00:10<00:00, 16.74it/s, batch loss=1.759 avg. loss=3.972]\n",
      "eval (epoch #183): 100%|██████████| 27/27 [00:00<00:00, 33.10it/s, batch loss=3.559 avg. loss=3.570]\n",
      "train (epoch #184): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=6.857 avg. loss=3.958]\n",
      "eval (epoch #184): 100%|██████████| 27/27 [00:00<00:00, 28.27it/s, batch loss=3.597 avg. loss=3.845]\n",
      "train (epoch #185): 100%|██████████| 184/184 [00:11<00:00, 15.38it/s, batch loss=2.483 avg. loss=3.966]\n",
      "eval (epoch #185): 100%|██████████| 27/27 [00:00<00:00, 29.07it/s, batch loss=3.699 avg. loss=3.596]\n",
      "train (epoch #186): 100%|██████████| 184/184 [00:11<00:00, 15.64it/s, batch loss=3.456 avg. loss=3.905]\n",
      "eval (epoch #186): 100%|██████████| 27/27 [00:00<00:00, 27.93it/s, batch loss=3.923 avg. loss=3.777]\n",
      "train (epoch #187): 100%|██████████| 184/184 [00:15<00:00, 11.94it/s, batch loss=3.850 avg. loss=3.973]\n",
      "eval (epoch #187): 100%|██████████| 27/27 [00:02<00:00, 13.50it/s, batch loss=3.926 avg. loss=4.111]\n",
      "train (epoch #188): 100%|██████████| 184/184 [00:14<00:00, 13.08it/s, batch loss=2.573 avg. loss=3.936]\n",
      "eval (epoch #188): 100%|██████████| 27/27 [00:00<00:00, 29.05it/s, batch loss=3.176 avg. loss=3.531]\n",
      "train (epoch #189): 100%|██████████| 184/184 [00:14<00:00, 12.37it/s, batch loss=5.595 avg. loss=3.895]\n",
      "eval (epoch #189): 100%|██████████| 27/27 [00:00<00:00, 29.13it/s, batch loss=2.944 avg. loss=3.716]\n",
      "train (epoch #190): 100%|██████████| 184/184 [00:11<00:00, 16.31it/s, batch loss=2.296 avg. loss=3.888]\n",
      "eval (epoch #190): 100%|██████████| 27/27 [00:00<00:00, 30.49it/s, batch loss=2.701 avg. loss=3.614]\n",
      "train (epoch #191): 100%|██████████| 184/184 [00:11<00:00, 16.22it/s, batch loss=4.365 avg. loss=3.879]\n",
      "eval (epoch #191): 100%|██████████| 27/27 [00:01<00:00, 23.75it/s, batch loss=4.184 avg. loss=4.061]\n",
      "train (epoch #192): 100%|██████████| 184/184 [00:11<00:00, 15.96it/s, batch loss=4.450 avg. loss=3.938]\n",
      "eval (epoch #192): 100%|██████████| 27/27 [00:00<00:00, 29.55it/s, batch loss=2.842 avg. loss=3.482]\n",
      "train (epoch #193): 100%|██████████| 184/184 [00:11<00:00, 16.01it/s, batch loss=4.185 avg. loss=3.891]\n",
      "eval (epoch #193): 100%|██████████| 27/27 [00:00<00:00, 29.73it/s, batch loss=4.684 avg. loss=3.907]\n",
      "train (epoch #194): 100%|██████████| 184/184 [00:11<00:00, 15.84it/s, batch loss=2.853 avg. loss=3.855]\n",
      "eval (epoch #194): 100%|██████████| 27/27 [00:00<00:00, 32.36it/s, batch loss=3.992 avg. loss=3.586]\n",
      "train (epoch #195): 100%|██████████| 184/184 [00:11<00:00, 15.59it/s, batch loss=7.331 avg. loss=3.915]\n",
      "eval (epoch #195): 100%|██████████| 27/27 [00:00<00:00, 31.29it/s, batch loss=2.568 avg. loss=3.597]\n",
      "train (epoch #196): 100%|██████████| 184/184 [00:11<00:00, 15.67it/s, batch loss=2.776 avg. loss=3.870]\n",
      "eval (epoch #196): 100%|██████████| 27/27 [00:00<00:00, 31.33it/s, batch loss=3.824 avg. loss=4.558]\n",
      "train (epoch #197): 100%|██████████| 184/184 [00:11<00:00, 16.43it/s, batch loss=4.785 avg. loss=3.858]\n",
      "eval (epoch #197): 100%|██████████| 27/27 [00:00<00:00, 31.02it/s, batch loss=6.852 avg. loss=4.044]\n",
      "train (epoch #198): 100%|██████████| 184/184 [00:11<00:00, 15.61it/s, batch loss=2.584 avg. loss=3.919]\n",
      "eval (epoch #198): 100%|██████████| 27/27 [00:00<00:00, 30.67it/s, batch loss=4.038 avg. loss=3.466]\n",
      "train (epoch #199): 100%|██████████| 184/184 [00:11<00:00, 16.46it/s, batch loss=3.793 avg. loss=3.867]\n",
      "eval (epoch #199): 100%|██████████| 27/27 [00:00<00:00, 30.11it/s, batch loss=2.603 avg. loss=3.429]\n",
      "train (epoch #200): 100%|██████████| 184/184 [00:11<00:00, 16.14it/s, batch loss=2.447 avg. loss=3.754]\n",
      "eval (epoch #200): 100%|██████████| 27/27 [00:00<00:00, 31.49it/s, batch loss=5.144 avg. loss=3.963]\n",
      "train (epoch #201): 100%|██████████| 184/184 [00:14<00:00, 13.03it/s, batch loss=2.579 avg. loss=3.867]\n",
      "eval (epoch #201): 100%|██████████| 27/27 [00:00<00:00, 29.78it/s, batch loss=2.040 avg. loss=3.637]\n",
      "train (epoch #202): 100%|██████████| 184/184 [00:11<00:00, 15.65it/s, batch loss=2.509 avg. loss=3.847]\n",
      "eval (epoch #202): 100%|██████████| 27/27 [00:00<00:00, 30.68it/s, batch loss=5.220 avg. loss=3.689]\n",
      "train (epoch #203): 100%|██████████| 184/184 [00:13<00:00, 14.05it/s, batch loss=2.401 avg. loss=3.807]\n",
      "eval (epoch #203): 100%|██████████| 27/27 [00:00<00:00, 30.73it/s, batch loss=2.549 avg. loss=3.811]\n",
      "train (epoch #204): 100%|██████████| 184/184 [00:21<00:00,  8.59it/s, batch loss=3.156 avg. loss=3.804]\n",
      "eval (epoch #204): 100%|██████████| 27/27 [00:01<00:00, 15.77it/s, batch loss=2.473 avg. loss=3.506]\n",
      "train (epoch #205): 100%|██████████| 184/184 [00:12<00:00, 15.25it/s, batch loss=1.585 avg. loss=3.796]\n",
      "eval (epoch #205): 100%|██████████| 27/27 [00:00<00:00, 30.44it/s, batch loss=2.571 avg. loss=3.470]\n",
      "train (epoch #206): 100%|██████████| 184/184 [00:11<00:00, 16.22it/s, batch loss=2.970 avg. loss=3.784]\n",
      "eval (epoch #206): 100%|██████████| 27/27 [00:00<00:00, 31.22it/s, batch loss=5.178 avg. loss=5.607]\n",
      "train (epoch #207): 100%|██████████| 184/184 [00:11<00:00, 16.60it/s, batch loss=2.827 avg. loss=3.793]\n",
      "eval (epoch #207): 100%|██████████| 27/27 [00:00<00:00, 32.26it/s, batch loss=4.438 avg. loss=3.885]\n",
      "train (epoch #208): 100%|██████████| 184/184 [00:11<00:00, 15.56it/s, batch loss=2.707 avg. loss=3.823]\n",
      "eval (epoch #208): 100%|██████████| 27/27 [00:00<00:00, 31.48it/s, batch loss=5.207 avg. loss=3.582]\n",
      "train (epoch #209): 100%|██████████| 184/184 [00:19<00:00,  9.31it/s, batch loss=1.188 avg. loss=3.739]\n",
      "eval (epoch #209): 100%|██████████| 27/27 [00:01<00:00, 19.68it/s, batch loss=5.869 avg. loss=3.558]\n",
      "train (epoch #210): 100%|██████████| 184/184 [00:15<00:00, 11.73it/s, batch loss=4.306 avg. loss=3.776]\n",
      "eval (epoch #210): 100%|██████████| 27/27 [00:00<00:00, 30.67it/s, batch loss=3.034 avg. loss=3.668]\n",
      "train (epoch #211): 100%|██████████| 184/184 [00:14<00:00, 12.66it/s, batch loss=2.917 avg. loss=3.738]\n",
      "eval (epoch #211): 100%|██████████| 27/27 [00:00<00:00, 29.11it/s, batch loss=3.839 avg. loss=4.100]\n",
      "train (epoch #212): 100%|██████████| 184/184 [00:11<00:00, 15.84it/s, batch loss=5.614 avg. loss=3.802]\n",
      "eval (epoch #212): 100%|██████████| 27/27 [00:00<00:00, 31.76it/s, batch loss=7.481 avg. loss=5.525]\n",
      "train (epoch #213): 100%|██████████| 184/184 [00:11<00:00, 15.53it/s, batch loss=2.594 avg. loss=3.857]\n",
      "eval (epoch #213): 100%|██████████| 27/27 [00:00<00:00, 30.62it/s, batch loss=2.941 avg. loss=3.507]\n",
      "train (epoch #214): 100%|██████████| 184/184 [00:11<00:00, 16.54it/s, batch loss=2.581 avg. loss=3.764]\n",
      "eval (epoch #214): 100%|██████████| 27/27 [00:00<00:00, 29.11it/s, batch loss=1.801 avg. loss=3.650]\n",
      "train (epoch #215): 100%|██████████| 184/184 [00:11<00:00, 15.92it/s, batch loss=1.704 avg. loss=3.752]\n",
      "eval (epoch #215): 100%|██████████| 27/27 [00:01<00:00, 22.54it/s, batch loss=3.966 avg. loss=3.873]\n",
      "train (epoch #216): 100%|██████████| 184/184 [00:11<00:00, 15.79it/s, batch loss=2.877 avg. loss=4.321]\n",
      "eval (epoch #216): 100%|██████████| 27/27 [00:00<00:00, 32.23it/s, batch loss=7.564 avg. loss=4.013]\n",
      "train (epoch #217): 100%|██████████| 184/184 [00:11<00:00, 15.56it/s, batch loss=3.020 avg. loss=4.018]\n",
      "eval (epoch #217): 100%|██████████| 27/27 [00:00<00:00, 30.10it/s, batch loss=5.984 avg. loss=5.018]\n",
      "train (epoch #218): 100%|██████████| 184/184 [00:19<00:00,  9.35it/s, batch loss=3.159 avg. loss=3.933]\n",
      "eval (epoch #218): 100%|██████████| 27/27 [00:01<00:00, 15.74it/s, batch loss=2.993 avg. loss=3.720]\n",
      "train (epoch #219): 100%|██████████| 184/184 [00:13<00:00, 13.46it/s, batch loss=3.022 avg. loss=3.901]\n",
      "eval (epoch #219): 100%|██████████| 27/27 [00:00<00:00, 31.61it/s, batch loss=4.730 avg. loss=4.046]\n",
      "train (epoch #220): 100%|██████████| 184/184 [00:11<00:00, 16.63it/s, batch loss=2.354 avg. loss=3.892]\n",
      "eval (epoch #220): 100%|██████████| 27/27 [00:00<00:00, 31.24it/s, batch loss=4.083 avg. loss=3.725]\n",
      "train (epoch #221): 100%|██████████| 184/184 [00:11<00:00, 15.89it/s, batch loss=2.342 avg. loss=3.849]\n",
      "eval (epoch #221): 100%|██████████| 27/27 [00:00<00:00, 29.30it/s, batch loss=2.715 avg. loss=3.495]\n",
      "train (epoch #222): 100%|██████████| 184/184 [00:11<00:00, 15.95it/s, batch loss=2.571 avg. loss=3.761]\n",
      "eval (epoch #222): 100%|██████████| 27/27 [00:00<00:00, 29.45it/s, batch loss=5.429 avg. loss=4.351]\n",
      "train (epoch #223): 100%|██████████| 184/184 [00:11<00:00, 15.41it/s, batch loss=1.729 avg. loss=3.859]\n",
      "eval (epoch #223): 100%|██████████| 27/27 [00:01<00:00, 18.66it/s, batch loss=4.557 avg. loss=3.791]\n",
      "train (epoch #224): 100%|██████████| 184/184 [00:14<00:00, 12.81it/s, batch loss=5.312 avg. loss=3.794]\n",
      "eval (epoch #224): 100%|██████████| 27/27 [00:00<00:00, 29.92it/s, batch loss=4.403 avg. loss=4.069]\n",
      "train (epoch #225): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=8.020 avg. loss=3.891]\n",
      "eval (epoch #225): 100%|██████████| 27/27 [00:00<00:00, 30.30it/s, batch loss=3.409 avg. loss=3.902]\n",
      "train (epoch #226): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=3.998 avg. loss=3.818]\n",
      "eval (epoch #226): 100%|██████████| 27/27 [00:01<00:00, 22.59it/s, batch loss=2.406 avg. loss=3.753]\n",
      "train (epoch #227): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=14.262 avg. loss=3.830]\n",
      "eval (epoch #227): 100%|██████████| 27/27 [00:00<00:00, 31.40it/s, batch loss=2.927 avg. loss=3.966]\n",
      "train (epoch #228): 100%|██████████| 184/184 [00:11<00:00, 15.76it/s, batch loss=2.551 avg. loss=3.823]\n",
      "eval (epoch #228): 100%|██████████| 27/27 [00:00<00:00, 31.91it/s, batch loss=1.713 avg. loss=3.387]\n",
      "train (epoch #229): 100%|██████████| 184/184 [00:11<00:00, 16.69it/s, batch loss=2.321 avg. loss=3.741]\n",
      "eval (epoch #229): 100%|██████████| 27/27 [00:00<00:00, 31.43it/s, batch loss=2.896 avg. loss=3.834]\n",
      "train (epoch #230): 100%|██████████| 184/184 [00:14<00:00, 12.42it/s, batch loss=1.765 avg. loss=3.741]\n",
      "eval (epoch #230): 100%|██████████| 27/27 [00:02<00:00, 12.50it/s, batch loss=3.358 avg. loss=3.799]\n",
      "train (epoch #231): 100%|██████████| 184/184 [00:11<00:00, 15.41it/s, batch loss=2.332 avg. loss=3.715]\n",
      "eval (epoch #231): 100%|██████████| 27/27 [00:00<00:00, 28.68it/s, batch loss=3.413 avg. loss=4.283]\n",
      "train (epoch #232): 100%|██████████| 184/184 [00:12<00:00, 14.84it/s, batch loss=1.783 avg. loss=3.752]\n",
      "eval (epoch #232): 100%|██████████| 27/27 [00:01<00:00, 22.35it/s, batch loss=4.142 avg. loss=3.557]\n",
      "train (epoch #233): 100%|██████████| 184/184 [00:11<00:00, 15.82it/s, batch loss=2.424 avg. loss=3.683]\n",
      "eval (epoch #233): 100%|██████████| 27/27 [00:00<00:00, 30.19it/s, batch loss=3.417 avg. loss=3.520]\n",
      "train (epoch #234): 100%|██████████| 184/184 [00:11<00:00, 15.84it/s, batch loss=4.167 avg. loss=3.806]\n",
      "eval (epoch #234): 100%|██████████| 27/27 [00:01<00:00, 16.52it/s, batch loss=2.548 avg. loss=3.423]\n",
      "train (epoch #235): 100%|██████████| 184/184 [00:11<00:00, 15.59it/s, batch loss=2.208 avg. loss=3.733]\n",
      "eval (epoch #235): 100%|██████████| 27/27 [00:00<00:00, 30.18it/s, batch loss=3.289 avg. loss=3.881]\n",
      "train (epoch #236): 100%|██████████| 184/184 [00:11<00:00, 15.74it/s, batch loss=5.562 avg. loss=3.720]\n",
      "eval (epoch #236): 100%|██████████| 27/27 [00:00<00:00, 29.41it/s, batch loss=11.711 avg. loss=3.860]\n",
      "train (epoch #237): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=2.775 avg. loss=3.717]\n",
      "eval (epoch #237): 100%|██████████| 27/27 [00:00<00:00, 28.10it/s, batch loss=4.659 avg. loss=5.033]\n",
      "train (epoch #238): 100%|██████████| 184/184 [00:11<00:00, 15.68it/s, batch loss=3.267 avg. loss=3.743]\n",
      "eval (epoch #238): 100%|██████████| 27/27 [00:00<00:00, 31.59it/s, batch loss=3.729 avg. loss=3.929]\n",
      "train (epoch #239): 100%|██████████| 184/184 [00:11<00:00, 16.31it/s, batch loss=3.258 avg. loss=3.699]\n",
      "eval (epoch #239): 100%|██████████| 27/27 [00:00<00:00, 28.08it/s, batch loss=3.302 avg. loss=3.466]\n",
      "train (epoch #240): 100%|██████████| 184/184 [00:11<00:00, 16.07it/s, batch loss=4.219 avg. loss=3.658]\n",
      "eval (epoch #240): 100%|██████████| 27/27 [00:01<00:00, 23.04it/s, batch loss=2.960 avg. loss=5.369]\n",
      "train (epoch #241): 100%|██████████| 184/184 [00:11<00:00, 16.29it/s, batch loss=4.079 avg. loss=3.693]\n",
      "eval (epoch #241): 100%|██████████| 27/27 [00:00<00:00, 29.55it/s, batch loss=4.343 avg. loss=3.776]\n",
      "train (epoch #242): 100%|██████████| 184/184 [00:19<00:00,  9.56it/s, batch loss=2.071 avg. loss=3.706]\n",
      "eval (epoch #242): 100%|██████████| 27/27 [00:01<00:00, 13.57it/s, batch loss=4.964 avg. loss=3.757]\n",
      "train (epoch #243): 100%|██████████| 184/184 [00:16<00:00, 11.42it/s, batch loss=3.676 avg. loss=3.659]\n",
      "eval (epoch #243): 100%|██████████| 27/27 [00:00<00:00, 28.95it/s, batch loss=3.718 avg. loss=3.507]\n",
      "train (epoch #244): 100%|██████████| 184/184 [00:11<00:00, 15.44it/s, batch loss=15.079 avg. loss=3.690]\n",
      "eval (epoch #244): 100%|██████████| 27/27 [00:00<00:00, 28.73it/s, batch loss=2.550 avg. loss=3.550]\n",
      "train (epoch #245): 100%|██████████| 184/184 [00:11<00:00, 15.77it/s, batch loss=2.277 avg. loss=3.748]\n",
      "eval (epoch #245): 100%|██████████| 27/27 [00:00<00:00, 29.73it/s, batch loss=2.380 avg. loss=3.453]\n",
      "train (epoch #246): 100%|██████████| 184/184 [00:11<00:00, 15.67it/s, batch loss=2.804 avg. loss=3.687]\n",
      "eval (epoch #246): 100%|██████████| 27/27 [00:00<00:00, 29.03it/s, batch loss=3.463 avg. loss=4.625]\n",
      "train (epoch #247): 100%|██████████| 184/184 [00:12<00:00, 15.15it/s, batch loss=2.960 avg. loss=3.685]\n",
      "eval (epoch #247): 100%|██████████| 27/27 [00:00<00:00, 31.09it/s, batch loss=3.160 avg. loss=3.367]\n",
      "train (epoch #248): 100%|██████████| 184/184 [00:11<00:00, 15.43it/s, batch loss=1.640 avg. loss=3.654]\n",
      "eval (epoch #248): 100%|██████████| 27/27 [00:00<00:00, 28.19it/s, batch loss=3.544 avg. loss=3.431]\n",
      "train (epoch #249): 100%|██████████| 184/184 [00:12<00:00, 15.05it/s, batch loss=8.776 avg. loss=3.658]\n",
      "eval (epoch #249): 100%|██████████| 27/27 [00:00<00:00, 29.38it/s, batch loss=4.227 avg. loss=3.856]\n",
      "train (epoch #250): 100%|██████████| 184/184 [00:11<00:00, 15.70it/s, batch loss=1.403 avg. loss=3.651]\n",
      "eval (epoch #250): 100%|██████████| 27/27 [00:00<00:00, 28.56it/s, batch loss=3.013 avg. loss=3.459]\n",
      "train (epoch #251): 100%|██████████| 184/184 [00:12<00:00, 15.04it/s, batch loss=5.130 avg. loss=3.663]\n",
      "eval (epoch #251): 100%|██████████| 27/27 [00:00<00:00, 29.62it/s, batch loss=3.853 avg. loss=3.730]\n",
      "train (epoch #252): 100%|██████████| 184/184 [00:12<00:00, 15.19it/s, batch loss=2.566 avg. loss=3.659]\n",
      "eval (epoch #252): 100%|██████████| 27/27 [00:00<00:00, 30.26it/s, batch loss=2.909 avg. loss=4.015]\n",
      "train (epoch #253): 100%|██████████| 184/184 [00:11<00:00, 15.96it/s, batch loss=2.675 avg. loss=3.740]\n",
      "eval (epoch #253): 100%|██████████| 27/27 [00:00<00:00, 27.92it/s, batch loss=2.844 avg. loss=3.602]\n",
      "train (epoch #254): 100%|██████████| 184/184 [00:11<00:00, 15.57it/s, batch loss=3.759 avg. loss=3.655]\n",
      "eval (epoch #254): 100%|██████████| 27/27 [00:00<00:00, 27.95it/s, batch loss=3.556 avg. loss=3.446]\n",
      "train (epoch #255): 100%|██████████| 184/184 [00:11<00:00, 15.45it/s, batch loss=8.371 avg. loss=3.619]\n",
      "eval (epoch #255): 100%|██████████| 27/27 [00:00<00:00, 28.30it/s, batch loss=3.744 avg. loss=3.812]\n",
      "train (epoch #256): 100%|██████████| 184/184 [00:11<00:00, 15.42it/s, batch loss=1.820 avg. loss=3.628]\n",
      "eval (epoch #256): 100%|██████████| 27/27 [00:00<00:00, 28.41it/s, batch loss=3.154 avg. loss=3.612]\n",
      "train (epoch #257): 100%|██████████| 184/184 [00:12<00:00, 15.30it/s, batch loss=3.015 avg. loss=3.582]\n",
      "eval (epoch #257): 100%|██████████| 27/27 [00:00<00:00, 28.89it/s, batch loss=2.689 avg. loss=3.582]\n",
      "train (epoch #258): 100%|██████████| 184/184 [00:11<00:00, 16.29it/s, batch loss=3.205 avg. loss=3.653]\n",
      "eval (epoch #258): 100%|██████████| 27/27 [00:00<00:00, 30.79it/s, batch loss=5.057 avg. loss=3.485]\n",
      "train (epoch #259): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=1.393 avg. loss=3.586]\n",
      "eval (epoch #259): 100%|██████████| 27/27 [00:00<00:00, 27.81it/s, batch loss=2.858 avg. loss=3.470]\n",
      "train (epoch #260): 100%|██████████| 184/184 [00:11<00:00, 15.67it/s, batch loss=3.156 avg. loss=3.614]\n",
      "eval (epoch #260): 100%|██████████| 27/27 [00:01<00:00, 21.70it/s, batch loss=2.812 avg. loss=3.360]\n",
      "train (epoch #261): 100%|██████████| 184/184 [00:11<00:00, 15.49it/s, batch loss=4.124 avg. loss=3.591]\n",
      "eval (epoch #261): 100%|██████████| 27/27 [00:00<00:00, 29.41it/s, batch loss=4.548 avg. loss=3.686]\n",
      "train (epoch #262): 100%|██████████| 184/184 [00:11<00:00, 15.57it/s, batch loss=3.870 avg. loss=3.674]\n",
      "eval (epoch #262): 100%|██████████| 27/27 [00:00<00:00, 28.12it/s, batch loss=2.639 avg. loss=3.471]\n",
      "train (epoch #263): 100%|██████████| 184/184 [00:12<00:00, 15.13it/s, batch loss=4.303 avg. loss=3.623]\n",
      "eval (epoch #263): 100%|██████████| 27/27 [00:00<00:00, 30.42it/s, batch loss=3.084 avg. loss=4.508]\n",
      "train (epoch #264): 100%|██████████| 184/184 [00:11<00:00, 15.56it/s, batch loss=1.381 avg. loss=3.698]\n",
      "eval (epoch #264): 100%|██████████| 27/27 [00:00<00:00, 28.51it/s, batch loss=4.607 avg. loss=3.551]\n",
      "train (epoch #265): 100%|██████████| 184/184 [00:11<00:00, 15.34it/s, batch loss=2.391 avg. loss=3.594]\n",
      "eval (epoch #265): 100%|██████████| 27/27 [00:00<00:00, 30.40it/s, batch loss=2.701 avg. loss=4.525]\n",
      "train (epoch #266): 100%|██████████| 184/184 [00:11<00:00, 15.52it/s, batch loss=5.696 avg. loss=3.613]\n",
      "eval (epoch #266): 100%|██████████| 27/27 [00:00<00:00, 30.12it/s, batch loss=2.866 avg. loss=3.523]\n",
      "train (epoch #267): 100%|██████████| 184/184 [00:11<00:00, 15.78it/s, batch loss=2.629 avg. loss=3.619]\n",
      "eval (epoch #267): 100%|██████████| 27/27 [00:00<00:00, 29.07it/s, batch loss=3.182 avg. loss=3.297]\n",
      "train (epoch #268): 100%|██████████| 184/184 [00:11<00:00, 15.52it/s, batch loss=2.555 avg. loss=3.602]\n",
      "eval (epoch #268): 100%|██████████| 27/27 [00:01<00:00, 23.06it/s, batch loss=3.238 avg. loss=3.371]\n",
      "train (epoch #269): 100%|██████████| 184/184 [00:11<00:00, 15.79it/s, batch loss=3.698 avg. loss=3.632]\n",
      "eval (epoch #269): 100%|██████████| 27/27 [00:00<00:00, 28.43it/s, batch loss=6.518 avg. loss=4.949]\n",
      "train (epoch #270): 100%|██████████| 184/184 [00:11<00:00, 15.61it/s, batch loss=1.713 avg. loss=3.574]\n",
      "eval (epoch #270): 100%|██████████| 27/27 [00:00<00:00, 27.80it/s, batch loss=4.682 avg. loss=4.043]\n",
      "train (epoch #271): 100%|██████████| 184/184 [00:11<00:00, 15.55it/s, batch loss=2.261 avg. loss=3.611]\n",
      "eval (epoch #271): 100%|██████████| 27/27 [00:00<00:00, 30.62it/s, batch loss=4.156 avg. loss=3.431]\n",
      "train (epoch #272): 100%|██████████| 184/184 [00:11<00:00, 15.77it/s, batch loss=2.072 avg. loss=3.535]\n",
      "eval (epoch #272): 100%|██████████| 27/27 [00:00<00:00, 30.06it/s, batch loss=4.138 avg. loss=3.458]\n",
      "train (epoch #273): 100%|██████████| 184/184 [00:11<00:00, 15.72it/s, batch loss=3.139 avg. loss=3.502]\n",
      "eval (epoch #273): 100%|██████████| 27/27 [00:00<00:00, 30.30it/s, batch loss=2.878 avg. loss=3.577]\n",
      "train (epoch #274): 100%|██████████| 184/184 [00:11<00:00, 15.66it/s, batch loss=1.568 avg. loss=3.574]\n",
      "eval (epoch #274): 100%|██████████| 27/27 [00:00<00:00, 29.78it/s, batch loss=3.979 avg. loss=3.385]\n",
      "train (epoch #275): 100%|██████████| 184/184 [00:11<00:00, 15.60it/s, batch loss=13.333 avg. loss=3.620]\n",
      "eval (epoch #275): 100%|██████████| 27/27 [00:00<00:00, 28.76it/s, batch loss=4.994 avg. loss=3.936]\n",
      "train (epoch #276): 100%|██████████| 184/184 [00:11<00:00, 15.51it/s, batch loss=4.540 avg. loss=3.617]\n",
      "eval (epoch #276): 100%|██████████| 27/27 [00:00<00:00, 27.62it/s, batch loss=4.522 avg. loss=4.688]\n",
      "train (epoch #277): 100%|██████████| 184/184 [00:12<00:00, 15.30it/s, batch loss=1.905 avg. loss=3.716]\n",
      "eval (epoch #277): 100%|██████████| 27/27 [00:00<00:00, 27.65it/s, batch loss=4.547 avg. loss=3.469]\n",
      "train (epoch #278): 100%|██████████| 184/184 [00:11<00:00, 15.45it/s, batch loss=2.516 avg. loss=3.563]\n",
      "eval (epoch #278): 100%|██████████| 27/27 [00:00<00:00, 28.16it/s, batch loss=3.207 avg. loss=3.352]\n",
      "train (epoch #279): 100%|██████████| 184/184 [00:11<00:00, 15.72it/s, batch loss=6.005 avg. loss=3.567]\n",
      "eval (epoch #279): 100%|██████████| 27/27 [00:01<00:00, 22.05it/s, batch loss=4.230 avg. loss=4.906]\n",
      "train (epoch #280): 100%|██████████| 184/184 [00:11<00:00, 15.78it/s, batch loss=5.373 avg. loss=3.625]\n",
      "eval (epoch #280): 100%|██████████| 27/27 [00:00<00:00, 28.37it/s, batch loss=2.500 avg. loss=3.367]\n",
      "train (epoch #281): 100%|██████████| 184/184 [00:11<00:00, 15.82it/s, batch loss=1.749 avg. loss=3.545]\n",
      "eval (epoch #281): 100%|██████████| 27/27 [00:00<00:00, 31.65it/s, batch loss=2.802 avg. loss=3.437]\n",
      "train (epoch #282): 100%|██████████| 184/184 [00:11<00:00, 15.83it/s, batch loss=3.599 avg. loss=3.607]\n",
      "eval (epoch #282): 100%|██████████| 27/27 [00:01<00:00, 22.53it/s, batch loss=2.292 avg. loss=3.331]\n",
      "train (epoch #283): 100%|██████████| 184/184 [00:14<00:00, 13.00it/s, batch loss=4.609 avg. loss=3.557]\n",
      "eval (epoch #283): 100%|██████████| 27/27 [00:00<00:00, 30.53it/s, batch loss=3.089 avg. loss=3.294]\n",
      "train (epoch #284): 100%|██████████| 184/184 [00:11<00:00, 15.61it/s, batch loss=3.304 avg. loss=3.537]\n",
      "eval (epoch #284): 100%|██████████| 27/27 [00:00<00:00, 28.57it/s, batch loss=2.733 avg. loss=3.383]\n",
      "train (epoch #285): 100%|██████████| 184/184 [00:12<00:00, 15.26it/s, batch loss=8.273 avg. loss=3.552]\n",
      "eval (epoch #285): 100%|██████████| 27/27 [00:00<00:00, 28.71it/s, batch loss=3.197 avg. loss=3.694]\n",
      "train (epoch #286): 100%|██████████| 184/184 [00:11<00:00, 15.96it/s, batch loss=5.296 avg. loss=3.597]\n",
      "eval (epoch #286): 100%|██████████| 27/27 [00:00<00:00, 29.33it/s, batch loss=5.615 avg. loss=3.711]\n",
      "train (epoch #287): 100%|██████████| 184/184 [00:11<00:00, 15.46it/s, batch loss=2.876 avg. loss=3.541]\n",
      "eval (epoch #287): 100%|██████████| 27/27 [00:00<00:00, 30.80it/s, batch loss=3.327 avg. loss=3.379]\n",
      "train (epoch #288): 100%|██████████| 184/184 [00:14<00:00, 12.35it/s, batch loss=1.693 avg. loss=3.487]\n",
      "eval (epoch #288): 100%|██████████| 27/27 [00:00<00:00, 27.67it/s, batch loss=2.918 avg. loss=3.446]\n",
      "train (epoch #289): 100%|██████████| 184/184 [00:11<00:00, 15.87it/s, batch loss=4.984 avg. loss=3.587]\n",
      "eval (epoch #289): 100%|██████████| 27/27 [00:00<00:00, 30.53it/s, batch loss=2.788 avg. loss=3.588]\n",
      "train (epoch #290): 100%|██████████| 184/184 [00:12<00:00, 15.27it/s, batch loss=2.457 avg. loss=3.524]\n",
      "eval (epoch #290): 100%|██████████| 27/27 [00:01<00:00, 21.06it/s, batch loss=5.368 avg. loss=3.460]\n",
      "train (epoch #291): 100%|██████████| 184/184 [00:13<00:00, 13.25it/s, batch loss=4.311 avg. loss=3.511]\n",
      "eval (epoch #291): 100%|██████████| 27/27 [00:00<00:00, 28.75it/s, batch loss=4.451 avg. loss=3.921]\n",
      "train (epoch #292): 100%|██████████| 184/184 [00:11<00:00, 15.58it/s, batch loss=2.744 avg. loss=3.489]\n",
      "eval (epoch #292): 100%|██████████| 27/27 [00:01<00:00, 21.95it/s, batch loss=2.863 avg. loss=3.564]\n",
      "train (epoch #293): 100%|██████████| 184/184 [00:11<00:00, 15.85it/s, batch loss=1.216 avg. loss=3.474]\n",
      "eval (epoch #293): 100%|██████████| 27/27 [00:00<00:00, 29.97it/s, batch loss=2.220 avg. loss=3.586]\n",
      "train (epoch #294): 100%|██████████| 184/184 [00:13<00:00, 13.49it/s, batch loss=4.408 avg. loss=3.468]\n",
      "eval (epoch #294): 100%|██████████| 27/27 [00:01<00:00, 20.57it/s, batch loss=3.674 avg. loss=3.764]\n",
      "train (epoch #295): 100%|██████████| 184/184 [00:14<00:00, 12.27it/s, batch loss=4.432 avg. loss=3.606]\n",
      "eval (epoch #295): 100%|██████████| 27/27 [00:00<00:00, 31.30it/s, batch loss=3.514 avg. loss=3.298]\n",
      "train (epoch #296): 100%|██████████| 184/184 [00:11<00:00, 15.78it/s, batch loss=4.904 avg. loss=3.491]\n",
      "eval (epoch #296): 100%|██████████| 27/27 [00:00<00:00, 28.46it/s, batch loss=3.398 avg. loss=3.382]\n",
      "train (epoch #297): 100%|██████████| 184/184 [00:13<00:00, 13.49it/s, batch loss=4.329 avg. loss=3.518]\n",
      "eval (epoch #297): 100%|██████████| 27/27 [00:01<00:00, 15.59it/s, batch loss=2.647 avg. loss=3.366]\n",
      "train (epoch #298): 100%|██████████| 184/184 [00:13<00:00, 13.17it/s, batch loss=28.182 avg. loss=3.599]\n",
      "eval (epoch #298): 100%|██████████| 27/27 [00:00<00:00, 28.59it/s, batch loss=2.483 avg. loss=3.984]\n",
      "train (epoch #299): 100%|██████████| 184/184 [00:11<00:00, 15.39it/s, batch loss=2.513 avg. loss=3.615]\n",
      "eval (epoch #299): 100%|██████████| 27/27 [00:00<00:00, 28.08it/s, batch loss=3.353 avg. loss=3.946]\n",
      "train (epoch #300): 100%|██████████| 184/184 [00:11<00:00, 15.52it/s, batch loss=0.919 avg. loss=3.514]\n",
      "eval (epoch #300): 100%|██████████| 27/27 [00:00<00:00, 29.23it/s, batch loss=3.060 avg. loss=3.299]\n",
      "train (epoch #301): 100%|██████████| 184/184 [00:11<00:00, 15.37it/s, batch loss=2.824 avg. loss=3.477]\n",
      "eval (epoch #301): 100%|██████████| 27/27 [00:01<00:00, 21.57it/s, batch loss=3.899 avg. loss=3.476]\n",
      "train (epoch #302): 100%|██████████| 184/184 [00:15<00:00, 11.96it/s, batch loss=2.045 avg. loss=3.531]\n",
      "eval (epoch #302): 100%|██████████| 27/27 [00:00<00:00, 30.63it/s, batch loss=1.851 avg. loss=3.294]\n",
      "train (epoch #303): 100%|██████████| 184/184 [00:11<00:00, 16.36it/s, batch loss=9.553 avg. loss=3.526]\n",
      "eval (epoch #303): 100%|██████████| 27/27 [00:01<00:00, 17.71it/s, batch loss=4.262 avg. loss=3.627]\n",
      "train (epoch #304): 100%|██████████| 184/184 [00:13<00:00, 13.23it/s, batch loss=2.492 avg. loss=3.533]\n",
      "eval (epoch #304): 100%|██████████| 27/27 [00:00<00:00, 30.01it/s, batch loss=4.105 avg. loss=3.307]\n",
      "train (epoch #305): 100%|██████████| 184/184 [00:11<00:00, 15.70it/s, batch loss=4.799 avg. loss=3.491]\n",
      "eval (epoch #305): 100%|██████████| 27/27 [00:00<00:00, 29.12it/s, batch loss=8.553 avg. loss=4.617]\n",
      "train (epoch #306): 100%|██████████| 184/184 [00:11<00:00, 15.81it/s, batch loss=2.018 avg. loss=3.455]\n",
      "eval (epoch #306): 100%|██████████| 27/27 [00:00<00:00, 29.37it/s, batch loss=2.677 avg. loss=3.269]\n",
      "train (epoch #307): 100%|██████████| 184/184 [00:11<00:00, 15.79it/s, batch loss=8.312 avg. loss=3.475]\n",
      "eval (epoch #307): 100%|██████████| 27/27 [00:01<00:00, 23.00it/s, batch loss=2.951 avg. loss=3.475]\n",
      "train (epoch #308): 100%|██████████| 184/184 [00:14<00:00, 12.80it/s, batch loss=2.162 avg. loss=3.484]\n",
      "eval (epoch #308): 100%|██████████| 27/27 [00:00<00:00, 30.25it/s, batch loss=2.595 avg. loss=3.449]\n",
      "train (epoch #309): 100%|██████████| 184/184 [00:11<00:00, 15.66it/s, batch loss=1.891 avg. loss=3.431]\n",
      "eval (epoch #309): 100%|██████████| 27/27 [00:00<00:00, 29.04it/s, batch loss=2.838 avg. loss=3.591]\n",
      "train (epoch #310): 100%|██████████| 184/184 [00:12<00:00, 15.22it/s, batch loss=1.635 avg. loss=3.435]\n",
      "eval (epoch #310): 100%|██████████| 27/27 [00:01<00:00, 18.44it/s, batch loss=3.843 avg. loss=3.735]\n",
      "train (epoch #311): 100%|██████████| 184/184 [00:13<00:00, 14.13it/s, batch loss=2.190 avg. loss=3.469]\n",
      "eval (epoch #311): 100%|██████████| 27/27 [00:00<00:00, 29.39it/s, batch loss=2.608 avg. loss=3.310]\n",
      "train (epoch #312): 100%|██████████| 184/184 [00:13<00:00, 13.60it/s, batch loss=3.670 avg. loss=3.426]\n",
      "eval (epoch #312): 100%|██████████| 27/27 [00:01<00:00, 17.89it/s, batch loss=2.080 avg. loss=3.300]\n",
      "train (epoch #313): 100%|██████████| 184/184 [00:13<00:00, 13.91it/s, batch loss=6.335 avg. loss=3.462]\n",
      "eval (epoch #313): 100%|██████████| 27/27 [00:01<00:00, 24.53it/s, batch loss=4.176 avg. loss=4.806]\n",
      "train (epoch #314): 100%|██████████| 184/184 [00:11<00:00, 15.53it/s, batch loss=4.345 avg. loss=3.502]\n",
      "eval (epoch #314): 100%|██████████| 27/27 [00:00<00:00, 31.70it/s, batch loss=4.391 avg. loss=3.654]\n",
      "train (epoch #315): 100%|██████████| 184/184 [00:11<00:00, 16.38it/s, batch loss=1.479 avg. loss=3.483]\n",
      "eval (epoch #315): 100%|██████████| 27/27 [00:00<00:00, 28.54it/s, batch loss=2.964 avg. loss=3.557]\n",
      "train (epoch #316): 100%|██████████| 184/184 [00:11<00:00, 16.12it/s, batch loss=2.178 avg. loss=3.457]\n",
      "eval (epoch #316): 100%|██████████| 27/27 [00:00<00:00, 29.78it/s, batch loss=2.927 avg. loss=3.382]\n",
      "train (epoch #317): 100%|██████████| 184/184 [00:11<00:00, 15.33it/s, batch loss=3.241 avg. loss=3.392]\n",
      "eval (epoch #317): 100%|██████████| 27/27 [00:02<00:00, 12.53it/s, batch loss=3.210 avg. loss=3.643]\n",
      "train (epoch #318): 100%|██████████| 184/184 [00:11<00:00, 15.90it/s, batch loss=2.281 avg. loss=3.452]\n",
      "eval (epoch #318): 100%|██████████| 27/27 [00:00<00:00, 30.91it/s, batch loss=3.761 avg. loss=3.471]\n",
      "train (epoch #319): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=2.236 avg. loss=3.481]\n",
      "eval (epoch #319): 100%|██████████| 27/27 [00:00<00:00, 31.52it/s, batch loss=1.689 avg. loss=3.311]\n",
      "train (epoch #320): 100%|██████████| 184/184 [00:11<00:00, 16.17it/s, batch loss=2.270 avg. loss=3.394]\n",
      "eval (epoch #320): 100%|██████████| 27/27 [00:00<00:00, 28.42it/s, batch loss=4.066 avg. loss=3.452]\n",
      "train (epoch #321): 100%|██████████| 184/184 [00:12<00:00, 14.30it/s, batch loss=2.220 avg. loss=3.463]\n",
      "eval (epoch #321): 100%|██████████| 27/27 [00:00<00:00, 28.60it/s, batch loss=3.281 avg. loss=3.491]\n",
      "train (epoch #322): 100%|██████████| 184/184 [00:11<00:00, 15.39it/s, batch loss=1.540 avg. loss=3.421]\n",
      "eval (epoch #322): 100%|██████████| 27/27 [00:00<00:00, 28.44it/s, batch loss=2.833 avg. loss=3.394]\n",
      "train (epoch #323): 100%|██████████| 184/184 [00:12<00:00, 14.84it/s, batch loss=2.639 avg. loss=3.472]\n",
      "eval (epoch #323): 100%|██████████| 27/27 [00:00<00:00, 29.64it/s, batch loss=2.521 avg. loss=3.293]\n",
      "train (epoch #324): 100%|██████████| 184/184 [00:11<00:00, 15.89it/s, batch loss=1.095 avg. loss=3.432]\n",
      "eval (epoch #324): 100%|██████████| 27/27 [00:00<00:00, 29.00it/s, batch loss=4.826 avg. loss=3.396]\n",
      "train (epoch #325): 100%|██████████| 184/184 [00:12<00:00, 15.12it/s, batch loss=2.078 avg. loss=3.452]\n",
      "eval (epoch #325): 100%|██████████| 27/27 [00:00<00:00, 30.37it/s, batch loss=3.020 avg. loss=3.338]\n",
      "train (epoch #326): 100%|██████████| 184/184 [00:11<00:00, 16.51it/s, batch loss=1.480 avg. loss=3.406]\n",
      "eval (epoch #326): 100%|██████████| 27/27 [00:00<00:00, 31.08it/s, batch loss=5.905 avg. loss=3.563]\n",
      "train (epoch #327): 100%|██████████| 184/184 [00:11<00:00, 16.16it/s, batch loss=7.009 avg. loss=3.476]\n",
      "eval (epoch #327): 100%|██████████| 27/27 [00:00<00:00, 28.81it/s, batch loss=4.111 avg. loss=4.193]\n",
      "train (epoch #328): 100%|██████████| 184/184 [00:12<00:00, 15.31it/s, batch loss=20.524 avg. loss=3.546]\n",
      "eval (epoch #328): 100%|██████████| 27/27 [00:00<00:00, 31.64it/s, batch loss=4.439 avg. loss=4.101]\n",
      "train (epoch #329): 100%|██████████| 184/184 [00:11<00:00, 16.29it/s, batch loss=5.348 avg. loss=3.555]\n",
      "eval (epoch #329): 100%|██████████| 27/27 [00:00<00:00, 30.77it/s, batch loss=4.117 avg. loss=3.350]\n",
      "train (epoch #330): 100%|██████████| 184/184 [00:11<00:00, 16.27it/s, batch loss=1.743 avg. loss=3.407]\n",
      "eval (epoch #330): 100%|██████████| 27/27 [00:00<00:00, 29.79it/s, batch loss=3.707 avg. loss=3.516]\n",
      "train (epoch #331): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=2.727 avg. loss=3.386]\n",
      "eval (epoch #331): 100%|██████████| 27/27 [00:00<00:00, 31.35it/s, batch loss=2.238 avg. loss=3.281]\n",
      "train (epoch #332): 100%|██████████| 184/184 [00:11<00:00, 16.12it/s, batch loss=2.295 avg. loss=3.420]\n",
      "eval (epoch #332): 100%|██████████| 27/27 [00:00<00:00, 30.72it/s, batch loss=3.206 avg. loss=3.371]\n",
      "train (epoch #333): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=3.604 avg. loss=3.411]\n",
      "eval (epoch #333): 100%|██████████| 27/27 [00:00<00:00, 30.84it/s, batch loss=3.431 avg. loss=3.703]\n",
      "train (epoch #334): 100%|██████████| 184/184 [00:11<00:00, 15.76it/s, batch loss=3.043 avg. loss=3.418]\n",
      "eval (epoch #334): 100%|██████████| 27/27 [00:00<00:00, 30.11it/s, batch loss=4.065 avg. loss=3.893]\n",
      "train (epoch #335): 100%|██████████| 184/184 [00:11<00:00, 15.70it/s, batch loss=2.221 avg. loss=3.392]\n",
      "eval (epoch #335): 100%|██████████| 27/27 [00:00<00:00, 28.53it/s, batch loss=2.664 avg. loss=3.289]\n",
      "train (epoch #336): 100%|██████████| 184/184 [00:11<00:00, 15.83it/s, batch loss=2.474 avg. loss=3.402]\n",
      "eval (epoch #336): 100%|██████████| 27/27 [00:00<00:00, 27.79it/s, batch loss=2.876 avg. loss=3.306]\n",
      "train (epoch #337): 100%|██████████| 184/184 [00:11<00:00, 15.87it/s, batch loss=2.114 avg. loss=3.377]\n",
      "eval (epoch #337): 100%|██████████| 27/27 [00:01<00:00, 22.71it/s, batch loss=7.021 avg. loss=3.715]\n",
      "train (epoch #338): 100%|██████████| 184/184 [00:11<00:00, 15.66it/s, batch loss=1.444 avg. loss=3.376]\n",
      "eval (epoch #338): 100%|██████████| 27/27 [00:00<00:00, 29.95it/s, batch loss=2.733 avg. loss=3.356]\n",
      "train (epoch #339): 100%|██████████| 184/184 [00:11<00:00, 16.09it/s, batch loss=1.621 avg. loss=3.385]\n",
      "eval (epoch #339): 100%|██████████| 27/27 [00:00<00:00, 31.23it/s, batch loss=2.279 avg. loss=3.307]\n",
      "train (epoch #340): 100%|██████████| 184/184 [00:21<00:00,  8.73it/s, batch loss=3.584 avg. loss=3.424]\n",
      "eval (epoch #340): 100%|██████████| 27/27 [00:02<00:00, 12.06it/s, batch loss=4.487 avg. loss=4.000]\n",
      "train (epoch #341): 100%|██████████| 184/184 [00:17<00:00, 10.61it/s, batch loss=2.053 avg. loss=3.401]\n",
      "eval (epoch #341): 100%|██████████| 27/27 [00:00<00:00, 27.76it/s, batch loss=2.622 avg. loss=3.251]\n",
      "train (epoch #342): 100%|██████████| 184/184 [00:12<00:00, 15.02it/s, batch loss=6.329 avg. loss=3.435]\n",
      "eval (epoch #342): 100%|██████████| 27/27 [00:00<00:00, 27.46it/s, batch loss=4.317 avg. loss=4.678]\n",
      "train (epoch #343): 100%|██████████| 184/184 [00:11<00:00, 15.49it/s, batch loss=1.889 avg. loss=3.434]\n",
      "eval (epoch #343): 100%|██████████| 27/27 [00:00<00:00, 27.44it/s, batch loss=4.043 avg. loss=3.332]\n",
      "train (epoch #344): 100%|██████████| 184/184 [00:11<00:00, 15.52it/s, batch loss=1.480 avg. loss=3.361]\n",
      "eval (epoch #344): 100%|██████████| 27/27 [00:00<00:00, 31.50it/s, batch loss=3.275 avg. loss=3.805]\n",
      "train (epoch #345): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=2.823 avg. loss=3.373]\n",
      "eval (epoch #345): 100%|██████████| 27/27 [00:01<00:00, 23.08it/s, batch loss=5.154 avg. loss=3.477]\n",
      "train (epoch #346): 100%|██████████| 184/184 [00:11<00:00, 16.03it/s, batch loss=3.596 avg. loss=3.356]\n",
      "eval (epoch #346): 100%|██████████| 27/27 [00:00<00:00, 28.97it/s, batch loss=2.106 avg. loss=3.320]\n",
      "train (epoch #347): 100%|██████████| 184/184 [00:11<00:00, 15.70it/s, batch loss=4.600 avg. loss=3.391]\n",
      "eval (epoch #347): 100%|██████████| 27/27 [00:01<00:00, 21.45it/s, batch loss=4.604 avg. loss=3.633]\n",
      "train (epoch #348): 100%|██████████| 184/184 [00:13<00:00, 13.66it/s, batch loss=1.418 avg. loss=3.356]\n",
      "eval (epoch #348): 100%|██████████| 27/27 [00:00<00:00, 30.06it/s, batch loss=5.085 avg. loss=3.452]\n",
      "train (epoch #349): 100%|██████████| 184/184 [00:11<00:00, 16.18it/s, batch loss=1.646 avg. loss=3.322]\n",
      "eval (epoch #349): 100%|██████████| 27/27 [00:00<00:00, 31.66it/s, batch loss=3.275 avg. loss=3.458]\n",
      "train (epoch #350): 100%|██████████| 184/184 [00:16<00:00, 11.34it/s, batch loss=2.274 avg. loss=3.295]\n",
      "eval (epoch #350): 100%|██████████| 27/27 [00:00<00:00, 30.89it/s, batch loss=2.386 avg. loss=3.276]\n",
      "train (epoch #351): 100%|██████████| 184/184 [00:11<00:00, 16.00it/s, batch loss=3.037 avg. loss=3.359]\n",
      "eval (epoch #351): 100%|██████████| 27/27 [00:00<00:00, 28.75it/s, batch loss=2.349 avg. loss=3.462]\n",
      "train (epoch #352): 100%|██████████| 184/184 [00:11<00:00, 16.55it/s, batch loss=1.744 avg. loss=3.351]\n",
      "eval (epoch #352): 100%|██████████| 27/27 [00:01<00:00, 19.13it/s, batch loss=3.861 avg. loss=3.355]\n",
      "train (epoch #353): 100%|██████████| 184/184 [00:13<00:00, 14.03it/s, batch loss=1.507 avg. loss=3.418]\n",
      "eval (epoch #353): 100%|██████████| 27/27 [00:00<00:00, 31.87it/s, batch loss=2.417 avg. loss=3.388]\n",
      "train (epoch #354): 100%|██████████| 184/184 [00:12<00:00, 14.27it/s, batch loss=4.834 avg. loss=3.359]\n",
      "eval (epoch #354): 100%|██████████| 27/27 [00:01<00:00, 17.70it/s, batch loss=3.356 avg. loss=3.891]\n",
      "train (epoch #355): 100%|██████████| 184/184 [00:15<00:00, 11.70it/s, batch loss=2.012 avg. loss=3.386]\n",
      "eval (epoch #355): 100%|██████████| 27/27 [00:01<00:00, 14.53it/s, batch loss=3.188 avg. loss=3.324]\n",
      "train (epoch #356): 100%|██████████| 184/184 [00:13<00:00, 13.98it/s, batch loss=1.900 avg. loss=3.358]\n",
      "eval (epoch #356): 100%|██████████| 27/27 [00:00<00:00, 30.58it/s, batch loss=3.296 avg. loss=3.381]\n",
      "train (epoch #357): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=1.817 avg. loss=3.292]\n",
      "eval (epoch #357): 100%|██████████| 27/27 [00:00<00:00, 30.27it/s, batch loss=3.752 avg. loss=3.431]\n",
      "train (epoch #358): 100%|██████████| 184/184 [00:11<00:00, 16.10it/s, batch loss=2.391 avg. loss=3.340]\n",
      "eval (epoch #358): 100%|██████████| 27/27 [00:00<00:00, 31.74it/s, batch loss=3.222 avg. loss=3.323]\n",
      "train (epoch #359): 100%|██████████| 184/184 [00:14<00:00, 12.63it/s, batch loss=2.048 avg. loss=3.347]\n",
      "eval (epoch #359): 100%|██████████| 27/27 [00:02<00:00, 11.43it/s, batch loss=2.856 avg. loss=3.278]\n",
      "train (epoch #360): 100%|██████████| 184/184 [00:20<00:00,  9.13it/s, batch loss=0.922 avg. loss=3.331]\n",
      "eval (epoch #360): 100%|██████████| 27/27 [00:02<00:00, 12.44it/s, batch loss=2.259 avg. loss=3.405]\n",
      "train (epoch #361): 100%|██████████| 184/184 [00:17<00:00, 10.26it/s, batch loss=2.593 avg. loss=3.308]\n",
      "eval (epoch #361): 100%|██████████| 27/27 [00:01<00:00, 15.99it/s, batch loss=4.243 avg. loss=3.747]\n",
      "train (epoch #362): 100%|██████████| 184/184 [00:18<00:00,  9.95it/s, batch loss=3.354 avg. loss=3.384]\n",
      "eval (epoch #362): 100%|██████████| 27/27 [00:01<00:00, 22.78it/s, batch loss=4.483 avg. loss=3.611]\n",
      "train (epoch #363): 100%|██████████| 184/184 [00:19<00:00,  9.28it/s, batch loss=1.317 avg. loss=3.381]\n",
      "eval (epoch #363): 100%|██████████| 27/27 [00:02<00:00, 11.66it/s, batch loss=3.097 avg. loss=3.750]\n",
      "train (epoch #364): 100%|██████████| 184/184 [00:16<00:00, 11.07it/s, batch loss=1.479 avg. loss=3.313]\n",
      "eval (epoch #364): 100%|██████████| 27/27 [00:00<00:00, 32.01it/s, batch loss=4.532 avg. loss=3.356]\n",
      "train (epoch #365): 100%|██████████| 184/184 [00:11<00:00, 15.57it/s, batch loss=2.606 avg. loss=3.255]\n",
      "eval (epoch #365): 100%|██████████| 27/27 [00:00<00:00, 29.95it/s, batch loss=3.216 avg. loss=3.533]\n",
      "train (epoch #366): 100%|██████████| 184/184 [00:11<00:00, 16.36it/s, batch loss=3.684 avg. loss=3.326]\n",
      "eval (epoch #366): 100%|██████████| 27/27 [00:00<00:00, 31.61it/s, batch loss=4.688 avg. loss=3.406]\n",
      "train (epoch #367): 100%|██████████| 184/184 [00:11<00:00, 15.90it/s, batch loss=1.160 avg. loss=3.295]\n",
      "eval (epoch #367): 100%|██████████| 27/27 [00:00<00:00, 32.20it/s, batch loss=2.259 avg. loss=3.250]\n",
      "train (epoch #368): 100%|██████████| 184/184 [00:11<00:00, 15.50it/s, batch loss=8.233 avg. loss=3.415]\n",
      "eval (epoch #368): 100%|██████████| 27/27 [00:00<00:00, 31.62it/s, batch loss=3.378 avg. loss=3.347]\n",
      "train (epoch #369): 100%|██████████| 184/184 [00:11<00:00, 16.06it/s, batch loss=6.549 avg. loss=3.357]\n",
      "eval (epoch #369): 100%|██████████| 27/27 [00:00<00:00, 31.12it/s, batch loss=1.965 avg. loss=3.148]\n",
      "train (epoch #370): 100%|██████████| 184/184 [00:11<00:00, 15.98it/s, batch loss=2.913 avg. loss=3.325]\n",
      "eval (epoch #370): 100%|██████████| 27/27 [00:00<00:00, 31.66it/s, batch loss=3.685 avg. loss=3.845]\n",
      "train (epoch #371): 100%|██████████| 184/184 [00:11<00:00, 16.35it/s, batch loss=2.749 avg. loss=3.320]\n",
      "eval (epoch #371): 100%|██████████| 27/27 [00:01<00:00, 24.85it/s, batch loss=2.777 avg. loss=3.163]\n",
      "train (epoch #372): 100%|██████████| 184/184 [00:11<00:00, 16.34it/s, batch loss=3.742 avg. loss=3.301]\n",
      "eval (epoch #372): 100%|██████████| 27/27 [00:00<00:00, 31.69it/s, batch loss=4.498 avg. loss=4.185]\n",
      "train (epoch #373): 100%|██████████| 184/184 [00:12<00:00, 14.40it/s, batch loss=1.034 avg. loss=3.418]\n",
      "eval (epoch #373): 100%|██████████| 27/27 [00:00<00:00, 31.16it/s, batch loss=2.317 avg. loss=3.271]\n",
      "train (epoch #374): 100%|██████████| 184/184 [00:11<00:00, 15.85it/s, batch loss=2.104 avg. loss=3.279]\n",
      "eval (epoch #374): 100%|██████████| 27/27 [00:00<00:00, 30.99it/s, batch loss=2.801 avg. loss=3.425]\n",
      "train (epoch #375): 100%|██████████| 184/184 [00:11<00:00, 16.56it/s, batch loss=0.951 avg. loss=3.269]\n",
      "eval (epoch #375): 100%|██████████| 27/27 [00:00<00:00, 31.40it/s, batch loss=5.009 avg. loss=3.505]\n",
      "train (epoch #376): 100%|██████████| 184/184 [00:11<00:00, 15.48it/s, batch loss=1.949 avg. loss=3.336]\n",
      "eval (epoch #376): 100%|██████████| 27/27 [00:00<00:00, 30.93it/s, batch loss=2.461 avg. loss=3.295]\n",
      "train (epoch #377): 100%|██████████| 184/184 [00:11<00:00, 16.02it/s, batch loss=1.162 avg. loss=3.212]\n",
      "eval (epoch #377): 100%|██████████| 27/27 [00:00<00:00, 31.76it/s, batch loss=3.176 avg. loss=3.363]\n",
      "train (epoch #378): 100%|██████████| 184/184 [00:11<00:00, 16.50it/s, batch loss=2.701 avg. loss=3.316]\n",
      "eval (epoch #378): 100%|██████████| 27/27 [00:00<00:00, 31.68it/s, batch loss=3.415 avg. loss=3.323]\n",
      "train (epoch #379): 100%|██████████| 184/184 [00:11<00:00, 16.27it/s, batch loss=3.879 avg. loss=3.414]\n",
      "eval (epoch #379): 100%|██████████| 27/27 [00:01<00:00, 23.94it/s, batch loss=3.283 avg. loss=3.764]\n",
      "train (epoch #380): 100%|██████████| 184/184 [00:11<00:00, 16.62it/s, batch loss=2.794 avg. loss=3.339]\n",
      "eval (epoch #380): 100%|██████████| 27/27 [00:00<00:00, 29.82it/s, batch loss=3.660 avg. loss=3.341]\n",
      "train (epoch #381): 100%|██████████| 184/184 [00:12<00:00, 15.27it/s, batch loss=3.446 avg. loss=3.283]\n",
      "eval (epoch #381): 100%|██████████| 27/27 [00:00<00:00, 30.90it/s, batch loss=4.922 avg. loss=3.403]\n",
      "train (epoch #382): 100%|██████████| 184/184 [00:11<00:00, 16.45it/s, batch loss=2.465 avg. loss=3.283]\n",
      "eval (epoch #382): 100%|██████████| 27/27 [00:00<00:00, 31.26it/s, batch loss=2.538 avg. loss=3.256]\n",
      "train (epoch #383): 100%|██████████| 184/184 [00:11<00:00, 16.67it/s, batch loss=3.681 avg. loss=3.234]\n",
      "eval (epoch #383): 100%|██████████| 27/27 [00:00<00:00, 31.14it/s, batch loss=3.069 avg. loss=3.297]\n",
      "train (epoch #384): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=1.766 avg. loss=3.236]\n",
      "eval (epoch #384): 100%|██████████| 27/27 [00:00<00:00, 30.70it/s, batch loss=2.542 avg. loss=3.433]\n",
      "train (epoch #385): 100%|██████████| 184/184 [00:11<00:00, 16.69it/s, batch loss=1.727 avg. loss=3.286]\n",
      "eval (epoch #385): 100%|██████████| 27/27 [00:00<00:00, 31.75it/s, batch loss=2.488 avg. loss=3.258]\n",
      "train (epoch #386): 100%|██████████| 184/184 [00:11<00:00, 16.50it/s, batch loss=5.605 avg. loss=3.249]\n",
      "eval (epoch #386): 100%|██████████| 27/27 [00:01<00:00, 23.34it/s, batch loss=2.095 avg. loss=3.191]\n",
      "train (epoch #387): 100%|██████████| 184/184 [00:11<00:00, 16.23it/s, batch loss=2.940 avg. loss=3.294]\n",
      "eval (epoch #387): 100%|██████████| 27/27 [00:00<00:00, 31.54it/s, batch loss=2.327 avg. loss=3.613]\n",
      "train (epoch #388): 100%|██████████| 184/184 [00:11<00:00, 16.52it/s, batch loss=1.159 avg. loss=3.262]\n",
      "eval (epoch #388): 100%|██████████| 27/27 [00:00<00:00, 31.95it/s, batch loss=2.706 avg. loss=3.342]\n",
      "train (epoch #389): 100%|██████████| 184/184 [00:11<00:00, 15.88it/s, batch loss=4.541 avg. loss=3.299]\n",
      "eval (epoch #389): 100%|██████████| 27/27 [00:00<00:00, 29.36it/s, batch loss=2.712 avg. loss=3.366]\n",
      "train (epoch #390): 100%|██████████| 184/184 [00:11<00:00, 16.44it/s, batch loss=1.713 avg. loss=3.315]\n",
      "eval (epoch #390): 100%|██████████| 27/27 [00:00<00:00, 31.37it/s, batch loss=4.122 avg. loss=3.272]\n",
      "train (epoch #391): 100%|██████████| 184/184 [00:11<00:00, 16.64it/s, batch loss=5.718 avg. loss=3.279]\n",
      "eval (epoch #391): 100%|██████████| 27/27 [00:00<00:00, 32.32it/s, batch loss=2.629 avg. loss=3.497]\n",
      "train (epoch #392): 100%|██████████| 184/184 [00:11<00:00, 16.31it/s, batch loss=2.725 avg. loss=3.250]\n",
      "eval (epoch #392): 100%|██████████| 27/27 [00:01<00:00, 24.35it/s, batch loss=2.097 avg. loss=3.302]\n",
      "train (epoch #393): 100%|██████████| 184/184 [00:11<00:00, 16.62it/s, batch loss=1.458 avg. loss=3.284]\n",
      "eval (epoch #393): 100%|██████████| 27/27 [00:00<00:00, 30.51it/s, batch loss=3.315 avg. loss=3.302]\n",
      "train (epoch #394): 100%|██████████| 184/184 [00:11<00:00, 16.43it/s, batch loss=3.285 avg. loss=3.232]\n",
      "eval (epoch #394): 100%|██████████| 27/27 [00:00<00:00, 30.35it/s, batch loss=3.502 avg. loss=4.072]\n",
      "train (epoch #395): 100%|██████████| 184/184 [00:11<00:00, 16.51it/s, batch loss=3.739 avg. loss=3.283]\n",
      "eval (epoch #395): 100%|██████████| 27/27 [00:00<00:00, 31.02it/s, batch loss=2.029 avg. loss=3.202]\n",
      "train (epoch #396): 100%|██████████| 184/184 [00:11<00:00, 16.29it/s, batch loss=3.055 avg. loss=3.217]\n",
      "eval (epoch #396): 100%|██████████| 27/27 [00:00<00:00, 28.67it/s, batch loss=2.773 avg. loss=3.353]\n",
      "train (epoch #397): 100%|██████████| 184/184 [00:11<00:00, 16.71it/s, batch loss=1.928 avg. loss=3.240]\n",
      "eval (epoch #397): 100%|██████████| 27/27 [00:00<00:00, 30.23it/s, batch loss=2.584 avg. loss=3.364]\n",
      "train (epoch #398): 100%|██████████| 184/184 [00:11<00:00, 16.33it/s, batch loss=2.414 avg. loss=3.226]\n",
      "eval (epoch #398): 100%|██████████| 27/27 [00:00<00:00, 30.39it/s, batch loss=2.812 avg. loss=3.290]\n",
      "train (epoch #399): 100%|██████████| 184/184 [00:16<00:00, 11.19it/s, batch loss=5.323 avg. loss=3.287]\n",
      "eval (epoch #399): 100%|██████████| 27/27 [00:00<00:00, 31.82it/s, batch loss=2.799 avg. loss=3.906]\n",
      "train (epoch #400): 100%|██████████| 184/184 [00:11<00:00, 16.30it/s, batch loss=2.082 avg. loss=3.255]\n",
      "eval (epoch #400): 100%|██████████| 27/27 [00:00<00:00, 32.76it/s, batch loss=2.742 avg. loss=3.366]\n",
      "train (epoch #401): 100%|██████████| 184/184 [00:23<00:00,  7.96it/s, batch loss=2.250 avg. loss=3.281]\n",
      "eval (epoch #401): 100%|██████████| 27/27 [00:01<00:00, 15.29it/s, batch loss=2.701 avg. loss=3.261]\n",
      "train (epoch #402): 100%|██████████| 184/184 [00:12<00:00, 15.30it/s, batch loss=1.187 avg. loss=3.225]\n",
      "eval (epoch #402): 100%|██████████| 27/27 [00:00<00:00, 30.67it/s, batch loss=3.591 avg. loss=3.459]\n",
      "train (epoch #403): 100%|██████████| 184/184 [00:18<00:00, 10.16it/s, batch loss=2.935 avg. loss=3.269]\n",
      "eval (epoch #403): 100%|██████████| 27/27 [00:01<00:00, 19.54it/s, batch loss=3.374 avg. loss=3.239]\n",
      "train (epoch #404): 100%|██████████| 184/184 [00:13<00:00, 13.60it/s, batch loss=1.865 avg. loss=3.237]\n",
      "eval (epoch #404): 100%|██████████| 27/27 [00:00<00:00, 31.25it/s, batch loss=3.366 avg. loss=3.212]\n",
      "train (epoch #405): 100%|██████████| 184/184 [00:11<00:00, 16.40it/s, batch loss=3.658 avg. loss=3.228]\n",
      "eval (epoch #405): 100%|██████████| 27/27 [00:00<00:00, 31.66it/s, batch loss=8.447 avg. loss=3.479]\n",
      "train (epoch #406): 100%|██████████| 184/184 [00:11<00:00, 16.22it/s, batch loss=3.365 avg. loss=3.285]\n",
      "eval (epoch #406): 100%|██████████| 27/27 [00:00<00:00, 31.01it/s, batch loss=2.185 avg. loss=3.308]\n",
      "train (epoch #407): 100%|██████████| 184/184 [00:11<00:00, 16.35it/s, batch loss=3.578 avg. loss=3.246]\n",
      "eval (epoch #407): 100%|██████████| 27/27 [00:00<00:00, 30.95it/s, batch loss=3.225 avg. loss=3.334]\n",
      "train (epoch #408): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=2.204 avg. loss=3.184]\n",
      "eval (epoch #408): 100%|██████████| 27/27 [00:00<00:00, 30.17it/s, batch loss=2.831 avg. loss=3.235]\n",
      "train (epoch #409): 100%|██████████| 184/184 [00:11<00:00, 15.87it/s, batch loss=2.822 avg. loss=3.286]\n",
      "eval (epoch #409): 100%|██████████| 27/27 [00:00<00:00, 31.94it/s, batch loss=2.818 avg. loss=3.580]\n",
      "train (epoch #410): 100%|██████████| 184/184 [00:11<00:00, 16.44it/s, batch loss=3.053 avg. loss=3.280]\n",
      "eval (epoch #410): 100%|██████████| 27/27 [00:00<00:00, 31.74it/s, batch loss=2.382 avg. loss=3.643]\n",
      "train (epoch #411): 100%|██████████| 184/184 [00:11<00:00, 16.45it/s, batch loss=4.747 avg. loss=3.226]\n",
      "eval (epoch #411): 100%|██████████| 27/27 [00:00<00:00, 30.64it/s, batch loss=2.625 avg. loss=3.439]\n",
      "train (epoch #412): 100%|██████████| 184/184 [00:11<00:00, 15.93it/s, batch loss=2.524 avg. loss=3.259]\n",
      "eval (epoch #412): 100%|██████████| 27/27 [00:00<00:00, 30.21it/s, batch loss=5.157 avg. loss=3.358]\n",
      "train (epoch #413): 100%|██████████| 184/184 [00:11<00:00, 16.66it/s, batch loss=5.696 avg. loss=3.576]\n",
      "eval (epoch #413): 100%|██████████| 27/27 [00:00<00:00, 31.77it/s, batch loss=3.638 avg. loss=3.895]\n",
      "train (epoch #414): 100%|██████████| 184/184 [00:11<00:00, 16.44it/s, batch loss=1.684 avg. loss=3.403]\n",
      "eval (epoch #414): 100%|██████████| 27/27 [00:01<00:00, 23.37it/s, batch loss=3.259 avg. loss=3.861]\n",
      "train (epoch #415): 100%|██████████| 184/184 [00:11<00:00, 16.35it/s, batch loss=1.907 avg. loss=3.298]\n",
      "eval (epoch #415): 100%|██████████| 27/27 [00:00<00:00, 31.30it/s, batch loss=3.634 avg. loss=3.620]\n",
      "train (epoch #416): 100%|██████████| 184/184 [00:11<00:00, 16.38it/s, batch loss=2.254 avg. loss=3.216]\n",
      "eval (epoch #416): 100%|██████████| 27/27 [00:00<00:00, 30.24it/s, batch loss=3.790 avg. loss=3.387]\n",
      "train (epoch #417): 100%|██████████| 184/184 [00:11<00:00, 16.07it/s, batch loss=2.573 avg. loss=3.395]\n",
      "eval (epoch #417): 100%|██████████| 27/27 [00:00<00:00, 29.83it/s, batch loss=2.286 avg. loss=3.714]\n",
      "train (epoch #418): 100%|██████████| 184/184 [00:11<00:00, 16.48it/s, batch loss=1.951 avg. loss=3.418]\n",
      "eval (epoch #418): 100%|██████████| 27/27 [00:00<00:00, 31.24it/s, batch loss=3.652 avg. loss=3.476]\n",
      "train (epoch #419): 100%|██████████| 184/184 [00:11<00:00, 16.40it/s, batch loss=3.342 avg. loss=3.301]\n",
      "eval (epoch #419): 100%|██████████| 27/27 [00:00<00:00, 31.62it/s, batch loss=5.035 avg. loss=3.671]\n",
      "train (epoch #420): 100%|██████████| 184/184 [00:11<00:00, 15.93it/s, batch loss=4.730 avg. loss=3.272]\n",
      "eval (epoch #420): 100%|██████████| 27/27 [00:00<00:00, 30.96it/s, batch loss=4.709 avg. loss=3.479]\n",
      "train (epoch #421): 100%|██████████| 184/184 [00:11<00:00, 16.40it/s, batch loss=4.737 avg. loss=3.286]\n",
      "eval (epoch #421): 100%|██████████| 27/27 [00:00<00:00, 31.84it/s, batch loss=5.849 avg. loss=3.919]\n",
      "train (epoch #422): 100%|██████████| 184/184 [00:11<00:00, 16.54it/s, batch loss=2.360 avg. loss=3.287]\n",
      "eval (epoch #422): 100%|██████████| 27/27 [00:00<00:00, 29.44it/s, batch loss=2.353 avg. loss=3.311]\n",
      "train (epoch #423): 100%|██████████| 184/184 [00:11<00:00, 15.73it/s, batch loss=1.994 avg. loss=3.318]\n",
      "eval (epoch #423): 100%|██████████| 27/27 [00:00<00:00, 31.50it/s, batch loss=1.897 avg. loss=3.352]\n",
      "train (epoch #424): 100%|██████████| 184/184 [00:11<00:00, 15.81it/s, batch loss=6.324 avg. loss=3.245]\n",
      "eval (epoch #424): 100%|██████████| 27/27 [00:00<00:00, 31.06it/s, batch loss=7.662 avg. loss=5.043]\n",
      "train (epoch #425): 100%|██████████| 184/184 [00:11<00:00, 16.25it/s, batch loss=2.521 avg. loss=3.337]\n",
      "eval (epoch #425): 100%|██████████| 27/27 [00:00<00:00, 30.40it/s, batch loss=3.013 avg. loss=3.275]\n",
      "train (epoch #426): 100%|██████████| 184/184 [00:11<00:00, 16.19it/s, batch loss=2.505 avg. loss=3.388]\n",
      "eval (epoch #426): 100%|██████████| 27/27 [00:00<00:00, 31.66it/s, batch loss=2.188 avg. loss=3.475]\n",
      "train (epoch #427): 100%|██████████| 184/184 [00:11<00:00, 15.92it/s, batch loss=2.175 avg. loss=3.300]\n",
      "eval (epoch #427): 100%|██████████| 27/27 [00:00<00:00, 30.41it/s, batch loss=4.084 avg. loss=3.785]\n",
      "train (epoch #428): 100%|██████████| 184/184 [00:11<00:00, 16.32it/s, batch loss=3.679 avg. loss=3.244]\n",
      "eval (epoch #428): 100%|██████████| 27/27 [00:00<00:00, 32.09it/s, batch loss=5.669 avg. loss=3.640]\n",
      "train (epoch #429): 100%|██████████| 184/184 [00:11<00:00, 15.97it/s, batch loss=4.962 avg. loss=3.207]\n",
      "eval (epoch #429): 100%|██████████| 27/27 [00:00<00:00, 29.69it/s, batch loss=2.131 avg. loss=3.255]\n",
      "train (epoch #430): 100%|██████████| 184/184 [00:11<00:00, 15.75it/s, batch loss=3.133 avg. loss=3.230]\n",
      "eval (epoch #430): 100%|██████████| 27/27 [00:00<00:00, 29.87it/s, batch loss=5.744 avg. loss=3.314]\n",
      "train (epoch #431): 100%|██████████| 184/184 [00:11<00:00, 16.15it/s, batch loss=2.447 avg. loss=3.207]\n",
      "eval (epoch #431): 100%|██████████| 27/27 [00:00<00:00, 30.88it/s, batch loss=2.313 avg. loss=3.414]\n",
      "train (epoch #432): 100%|██████████| 184/184 [00:11<00:00, 16.53it/s, batch loss=2.047 avg. loss=3.227]\n",
      "eval (epoch #432): 100%|██████████| 27/27 [00:00<00:00, 30.79it/s, batch loss=3.629 avg. loss=3.294]\n",
      "train (epoch #433): 100%|██████████| 184/184 [00:11<00:00, 16.36it/s, batch loss=3.315 avg. loss=3.285]\n",
      "eval (epoch #433): 100%|██████████| 27/27 [00:01<00:00, 23.24it/s, batch loss=2.220 avg. loss=3.312]\n",
      "train (epoch #434): 100%|██████████| 184/184 [00:11<00:00, 16.18it/s, batch loss=2.098 avg. loss=3.201]\n",
      "eval (epoch #434): 100%|██████████| 27/27 [00:00<00:00, 30.53it/s, batch loss=2.665 avg. loss=3.249]\n",
      "train (epoch #435): 100%|██████████| 184/184 [00:11<00:00, 16.26it/s, batch loss=2.163 avg. loss=3.381]\n",
      "eval (epoch #435): 100%|██████████| 27/27 [00:00<00:00, 30.97it/s, batch loss=2.656 avg. loss=3.534]\n",
      "train (epoch #436): 100%|██████████| 184/184 [00:11<00:00, 16.05it/s, batch loss=2.365 avg. loss=3.377]\n",
      "eval (epoch #436): 100%|██████████| 27/27 [00:00<00:00, 30.85it/s, batch loss=2.781 avg. loss=3.251]\n",
      "train (epoch #437): 100%|██████████| 184/184 [00:11<00:00, 15.55it/s, batch loss=6.165 avg. loss=3.227]\n",
      "eval (epoch #437): 100%|██████████| 27/27 [00:00<00:00, 30.77it/s, batch loss=2.277 avg. loss=3.675]\n",
      "train (epoch #438): 100%|██████████| 184/184 [00:12<00:00, 15.30it/s, batch loss=2.931 avg. loss=3.248]\n",
      "eval (epoch #438): 100%|██████████| 27/27 [00:00<00:00, 27.73it/s, batch loss=4.831 avg. loss=3.367]\n",
      "train (epoch #439): 100%|██████████| 184/184 [00:12<00:00, 15.06it/s, batch loss=3.090 avg. loss=3.178]\n",
      "eval (epoch #439): 100%|██████████| 27/27 [00:01<00:00, 21.22it/s, batch loss=2.879 avg. loss=3.396]\n",
      "train (epoch #440): 100%|██████████| 184/184 [00:11<00:00, 15.45it/s, batch loss=2.278 avg. loss=3.166]\n",
      "eval (epoch #440): 100%|██████████| 27/27 [00:01<00:00, 26.56it/s, batch loss=4.103 avg. loss=3.347]\n",
      "train (epoch #441): 100%|██████████| 184/184 [00:12<00:00, 14.35it/s, batch loss=1.749 avg. loss=3.169]\n",
      "eval (epoch #441): 100%|██████████| 27/27 [00:00<00:00, 27.22it/s, batch loss=2.218 avg. loss=3.214]\n",
      "train (epoch #442): 100%|██████████| 184/184 [00:11<00:00, 15.41it/s, batch loss=3.266 avg. loss=3.198]\n",
      "eval (epoch #442): 100%|██████████| 27/27 [00:00<00:00, 28.74it/s, batch loss=5.934 avg. loss=3.932]\n",
      "train (epoch #443): 100%|██████████| 184/184 [00:11<00:00, 15.91it/s, batch loss=4.264 avg. loss=3.247]\n",
      "eval (epoch #443): 100%|██████████| 27/27 [00:00<00:00, 30.17it/s, batch loss=3.583 avg. loss=3.544]\n",
      "train (epoch #444): 100%|██████████| 184/184 [00:11<00:00, 15.57it/s, batch loss=4.902 avg. loss=3.233]\n",
      "eval (epoch #444): 100%|██████████| 27/27 [00:00<00:00, 28.97it/s, batch loss=3.926 avg. loss=4.214]\n",
      "train (epoch #445): 100%|██████████| 184/184 [00:11<00:00, 16.00it/s, batch loss=3.143 avg. loss=3.246]\n",
      "eval (epoch #445): 100%|██████████| 27/27 [00:00<00:00, 30.33it/s, batch loss=2.407 avg. loss=3.766]\n",
      "train (epoch #446): 100%|██████████| 184/184 [00:11<00:00, 16.00it/s, batch loss=3.023 avg. loss=3.240]\n",
      "eval (epoch #446): 100%|██████████| 27/27 [00:01<00:00, 22.68it/s, batch loss=2.870 avg. loss=3.288]\n",
      "train (epoch #447): 100%|██████████| 184/184 [00:11<00:00, 16.08it/s, batch loss=2.957 avg. loss=3.159]\n",
      "eval (epoch #447): 100%|██████████| 27/27 [00:00<00:00, 30.94it/s, batch loss=1.930 avg. loss=3.223]\n",
      "train (epoch #448): 100%|██████████| 184/184 [00:11<00:00, 16.14it/s, batch loss=4.130 avg. loss=3.174]\n",
      "eval (epoch #448): 100%|██████████| 27/27 [00:00<00:00, 30.25it/s, batch loss=5.580 avg. loss=3.452]\n",
      "train (epoch #449): 100%|██████████| 184/184 [00:11<00:00, 15.59it/s, batch loss=2.439 avg. loss=3.210]\n",
      "eval (epoch #449): 100%|██████████| 27/27 [00:00<00:00, 29.88it/s, batch loss=4.410 avg. loss=3.301]\n",
      "train (epoch #450): 100%|██████████| 184/184 [00:11<00:00, 15.95it/s, batch loss=4.163 avg. loss=3.158]\n",
      "eval (epoch #450): 100%|██████████| 27/27 [00:00<00:00, 30.49it/s, batch loss=4.022 avg. loss=3.594]\n",
      "train (epoch #451): 100%|██████████| 184/184 [00:11<00:00, 15.94it/s, batch loss=4.122 avg. loss=3.257]\n",
      "eval (epoch #451): 100%|██████████| 27/27 [00:00<00:00, 30.04it/s, batch loss=2.586 avg. loss=3.267]\n",
      "train (epoch #452): 100%|██████████| 184/184 [00:11<00:00, 15.69it/s, batch loss=4.930 avg. loss=3.428]\n",
      "eval (epoch #452): 100%|██████████| 27/27 [00:00<00:00, 30.45it/s, batch loss=2.207 avg. loss=3.448]\n",
      "train (epoch #453): 100%|██████████| 184/184 [00:11<00:00, 16.11it/s, batch loss=1.872 avg. loss=3.281]\n",
      "eval (epoch #453): 100%|██████████| 27/27 [00:00<00:00, 30.50it/s, batch loss=3.331 avg. loss=3.443]\n",
      "train (epoch #454): 100%|██████████| 184/184 [00:11<00:00, 15.80it/s, batch loss=1.409 avg. loss=3.194]\n",
      "eval (epoch #454): 100%|██████████| 27/27 [00:00<00:00, 28.84it/s, batch loss=4.089 avg. loss=3.376]\n",
      "train (epoch #455): 100%|██████████| 184/184 [00:11<00:00, 15.62it/s, batch loss=3.592 avg. loss=3.193]\n",
      "eval (epoch #455): 100%|██████████| 27/27 [00:00<00:00, 30.39it/s, batch loss=4.240 avg. loss=4.077]\n",
      "train (epoch #456): 100%|██████████| 184/184 [00:11<00:00, 16.13it/s, batch loss=2.194 avg. loss=3.238]\n",
      "eval (epoch #456): 100%|██████████| 27/27 [00:00<00:00, 30.60it/s, batch loss=3.162 avg. loss=3.455]\n",
      "train (epoch #457): 100%|██████████| 184/184 [00:11<00:00, 15.98it/s, batch loss=2.778 avg. loss=3.180]\n",
      "eval (epoch #457): 100%|██████████| 27/27 [00:00<00:00, 29.75it/s, batch loss=3.397 avg. loss=3.463]\n",
      "train (epoch #458): 100%|██████████| 184/184 [00:11<00:00, 15.58it/s, batch loss=2.854 avg. loss=3.223]\n",
      "eval (epoch #458): 100%|██████████| 27/27 [00:00<00:00, 28.60it/s, batch loss=2.850 avg. loss=3.343]\n",
      "train (epoch #459): 100%|██████████| 184/184 [00:11<00:00, 15.90it/s, batch loss=3.535 avg. loss=3.166]\n",
      "eval (epoch #459): 100%|██████████| 27/27 [00:00<00:00, 29.52it/s, batch loss=2.966 avg. loss=3.279]\n",
      "train (epoch #460): 100%|██████████| 184/184 [00:11<00:00, 15.90it/s, batch loss=3.766 avg. loss=3.192]\n",
      "eval (epoch #460): 100%|██████████| 27/27 [00:00<00:00, 29.99it/s, batch loss=3.008 avg. loss=3.175]\n",
      "train (epoch #461): 100%|██████████| 184/184 [00:11<00:00, 15.72it/s, batch loss=2.928 avg. loss=3.232]\n",
      "eval (epoch #461): 100%|██████████| 27/27 [00:00<00:00, 29.59it/s, batch loss=2.864 avg. loss=3.469]\n",
      "train (epoch #462): 100%|██████████| 184/184 [00:11<00:00, 15.44it/s, batch loss=1.933 avg. loss=3.484]\n",
      "eval (epoch #462): 100%|██████████| 27/27 [00:00<00:00, 29.42it/s, batch loss=3.902 avg. loss=3.360]\n",
      "train (epoch #463): 100%|██████████| 184/184 [00:11<00:00, 15.50it/s, batch loss=1.696 avg. loss=3.232]\n",
      "eval (epoch #463): 100%|██████████| 27/27 [00:00<00:00, 28.93it/s, batch loss=2.868 avg. loss=3.516]\n",
      "train (epoch #464): 100%|██████████| 184/184 [00:11<00:00, 15.59it/s, batch loss=1.966 avg. loss=3.264]\n",
      "eval (epoch #464): 100%|██████████| 27/27 [00:00<00:00, 29.87it/s, batch loss=2.658 avg. loss=3.154]\n",
      "train (epoch #465): 100%|██████████| 184/184 [00:12<00:00, 15.25it/s, batch loss=1.613 avg. loss=3.124]\n",
      "eval (epoch #465): 100%|██████████| 27/27 [00:00<00:00, 30.17it/s, batch loss=4.712 avg. loss=3.313]\n",
      "train (epoch #466): 100%|██████████| 184/184 [00:11<00:00, 16.14it/s, batch loss=1.700 avg. loss=3.129]\n",
      "eval (epoch #466): 100%|██████████| 27/27 [00:00<00:00, 30.24it/s, batch loss=3.649 avg. loss=3.470]\n",
      "train (epoch #467): 100%|██████████| 184/184 [00:11<00:00, 15.85it/s, batch loss=2.302 avg. loss=3.163]\n",
      "eval (epoch #467): 100%|██████████| 27/27 [00:01<00:00, 23.05it/s, batch loss=3.496 avg. loss=3.483]\n",
      "train (epoch #468): 100%|██████████| 184/184 [00:11<00:00, 16.13it/s, batch loss=2.487 avg. loss=3.207]\n",
      "eval (epoch #468): 100%|██████████| 27/27 [00:00<00:00, 29.52it/s, batch loss=6.069 avg. loss=3.740]\n",
      "train (epoch #469): 100%|██████████| 184/184 [00:11<00:00, 16.07it/s, batch loss=2.196 avg. loss=3.145]\n",
      "eval (epoch #469): 100%|██████████| 27/27 [00:00<00:00, 30.87it/s, batch loss=8.728 avg. loss=3.445]\n",
      "train (epoch #470): 100%|██████████| 184/184 [00:11<00:00, 15.67it/s, batch loss=1.950 avg. loss=3.134]\n",
      "eval (epoch #470): 100%|██████████| 27/27 [00:00<00:00, 29.99it/s, batch loss=2.391 avg. loss=3.478]\n",
      "train (epoch #471): 100%|██████████| 184/184 [00:11<00:00, 16.20it/s, batch loss=3.235 avg. loss=3.174]\n",
      "eval (epoch #471): 100%|██████████| 27/27 [00:00<00:00, 30.69it/s, batch loss=3.320 avg. loss=3.270]\n",
      "train (epoch #472): 100%|██████████| 184/184 [00:11<00:00, 16.33it/s, batch loss=2.416 avg. loss=3.180]\n",
      "eval (epoch #472): 100%|██████████| 27/27 [00:00<00:00, 30.75it/s, batch loss=4.007 avg. loss=3.545]\n",
      "train (epoch #473): 100%|██████████| 184/184 [00:11<00:00, 15.83it/s, batch loss=2.101 avg. loss=3.146]\n",
      "eval (epoch #473): 100%|██████████| 27/27 [00:00<00:00, 30.57it/s, batch loss=3.360 avg. loss=3.168]\n",
      "train (epoch #474): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=2.436 avg. loss=3.142]\n",
      "eval (epoch #474): 100%|██████████| 27/27 [00:00<00:00, 30.76it/s, batch loss=5.926 avg. loss=3.589]\n",
      "train (epoch #475): 100%|██████████| 184/184 [00:11<00:00, 15.68it/s, batch loss=1.629 avg. loss=3.158]\n",
      "eval (epoch #475): 100%|██████████| 27/27 [00:00<00:00, 28.55it/s, batch loss=5.285 avg. loss=3.277]\n",
      "train (epoch #476): 100%|██████████| 184/184 [00:11<00:00, 15.66it/s, batch loss=2.835 avg. loss=3.120]\n",
      "eval (epoch #476): 100%|██████████| 27/27 [00:00<00:00, 30.07it/s, batch loss=3.588 avg. loss=3.502]\n",
      "train (epoch #477): 100%|██████████| 184/184 [00:11<00:00, 16.19it/s, batch loss=2.643 avg. loss=3.179]\n",
      "eval (epoch #477): 100%|██████████| 27/27 [00:01<00:00, 24.02it/s, batch loss=3.413 avg. loss=3.277]\n",
      "train (epoch #478): 100%|██████████| 184/184 [00:11<00:00, 16.04it/s, batch loss=2.499 avg. loss=3.171]\n",
      "eval (epoch #478): 100%|██████████| 27/27 [00:00<00:00, 30.24it/s, batch loss=4.631 avg. loss=4.660]\n",
      "train (epoch #479): 100%|██████████| 184/184 [00:11<00:00, 15.82it/s, batch loss=2.857 avg. loss=3.228]\n",
      "eval (epoch #479): 100%|██████████| 27/27 [00:00<00:00, 30.82it/s, batch loss=2.545 avg. loss=3.296]\n",
      "train (epoch #480): 100%|██████████| 184/184 [00:11<00:00, 16.23it/s, batch loss=2.330 avg. loss=3.175]\n",
      "eval (epoch #480): 100%|██████████| 27/27 [00:00<00:00, 28.99it/s, batch loss=2.182 avg. loss=3.243]\n",
      "train (epoch #481): 100%|██████████| 184/184 [00:11<00:00, 15.74it/s, batch loss=1.133 avg. loss=3.216]\n",
      "eval (epoch #481): 100%|██████████| 27/27 [00:00<00:00, 29.54it/s, batch loss=4.488 avg. loss=3.624]\n",
      "train (epoch #482): 100%|██████████| 184/184 [00:11<00:00, 16.07it/s, batch loss=4.899 avg. loss=3.183]\n",
      "eval (epoch #482): 100%|██████████| 27/27 [00:00<00:00, 30.26it/s, batch loss=5.454 avg. loss=3.957]\n",
      "train (epoch #483): 100%|██████████| 184/184 [00:11<00:00, 15.74it/s, batch loss=4.168 avg. loss=3.223]\n",
      "eval (epoch #483): 100%|██████████| 27/27 [00:00<00:00, 29.01it/s, batch loss=2.959 avg. loss=3.299]\n",
      "train (epoch #484): 100%|██████████| 184/184 [00:11<00:00, 15.84it/s, batch loss=9.185 avg. loss=3.185]\n",
      "eval (epoch #484): 100%|██████████| 27/27 [00:00<00:00, 29.73it/s, batch loss=5.848 avg. loss=3.564]\n",
      "train (epoch #485): 100%|██████████| 184/184 [00:11<00:00, 16.18it/s, batch loss=3.108 avg. loss=3.425]\n",
      "eval (epoch #485): 100%|██████████| 27/27 [00:01<00:00, 23.05it/s, batch loss=2.587 avg. loss=3.452]\n",
      "train (epoch #486): 100%|██████████| 184/184 [00:11<00:00, 16.03it/s, batch loss=3.132 avg. loss=3.277]\n",
      "eval (epoch #486): 100%|██████████| 27/27 [00:00<00:00, 29.56it/s, batch loss=4.760 avg. loss=3.319]\n",
      "train (epoch #487): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=1.242 avg. loss=3.194]\n",
      "eval (epoch #487): 100%|██████████| 27/27 [00:00<00:00, 31.44it/s, batch loss=4.077 avg. loss=3.397]\n",
      "train (epoch #488): 100%|██████████| 184/184 [00:11<00:00, 15.54it/s, batch loss=2.291 avg. loss=3.193]\n",
      "eval (epoch #488): 100%|██████████| 27/27 [00:00<00:00, 29.50it/s, batch loss=2.021 avg. loss=3.251]\n",
      "train (epoch #489): 100%|██████████| 184/184 [00:11<00:00, 15.93it/s, batch loss=1.799 avg. loss=3.190]\n",
      "eval (epoch #489): 100%|██████████| 27/27 [00:00<00:00, 28.98it/s, batch loss=3.130 avg. loss=3.221]\n",
      "train (epoch #490): 100%|██████████| 184/184 [00:11<00:00, 16.24it/s, batch loss=3.960 avg. loss=3.168]\n",
      "eval (epoch #490): 100%|██████████| 27/27 [00:00<00:00, 29.64it/s, batch loss=2.656 avg. loss=3.279]\n",
      "train (epoch #491): 100%|██████████| 184/184 [00:11<00:00, 15.62it/s, batch loss=3.676 avg. loss=3.156]\n",
      "eval (epoch #491): 100%|██████████| 27/27 [00:00<00:00, 29.90it/s, batch loss=4.004 avg. loss=3.516]\n",
      "train (epoch #492): 100%|██████████| 184/184 [00:11<00:00, 16.20it/s, batch loss=2.090 avg. loss=3.144]\n",
      "eval (epoch #492): 100%|██████████| 27/27 [00:00<00:00, 30.65it/s, batch loss=2.404 avg. loss=3.288]\n",
      "train (epoch #493): 100%|██████████| 184/184 [00:11<00:00, 15.60it/s, batch loss=3.030 avg. loss=3.131]\n",
      "eval (epoch #493): 100%|██████████| 27/27 [00:00<00:00, 27.66it/s, batch loss=3.016 avg. loss=3.324]\n",
      "train (epoch #494): 100%|██████████| 184/184 [00:11<00:00, 16.21it/s, batch loss=2.399 avg. loss=3.104]\n",
      "eval (epoch #494): 100%|██████████| 27/27 [00:00<00:00, 30.70it/s, batch loss=2.634 avg. loss=3.770]\n",
      "train (epoch #495): 100%|██████████| 184/184 [00:11<00:00, 16.19it/s, batch loss=2.280 avg. loss=3.162]\n",
      "eval (epoch #495): 100%|██████████| 27/27 [00:00<00:00, 30.07it/s, batch loss=2.297 avg. loss=3.310]\n",
      "train (epoch #496): 100%|██████████| 184/184 [00:11<00:00, 15.70it/s, batch loss=2.649 avg. loss=3.109]\n",
      "eval (epoch #496): 100%|██████████| 27/27 [00:00<00:00, 30.36it/s, batch loss=4.529 avg. loss=3.259]\n",
      "train (epoch #497): 100%|██████████| 184/184 [00:11<00:00, 16.20it/s, batch loss=1.830 avg. loss=3.125]\n",
      "eval (epoch #497): 100%|██████████| 27/27 [00:00<00:00, 28.48it/s, batch loss=2.279 avg. loss=3.312]\n",
      "train (epoch #498): 100%|██████████| 184/184 [00:11<00:00, 15.79it/s, batch loss=2.478 avg. loss=3.111]\n",
      "eval (epoch #498): 100%|██████████| 27/27 [00:00<00:00, 30.51it/s, batch loss=4.331 avg. loss=4.725]\n",
      "train (epoch #499): 100%|██████████| 184/184 [00:11<00:00, 16.17it/s, batch loss=3.857 avg. loss=3.195]\n",
      "eval (epoch #499): 100%|██████████| 27/27 [00:01<00:00, 23.80it/s, batch loss=2.154 avg. loss=3.274]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):    \n",
    "    train_loss = loop(model, train_loader, epoch)\n",
    "    val_loss = loop(model, val_loader, epoch, evaluation=True)\n",
    "    \n",
    "    # save model \n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(model.state_dict(), \"{}/gcn_model_{}.pt\".format(mydrive, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7LnohCArzNX"
   },
   "source": [
    "Show us a scatter plot of the training and test data, with MSEs labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "HDzoau9Vu27b",
    "outputId": "21f74580-f7c5-4d81-ac27-11de07df4001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f204b474110>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256, collate_fn=collate_graphs)\n",
    "\n",
    "test_pred_list = []\n",
    "test_label_list = []\n",
    "\n",
    "model.eval()\n",
    "for batch in test_loader: \n",
    "    \n",
    "    z, a, N, y = batch \n",
    "    z = z.to(device)\n",
    "    a = a.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    pred = model(z, a, N)\n",
    "    test_pred_list+=pred.detach().cpu().numpy().tolist()\n",
    "    test_label_list+=y.detach().cpu().numpy().tolist()\n",
    "\n",
    "train_pred_list = []\n",
    "train_label_list = []\n",
    "\n",
    "\n",
    "for batch in train_loader: \n",
    "    \n",
    "    z, a, N, y = batch \n",
    "    z = z.to(device)\n",
    "    a = a.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    pred = model(z, a, N)\n",
    "\n",
    "    train_pred_list+=pred.detach().cpu().numpy().tolist()\n",
    "    train_label_list+=y.detach().cpu().numpy().tolist()\n",
    "\n",
    "test_loss = np.power( np.array(test_pred_list) - np.array(test_label_list), 2).mean()\n",
    "train_loss = np.power( np.array(train_pred_list) - np.array(train_label_list), 2).mean()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(test_pred_list, test_label_list, s=3, alpha=0.5, label='Test MSE: {:.4f} Borh^6'.format(test_loss))\n",
    "plt.scatter(train_pred_list, train_label_list, s=3 , alpha=0.5, label='Train MSE: {:.4f} Borh^6'.format(train_loss))\n",
    "plt.ylabel(\"True Value\")\n",
    "plt.xlabel(\"Predicted Value\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pset4_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
