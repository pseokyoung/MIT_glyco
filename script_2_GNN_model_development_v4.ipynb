{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45d8ea4",
   "metadata": {},
   "source": [
    "**What is a graph neural network?**\n",
    "\n",
    "A graph neural network (GNN) is a type of neural network designed to process data represented as a graph. \n",
    "GNNs are used for a variety of tasks such as node classification, link prediction, and graph generation. \n",
    "They are particularly useful for problems that involve analyzing relationships between entities, such as social networks, molecular structures, and transportation networks. GNNs use a combination of neural networks and graph theory to learn features and representations of nodes and edges in a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c07a1d",
   "metadata": {},
   "source": [
    "**A mechanism of GNNs**\n",
    "\n",
    "Graph Neural Networks (GNNs) are a type of neural network designed to process data represented as a graph. They use a combination of neural networks and graph theory to learn features and representations of nodes and edges in a graph.\n",
    "\n",
    "A graph is a collection of nodes (also called vertices) and edges (also called connections) that connect the nodes. Each node in the graph represents an entity and each edge represents a relationship between two entities.\n",
    "\n",
    "The main mechanism of GNNs is the message passing process. In message passing, the model iteratively updates the representation of each node based on the representations of its neighboring nodes. This process is done by passing messages from each node to its neighboring nodes through the edges that connect them.\n",
    "\n",
    "The message passing process can be divided into two main steps:\n",
    "1. Aggregation: In the aggregation step, the model aggregates the information from the neighboring nodes. This is usually done by taking a weighted sum of the representations of the neighboring nodes, where the weights are learned by the model.\n",
    "1. Update: In the update step, the model updates the representation of the current node based on the aggregated information. This is usually done by applying a neural network to the aggregated information.\n",
    "\n",
    "This process is repeated for a fixed number of iterations or until a stopping criterion is met.\n",
    "One of the key components of GNNs is the use of a trainable function called the \"GNN layer\" which is applied to the updated representation of each node, this function is used to update the representation of each node and can be a neural network like MLP, RNN, CNN. The final representation of each node is used as the output of the GNN.\n",
    "\n",
    "GNNs can be applied to various tasks such as node classification, link prediction, and graph generation. They have been widely used in various fields such as natural language processing, computer vision, bioinformatics, and social networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9d86b",
   "metadata": {},
   "source": [
    "**Several types of GNNs**\n",
    "\n",
    "There are several types of Graph Neural Networks (GNNs) that have been developed, each with its own strengths and weaknesses. Some of the most common types of GNNs include:\n",
    "\n",
    "1. Graph Convolutional Networks (GCNs): GCNs use a convolutional architecture to process graph data. They are based on the convolutional neural networks (CNNs) used in image processing, but are adapted to work with graph-structured data. GCNs use a local neighborhood aggregation scheme to update the representation of each node.\n",
    "\n",
    "1. Graph Attention Networks (GATs): GATs use an attention mechanism to weight the contributions of neighboring nodes when updating the representation of each node. This allows the model to focus on the most important neighbors for each node.\n",
    "\n",
    "1. Graph Recurrent Networks (GRNs): GRNs use a recurrent architecture to process graph data. They are based on the recurrent neural networks (RNNs) used in sequential data, but are adapted to work with graph-structured data.\n",
    "\n",
    "1. Graph Auto-Encoders (GAEs): GAEs are neural networks that are designed to learn representations of graphs in a low-dimensional space. They consist of two main components: an encoder that maps the graph to a low-dimensional representation, and a decoder that maps the low-dimensional representation back to the original graph.\n",
    "\n",
    "1. Graph Transformer Networks (GTNs): GTNs are based on the transformer architecture, which is widely used in natural language processing. They use self-attention mechanisms to weight the contributions of neighboring nodes when updating the representation of each node.\n",
    "\n",
    "1. Simplifying GNNs: Simplifying GNNs are designed to reduce the complexity of GNNs, making them more efficient and scalable. Some examples of simplifying GNNs include GraphSAGE, FastGCN, and JK-Net.\n",
    "\n",
    "These are some of the most popular types of GNNs, but there are many other variations and combinations that have been proposed in the literature. The choice of GNN architecture will depend on the specific problem and dataset you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdff43",
   "metadata": {},
   "source": [
    "**Graph Convolutional Networks (GCNs)**\n",
    "\n",
    "Graph Convolutional Networks (GCNs) are one of the most popular types of Graph Neural Networks (GNNs). They were first introduced by Thomas Kipf and Max Welling in 2017 in the paper \"Semi-Supervised Classification with Graph Convolutional Networks\".\n",
    "\n",
    "GCNs are based on the convolutional neural networks (CNNs) used in image processing, but are adapted to work with graph-structured data. They use a local neighborhood aggregation scheme to update the representation of each node and a trainable convolutional kernel to learn the weights.\n",
    "\n",
    "GCNs have been successfully applied to a variety of graph-based tasks such as node classification, link prediction, and graph generation. They have been widely used in various fields such as natural language processing, computer vision, bioinformatics, and social networks.\n",
    "\n",
    "One of the key advantages of GCNs is that they can effectively capture local structural information in the graph, making them well suited for tasks that involve analyzing relationships between entities. They also have a relatively simple architecture and can be trained efficiently on large graphs.\n",
    "\n",
    "Due to their simplicity and good performance, GCNs have become the go-to choice for many researchers working on graph-based problems. It is not the only one but one of the most popular GNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803dea2",
   "metadata": {},
   "source": [
    "**1. Aggregation steps**\n",
    "\n",
    "In Graph Convolutional Networks (GCNs), the aggregation step is used to combine the information from a node's neighborhood in order to update the node's representation. The aggregation step is typically performed using a weighted sum, where the weights are learned during training.\n",
    "\n",
    "The mathematical formulation of the aggregation step in GCNs can be represented as follows:\n",
    "\n",
    "Given a graph $G = (V, E)$ with n nodes and m edges, and a set of node features $X ∈ R^{nxd}$, where d is the number of features for each node.\n",
    "\n",
    "Let A be the adjacency matrix of the graph, where $A_{i,j} = 1$ if there is an edge between node i and j, and 0 otherwise.\n",
    "\n",
    "Let D be the degree matrix of the graph, where $D_{i,i}$ is the degree of node i.\n",
    "\n",
    "The normalized adjacency matrix is defined as:\n",
    "$\\hat{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$\n",
    "\n",
    "The graph convolution is defined as:\n",
    "$H^{(l+1)} = \\sigma(\\hat{A}H^{(l)}W^{(l)})$\n",
    "\n",
    "where $H^(l)$ is the node representations at the l-th layer, $W^(l)$ is the weight matrix of the l-th layer, and $σ$ is the non-linear activation function.\n",
    "\n",
    "The above equation describes the aggregation step in GCNs, where the representations of the node's neighbors are combined with the node's own representation and the learned weight matrix, in order to update the node's representation.\n",
    "\n",
    "In summary, the aggregation step in GCNs is a mathematical operation that combines the information from a node's neighborhood in order to update the node's representation. It is performed using a weighted sum, where the weights are learned during training, and it is typically performed by multiplying the adjacency matrix by the node feature matrix and a learned weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d5149",
   "metadata": {},
   "source": [
    "**2. Update steps**\n",
    "\n",
    "In Graph Convolutional Networks (GCNs), the update step is used to update the representation of each node in the graph, based on the information gathered from the aggregation step. The update step is typically performed using a linear transformation of the aggregated information, where the transformation is learned during training.\n",
    "\n",
    "The mathematical formulation of the update step in GCNs can be represented as follows:\n",
    "\n",
    "Given the aggregation step, which computes the new node features as:\n",
    "$H^{(l+1)} = \\sigma(\\hat{A}H^{(l)}W^{(l)})$\n",
    "\n",
    "Where $H^{(l)}$ is the node representations at the l-th layer, $W^{(l)}$ is the weight matrix of the l-th layer, $\\hat{A}$ is the normalized adjacency matrix, and $\\sigma$ is the non-linear activation function.\n",
    "\n",
    "The update step can be written as:\n",
    "$H^{(l+1)} = H^{(l+1)} + H^{(l)}$\n",
    "\n",
    "This equation simply adds the new node representations computed by the aggregation step to the current node representations. This allows the model to incorporate both the local structural information from the neighborhood and the node's own features.\n",
    "\n",
    "In summary, the update step of GCN is a mathematical operation that updates the representation of each node in the graph based on the information gathered from the aggregation step. It is performed using a linear transformation of the aggregated information, where the transformation is learned during training, and it is typically performed by adding the new node representations computed by the aggregation step to the current node representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d20e8",
   "metadata": {},
   "source": [
    "**Between Keras and Pytorch, which one is more popular for implementing GNN algorithms?**\n",
    "\n",
    "Both Pytorch and Keras are popular deep learning libraries, and both have support for implementing Graph Neural Networks (GNNs). The choice between the two depends on the specific requirements of your project and your personal preferences.\n",
    "\n",
    "Pytorch is a more low-level library and provides more flexibility in terms of customizing the model's architecture and training process. It also has a large community of researchers and developers who contribute to its development and provide support. Pytorch also has a lot of built-in functions for graph processing and various GNNs architectures, which makes it easy to implement GNNs.\n",
    "\n",
    "Keras, on the other hand, is a higher-level library that provides a simpler and more intuitive interface for building and training models. It is also more user-friendly, making it a popular choice for researchers and practitioners who are new to deep learning. Keras also provides support for GNNs through external libraries such as Keras-GCN and keras-gat, but the support is not as extensive as Pytorch.\n",
    "\n",
    "In practice, both libraries are widely used in research and industry, and both have their own advantages and disadvantages. It ultimately comes down to the specific needs of your project and your own experience and preferences. If you are familiar with Pytorch and you need to implement complex GNNs architectures, Pytorch might be the better choice. But if you are new to deep learning and you prefer a more user-friendly interface, Keras might be the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61c7790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T05:55:23.184575Z",
     "start_time": "2023-01-17T05:55:20.008365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project\\MIT_glyco\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "from os import getcwd \n",
    "from os.path import exists\n",
    "\n",
    "print(getcwd()) # current working directory\n",
    "\n",
    "version = 'v4'\n",
    "update = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffffd073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T05:55:23.215465Z",
     "start_time": "2023-01-17T05:55:23.186540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of proteins: 273\n"
     ]
    }
   ],
   "source": [
    "load_name = f'{version}_all_sites_group.csv'\n",
    "all_sites = pd.read_csv(load_name)\n",
    "\n",
    "protein_list = list(all_sites.protein.unique())\n",
    "pass_list = [\"P24622_2\", \"Q91YE8_2\"] #these proteins have positive sites which are out of bound\n",
    "\n",
    "for x in pass_list:\n",
    "    protein_list.remove(x) \n",
    "    \n",
    "print(\"total number of proteins:\", len(protein_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca9ba7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:11:55.056500Z",
     "start_time": "2023-01-17T14:11:49.678674Z"
    }
   },
   "outputs": [],
   "source": [
    "load_path = './protein_dataset'\n",
    "for i, name in enumerate(protein_list):\n",
    "    load_name = f\"{load_path}/{version}_dataset_{name}.csv\"\n",
    "    temp = pd.read_csv(load_name, index_col=0)\n",
    "    temp['protein'] = temp.index.name\n",
    "\n",
    "    if i==0:\n",
    "        dataset = temp\n",
    "    else:\n",
    "        dataset = pd.concat([dataset, temp], axis=0)\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13962f42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:11:55.150956Z",
     "start_time": "2023-01-17T14:11:55.057176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 258653 entries, 0 to 258652\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   #                258653 non-null  int64  \n",
      " 1   SEQ              258653 non-null  object \n",
      " 2   SS               258653 non-null  object \n",
      " 3   ASA              258653 non-null  float64\n",
      " 4   Phi              258653 non-null  float64\n",
      " 5   Psi              258653 non-null  float64\n",
      " 6   Theta(i-1=>i+1)  258653 non-null  float64\n",
      " 7   Tau(i-2=>i+2)    258653 non-null  float64\n",
      " 8   HSE_alpha_up     258653 non-null  float64\n",
      " 9   HSE_alpha_down   258653 non-null  float64\n",
      " 10  P(C)             258653 non-null  float64\n",
      " 11  P(H)             258653 non-null  float64\n",
      " 12  P(E)             258653 non-null  float64\n",
      " 13  flexibility      258051 non-null  float64\n",
      " 14  side_-1          258653 non-null  object \n",
      " 15  side_1           258653 non-null  object \n",
      " 16  side_2           258653 non-null  object \n",
      " 17  side_3           258653 non-null  object \n",
      " 18  side_4           258653 non-null  object \n",
      " 19  side_5           258653 non-null  object \n",
      " 20  nAli             258653 non-null  int64  \n",
      " 21  nPos             258653 non-null  int64  \n",
      " 22  nS/nT            258653 non-null  int64  \n",
      " 23  Proline          258653 non-null  int64  \n",
      " 24  phi_psi          258653 non-null  object \n",
      " 25  positivity       258653 non-null  int64  \n",
      " 26  protein          258653 non-null  object \n",
      "dtypes: float64(11), int64(6), object(10)\n",
      "memory usage: 53.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3cbcb02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:11:55.714493Z",
     "start_time": "2023-01-17T14:11:55.442221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SEQ_A', 'SEQ_C', 'SEQ_D', 'SEQ_E', 'SEQ_F', 'SEQ_G', 'SEQ_H',\n",
       "       'SEQ_I', 'SEQ_K', 'SEQ_L', 'SEQ_M', 'SEQ_N', 'SEQ_P', 'SEQ_Q',\n",
       "       'SEQ_R', 'SEQ_S', 'SEQ_T', 'SEQ_V', 'SEQ_W', 'SEQ_Y', 'nAli_0',\n",
       "       'nAli_1', 'nAli_2', 'nAli_3', 'nPos_0', 'nPos_1', 'nPos_2',\n",
       "       'nPos_3', 'nS/nT_0', 'nS/nT_1', 'nS/nT_2', 'nS/nT_3', 'nS/nT_4',\n",
       "       'nS/nT_5', 'nS/nT_6', 'nS/nT_7', 'nS/nT_8', 'nS/nT_9', 'nS/nT_10',\n",
       "       'nS/nT_11', 'nS/nT_12', 'nS/nT_13', 'nS/nT_14', 'nS/nT_15',\n",
       "       'nS/nT_16', 'nS/nT_17', 'nS/nT_18', 'nS/nT_19', 'nS/nT_20',\n",
       "       'nS/nT_21', 'SS_C', 'SS_E', 'SS_H', 'phi_psi_alpha',\n",
       "       'phi_psi_beta', 'phi_psi_other', 'side_-1_None', 'side_-1_cycle',\n",
       "       'side_-1_gly', 'side_-1_long', 'side_-1_normal', 'side_-1_pro',\n",
       "       'side_-1_small', 'side_-1_very_small', 'side_1_None',\n",
       "       'side_1_cycle', 'side_1_gly', 'side_1_long', 'side_1_normal',\n",
       "       'side_1_pro', 'side_1_small', 'side_1_very_small', 'side_2_None',\n",
       "       'side_2_cycle', 'side_2_gly', 'side_2_long', 'side_2_normal',\n",
       "       'side_2_pro', 'side_2_small', 'side_2_very_small', 'side_3_None',\n",
       "       'side_3_cycle', 'side_3_gly', 'side_3_long', 'side_3_normal',\n",
       "       'side_3_pro', 'side_3_small', 'side_3_very_small', 'side_4_None',\n",
       "       'side_4_cycle', 'side_4_gly', 'side_4_long', 'side_4_normal',\n",
       "       'side_4_pro', 'side_4_small', 'side_4_very_small', 'side_5_None',\n",
       "       'side_5_cycle', 'side_5_gly', 'side_5_long', 'side_5_normal',\n",
       "       'side_5_pro', 'side_5_small', 'side_5_very_small', 'positivity_0',\n",
       "       'positivity_1'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "header = ['#', 'protein']\n",
    "x_cts = ['flexibility', 'Proline']\n",
    "x_cat = ['SEQ', 'nAli', 'nPos', 'nS/nT', 'SS', 'phi_psi', \n",
    "         'side_-1', 'side_1', 'side_2', 'side_3', 'side_4', 'side_5']\n",
    "y = ['positivity']\n",
    "\n",
    "dummies = pd.get_dummies(dataset[x_cat+y], columns=x_cat+y)\n",
    "display(np.array(dummies.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1c12d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:11:57.146353Z",
     "start_time": "2023-01-17T14:11:57.101469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['#', 'protein', 'flexibility', 'Proline', 'SEQ_A', 'SEQ_C',\n",
       "       'SEQ_D', 'SEQ_E', 'SEQ_F', 'SEQ_G', 'SEQ_H', 'SEQ_I', 'SEQ_K',\n",
       "       'SEQ_L', 'SEQ_M', 'SEQ_N', 'SEQ_P', 'SEQ_Q', 'SEQ_R', 'SEQ_S',\n",
       "       'SEQ_T', 'SEQ_V', 'SEQ_W', 'SEQ_Y', 'nAli_0', 'nAli_1', 'nAli_2',\n",
       "       'nAli_3', 'nPos_0', 'nPos_1', 'nPos_2', 'nPos_3', 'nS/nT_0',\n",
       "       'nS/nT_1', 'nS/nT_2', 'nS/nT_3', 'nS/nT_4', 'nS/nT_5', 'nS/nT_6',\n",
       "       'nS/nT_7', 'nS/nT_8', 'nS/nT_9', 'nS/nT_10', 'nS/nT_11',\n",
       "       'nS/nT_12', 'nS/nT_13', 'nS/nT_14', 'nS/nT_15', 'nS/nT_16',\n",
       "       'nS/nT_17', 'nS/nT_18', 'nS/nT_19', 'nS/nT_20', 'nS/nT_21', 'SS_C',\n",
       "       'SS_E', 'SS_H', 'phi_psi_alpha', 'phi_psi_beta', 'phi_psi_other',\n",
       "       'side_-1_None', 'side_-1_cycle', 'side_-1_gly', 'side_-1_long',\n",
       "       'side_-1_normal', 'side_-1_pro', 'side_-1_small',\n",
       "       'side_-1_very_small', 'side_1_None', 'side_1_cycle', 'side_1_gly',\n",
       "       'side_1_long', 'side_1_normal', 'side_1_pro', 'side_1_small',\n",
       "       'side_1_very_small', 'side_2_None', 'side_2_cycle', 'side_2_gly',\n",
       "       'side_2_long', 'side_2_normal', 'side_2_pro', 'side_2_small',\n",
       "       'side_2_very_small', 'side_3_None', 'side_3_cycle', 'side_3_gly',\n",
       "       'side_3_long', 'side_3_normal', 'side_3_pro', 'side_3_small',\n",
       "       'side_3_very_small', 'side_4_None', 'side_4_cycle', 'side_4_gly',\n",
       "       'side_4_long', 'side_4_normal', 'side_4_pro', 'side_4_small',\n",
       "       'side_4_very_small', 'side_5_None', 'side_5_cycle', 'side_5_gly',\n",
       "       'side_5_long', 'side_5_normal', 'side_5_pro', 'side_5_small',\n",
       "       'side_5_very_small', 'positivity_0', 'positivity_1'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_onehot = pd.concat([dataset[header], dataset[x_cts], dummies], axis=1)\n",
    "display(np.array(dataset_onehot.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86d77e2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:15:56.431835Z",
     "start_time": "2023-01-17T14:15:32.064844Z"
    }
   },
   "outputs": [],
   "source": [
    "### generate GNN dataset###\n",
    "\n",
    "win_size = 10\n",
    "mat_size = 2*win_size+1\n",
    "feat_size = dataset_onehot.shape[1]-4\n",
    "name_list = []\n",
    "adj_list  = []\n",
    "feat_list = []\n",
    "label_list = []\n",
    "for name in protein_list:\n",
    "    data = dataset_onehot[dataset_onehot['protein']==name].reset_index(drop=True)\n",
    "    ST_index = np.where((data['SEQ_S']==1) | (data['SEQ_T']==1))[0]\n",
    "    for index in ST_index:\n",
    "        start_index = min(max(index-win_size, 0), len(data))\n",
    "        end_index   = max(min(index+win_size+1, len(data)), 0)\n",
    "        \n",
    "        zero_matrix = np.zeros((mat_size,mat_size))\n",
    "        adj_matrix = np.eye(mat_size, k=-1) + np.eye(mat_size) + np.eye(mat_size, k=1)\n",
    "        feat_matrix = np.zeros((mat_size,feat_size))\n",
    "        \n",
    "        down_lim = index - win_size\n",
    "        up_lim = index + win_size\n",
    "        if down_lim < 0:\n",
    "            adj_matrix[:-down_lim, :-down_lim] = zero_matrix[:-down_lim, :-down_lim]\n",
    "            feat_matrix[-down_lim:] = data.iloc[start_index:end_index, 2:-2].values\n",
    "        elif up_lim > len(data)-1:\n",
    "            adj_matrix[len(data)-up_lim-1:, len(data)-up_lim-1:] = zero_matrix[len(data)-up_lim-1:, len(data)-up_lim-1:]\n",
    "            feat_matrix[:len(data)-up_lim-1] = data.iloc[start_index:end_index, 2:-2].values\n",
    "        else:\n",
    "            feat_matrix = data.iloc[start_index:end_index, 2:-2].values\n",
    "        label = data.iloc[[index], -2:].values\n",
    "\n",
    "        name_list.append(name)\n",
    "        adj_list.append(adj_matrix)\n",
    "        feat_list.append(feat_matrix)\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f03d726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:19:35.412157Z",
     "start_time": "2023-01-17T14:19:35.384232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41432, 1, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(label_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74ce2b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:27:09.919819Z",
     "start_time": "2023-01-17T13:27:09.907850Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6153abe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:22:30.541165Z",
     "start_time": "2023-01-17T14:22:30.159845Z"
    }
   },
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True\n",
    "\n",
    "class GCNDataset(Dataset):\n",
    "    def __init__(self, list_feature, list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "    \n",
    "def partition(list_feature, list_adj, list_logP, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    logP_val = list_logP[num_train:num_train + num_val]\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    logP_test = list_logP[num_total - num_test:]\n",
    "        \n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "\n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition\n",
    "\n",
    "dict_partition = partition(np.array(feat_list), np.array(adj_list), np.array(label_list), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eabb8ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:27:12.048152Z",
     "start_time": "2023-01-17T13:27:12.028205Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9f39237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:22:56.404590Z",
     "start_time": "2023-01-17T14:22:56.389598Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_amino, act=None, bn=False):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_amino)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a43cb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:34:04.727085Z",
     "start_time": "2023-01-17T14:34:04.714117Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        list_feature, list_adj, list_label = data\n",
    "        list_feature = list_feature.cuda().float()\n",
    "        list_adj = list_adj.cuda().float()\n",
    "        list_label = list_label.cuda().float().view(-1, 1)\n",
    "        outputs = net(list_feature, list_adj)\n",
    "\n",
    "        loss = criterion(outputs, list_label)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return net, train_loss\n",
    "\n",
    "\n",
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            list_feature, list_adj, list_label = data\n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_label = list_label.cuda().float().view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "    return val_loss\n",
    "\n",
    "# def test(net, partition, args):\n",
    "#     testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "#                                              batch_size=args.test_batch_size, \n",
    "#                                              shuffle=False, num_workers=2)\n",
    "#     net.eval()\n",
    "#     with torch.no_grad():\n",
    "#         logP_total = list()\n",
    "#         pred_logP_total = list()\n",
    "#         for data in testloader:\n",
    "#             list_feature, list_adj, list_label = data\n",
    "#             list_feature = list_feature.cuda().float()\n",
    "#             list_adj = list_adj.cuda().float()\n",
    "#             list_label = list_label.cuda().float()\n",
    "#             label_total += list_label.tolist()\n",
    "#             list_label = list_label.view(-1, 1)\n",
    "            \n",
    "#             outputs = net(list_feature, list_adj)\n",
    "#             pred_label_total += outputs.view(-1).tolist()\n",
    "\n",
    "#         mae = mean_absolute_error(label_total, pred_label_total)\n",
    "#         std = np.std(np.array(label_total)-np.array(pred_label_total))\n",
    "    \n",
    "#     return mae, std, label_total, pred_label_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67c254bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T14:35:13.196918Z",
     "start_time": "2023-01-17T14:35:13.179943Z"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "  \n",
    "    net = GCNNet(args)\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "#     mae, std, logP_total, pred_logP_total = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['mae'] = mae\n",
    "    result['std'] = std\n",
    "#     result['logP_total'] = logP_total\n",
    "#     result['pred_logP_total'] = pred_logP_total\n",
    "    return vars(args), result\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15898ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dual Attention",
   "language": "python",
   "name": "dualattn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
