{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceeebd00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T04:35:55.814497Z",
     "start_time": "2023-01-21T04:35:55.246326Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from os import getcwd \n",
    "from os.path import exists\n",
    "\n",
    "getcwd() # current working directory\n",
    "\n",
    "version = 'v4'\n",
    "update = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92892710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T04:35:57.167941Z",
     "start_time": "2023-01-21T04:35:55.816683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of proteins:       272\n",
      "total number of samples:        257578\n",
      "total number of positive sites: 529\n",
      "total number of negative sites: 257049\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>SEQ</th>\n",
       "      <th>SS</th>\n",
       "      <th>ASA</th>\n",
       "      <th>Phi</th>\n",
       "      <th>Psi</th>\n",
       "      <th>Theta(i-1=&gt;i+1)</th>\n",
       "      <th>Tau(i-2=&gt;i+2)</th>\n",
       "      <th>HSE_alpha_up</th>\n",
       "      <th>HSE_alpha_down</th>\n",
       "      <th>...</th>\n",
       "      <th>side_3</th>\n",
       "      <th>side_4</th>\n",
       "      <th>side_5</th>\n",
       "      <th>nAli</th>\n",
       "      <th>nPos</th>\n",
       "      <th>nS/nT</th>\n",
       "      <th>Proline</th>\n",
       "      <th>phi_psi</th>\n",
       "      <th>positivity</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>112.7</td>\n",
       "      <td>-100.9</td>\n",
       "      <td>139.3</td>\n",
       "      <td>119.5</td>\n",
       "      <td>165.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>...</td>\n",
       "      <td>pro</td>\n",
       "      <td>cycle</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>A2ABU4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>103.3</td>\n",
       "      <td>-102.0</td>\n",
       "      <td>132.1</td>\n",
       "      <td>117.6</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>13.9</td>\n",
       "      <td>...</td>\n",
       "      <td>cycle</td>\n",
       "      <td>small</td>\n",
       "      <td>pro</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>A2ABU4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>C</td>\n",
       "      <td>50.9</td>\n",
       "      <td>-97.8</td>\n",
       "      <td>134.7</td>\n",
       "      <td>118.5</td>\n",
       "      <td>-149.2</td>\n",
       "      <td>16.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>...</td>\n",
       "      <td>small</td>\n",
       "      <td>pro</td>\n",
       "      <td>gly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>A2ABU4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>P</td>\n",
       "      <td>C</td>\n",
       "      <td>77.2</td>\n",
       "      <td>-69.2</td>\n",
       "      <td>144.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-105.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>16.7</td>\n",
       "      <td>...</td>\n",
       "      <td>pro</td>\n",
       "      <td>gly</td>\n",
       "      <td>small</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>A2ABU4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>80.3</td>\n",
       "      <td>-95.4</td>\n",
       "      <td>141.5</td>\n",
       "      <td>118.6</td>\n",
       "      <td>-135.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>...</td>\n",
       "      <td>gly</td>\n",
       "      <td>small</td>\n",
       "      <td>very_small</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>A2ABU4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257573</th>\n",
       "      <td>2892</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>110.1</td>\n",
       "      <td>-95.9</td>\n",
       "      <td>129.5</td>\n",
       "      <td>114.1</td>\n",
       "      <td>-151.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>...</td>\n",
       "      <td>long</td>\n",
       "      <td>small</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9Y520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257574</th>\n",
       "      <td>2893</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>129.9</td>\n",
       "      <td>-89.6</td>\n",
       "      <td>124.2</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-140.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>small</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9Y520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257575</th>\n",
       "      <td>2894</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>106.7</td>\n",
       "      <td>-91.4</td>\n",
       "      <td>100.3</td>\n",
       "      <td>109.1</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>9.1</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9Y520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257576</th>\n",
       "      <td>2895</td>\n",
       "      <td>K</td>\n",
       "      <td>C</td>\n",
       "      <td>160.4</td>\n",
       "      <td>-89.3</td>\n",
       "      <td>66.8</td>\n",
       "      <td>107.1</td>\n",
       "      <td>136.5</td>\n",
       "      <td>2.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9Y520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257577</th>\n",
       "      <td>2896</td>\n",
       "      <td>S</td>\n",
       "      <td>C</td>\n",
       "      <td>98.4</td>\n",
       "      <td>-91.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>Q9Y520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257578 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           # SEQ SS    ASA    Phi    Psi  Theta(i-1=>i+1)  Tau(i-2=>i+2)  \\\n",
       "0          1   M  C  112.7 -100.9  139.3            119.5          165.0   \n",
       "1          2   T  C  103.3 -102.0  132.1            117.6         -150.0   \n",
       "2          3   L  C   50.9  -97.8  134.7            118.5         -149.2   \n",
       "3          4   P  C   77.2  -69.2  144.0            111.0         -105.3   \n",
       "4          5   H  C   80.3  -95.4  141.5            118.6         -135.6   \n",
       "...      ...  .. ..    ...    ...    ...              ...            ...   \n",
       "257573  2892   E  C  110.1  -95.9  129.5            114.1         -151.5   \n",
       "257574  2893   E  C  129.9  -89.6  124.2            111.0         -140.1   \n",
       "257575  2894   T  C  106.7  -91.4  100.3            109.1         -163.0   \n",
       "257576  2895   K  C  160.4  -89.3   66.8            107.1          136.5   \n",
       "257577  2896   S  C   98.4  -91.9  113.0            110.0          100.4   \n",
       "\n",
       "        HSE_alpha_up  HSE_alpha_down  ...  side_3  side_4      side_5  nAli  \\\n",
       "0                8.5             9.1  ...     pro   cycle       small     0   \n",
       "1                3.8            13.9  ...   cycle   small         pro     0   \n",
       "2               16.4            11.6  ...   small     pro         gly     0   \n",
       "3                7.5            16.7  ...     pro     gly       small     1   \n",
       "4               13.3            13.3  ...     gly   small  very_small     2   \n",
       "...              ...             ...  ...     ...     ...         ...   ...   \n",
       "257573           8.1            12.4  ...    long   small        None     1   \n",
       "257574           5.4            11.0  ...   small    None        None     0   \n",
       "257575           3.1             9.1  ...    None    None        None     0   \n",
       "257576           2.7             7.7  ...    None    None        None     0   \n",
       "257577           2.1             6.2  ...    None    None        None     0   \n",
       "\n",
       "       nPos nS/nT Proline phi_psi positivity protein  \n",
       "0         0     3       0   alpha          0  A2ABU4  \n",
       "1         0     3       0   alpha          0  A2ABU4  \n",
       "2         0     3       1   alpha          0  A2ABU4  \n",
       "3         0     3       0   alpha          0  A2ABU4  \n",
       "4         0     3       0   alpha          0  A2ABU4  \n",
       "...     ...   ...     ...     ...        ...     ...  \n",
       "257573    1     3       0   alpha          0  Q9Y520  \n",
       "257574    0     3       0   alpha          0  Q9Y520  \n",
       "257575    0     3       0   alpha          0  Q9Y520  \n",
       "257576    1     3       0   other          0  Q9Y520  \n",
       "257577    1     3       0   alpha          0  Q9Y520  \n",
       "\n",
       "[257578 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_name = \"v4_data_all_sites.csv\"\n",
    "dataset = pd.read_csv(load_name)\n",
    "\n",
    "positive_sites = dataset[dataset['positivity']==1]\n",
    "negative_sites = dataset[dataset['positivity']==0]\n",
    "\n",
    "print(\"total number of proteins:      \", len(dataset.protein.unique()))\n",
    "print(\"total number of samples:       \", len(dataset))\n",
    "print(\"total number of positive sites:\", len(positive_sites))\n",
    "print(\"total number of negative sites:\", len(negative_sites))\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb592ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T04:35:57.479603Z",
     "start_time": "2023-01-21T04:35:57.170007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 257578 entries, 0 to 257577\n",
      "Data columns (total 27 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   #                257578 non-null  int64  \n",
      " 1   SEQ              257578 non-null  object \n",
      " 2   SS               257578 non-null  object \n",
      " 3   ASA              257578 non-null  float64\n",
      " 4   Phi              257578 non-null  float64\n",
      " 5   Psi              257578 non-null  float64\n",
      " 6   Theta(i-1=>i+1)  257578 non-null  float64\n",
      " 7   Tau(i-2=>i+2)    257578 non-null  float64\n",
      " 8   HSE_alpha_up     257578 non-null  float64\n",
      " 9   HSE_alpha_down   257578 non-null  float64\n",
      " 10  P(C)             257578 non-null  float64\n",
      " 11  P(H)             257578 non-null  float64\n",
      " 12  P(E)             257578 non-null  float64\n",
      " 13  flexibility      257578 non-null  float64\n",
      " 14  side_-1          257578 non-null  object \n",
      " 15  side_1           257578 non-null  object \n",
      " 16  side_2           257578 non-null  object \n",
      " 17  side_3           257578 non-null  object \n",
      " 18  side_4           257578 non-null  object \n",
      " 19  side_5           257578 non-null  object \n",
      " 20  nAli             257578 non-null  int64  \n",
      " 21  nPos             257578 non-null  int64  \n",
      " 22  nS/nT            257578 non-null  int64  \n",
      " 23  Proline          257578 non-null  int64  \n",
      " 24  phi_psi          257578 non-null  object \n",
      " 25  positivity       257578 non-null  int64  \n",
      " 26  protein          257578 non-null  object \n",
      "dtypes: float64(11), int64(6), object(10)\n",
      "memory usage: 53.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8ec9e",
   "metadata": {},
   "source": [
    "## case 1: without window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d7578e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T04:35:57.650977Z",
     "start_time": "2023-01-21T04:35:57.481603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257578, 34)\n",
      "(257578, 1)\n",
      "\n",
      "x columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                 ASA\n",
       "1                 Phi\n",
       "2                 Psi\n",
       "3     Theta(i-1=>i+1)\n",
       "4       Tau(i-2=>i+2)\n",
       "5        HSE_alpha_up\n",
       "6      HSE_alpha_down\n",
       "7                P(C)\n",
       "8                P(H)\n",
       "9                P(E)\n",
       "10        flexibility\n",
       "11              SEQ_A\n",
       "12              SEQ_C\n",
       "13              SEQ_D\n",
       "14              SEQ_E\n",
       "15              SEQ_F\n",
       "16              SEQ_G\n",
       "17              SEQ_H\n",
       "18              SEQ_I\n",
       "19              SEQ_K\n",
       "20              SEQ_L\n",
       "21              SEQ_M\n",
       "22              SEQ_N\n",
       "23              SEQ_P\n",
       "24              SEQ_Q\n",
       "25              SEQ_R\n",
       "26              SEQ_S\n",
       "27              SEQ_T\n",
       "28              SEQ_V\n",
       "29              SEQ_W\n",
       "30              SEQ_Y\n",
       "31               SS_C\n",
       "32               SS_E\n",
       "33               SS_H\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cat = ['SEQ', 'SS']\n",
    "x_cts = ['ASA', 'Phi', 'Psi', 'Theta(i-1=>i+1)', 'Tau(i-2=>i+2)', 'HSE_alpha_up', 'HSE_alpha_down', \n",
    "         'P(C)', 'P(H)', 'P(E)', 'flexibility']\n",
    "y_label = ['positivity']\n",
    "\n",
    "data_x = pd.get_dummies(dataset[x_cts+x_cat], columns=x_cat)\n",
    "data_y = dataset[y_label]\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)\n",
    "\n",
    "print(\"\\nx columns:\")\n",
    "display(pd.Series(data_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "630c2cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:00:28.275098Z",
     "start_time": "2023-01-21T05:00:18.806688Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = data_x\n",
    "output_data = data_y\n",
    "window_size = 10\n",
    "window_len  = 2 * window_size + 1\n",
    "\n",
    "protein_list = list(dataset.protein.unique())\n",
    "rnn_input = []\n",
    "rnn_output = []\n",
    "for name in protein_list:\n",
    "    data = dataset[dataset['protein']==name]\n",
    "    low_bound = data.index[0]\n",
    "    up_bound  = data.index[-1]\n",
    "    ST_idx = np.where((data['SEQ']=='S')|(data['SEQ']=='T'))[0] + low_bound\n",
    "    \n",
    "    for idx in ST_idx:\n",
    "        start_idx = idx - window_size\n",
    "        end_idx   = idx + window_size + 1\n",
    "        \n",
    "#         print(f\"{name}, {low_bound}, {up_bound}, {start_idx}, {end_idx}\")\n",
    "        if start_idx < low_bound:\n",
    "            zeros = np.zeros((low_bound-start_idx,input_data.shape[1]))\n",
    "            temp  = input_data.iloc[low_bound:end_idx].values\n",
    "            temp  = np.concatenate([zeros, temp], axis=0)\n",
    "            \n",
    "        elif end_idx > up_bound + 1:\n",
    "            zeros = np.zeros((end_idx-up_bound-1,input_data.shape[1]))\n",
    "            temp  = input_data.iloc[start_idx:up_bound+1].values\n",
    "            temp  = np.concatenate([temp, zeros], axis=0)\n",
    "            \n",
    "        else:\n",
    "            temp  = input_data.iloc[start_idx:end_idx].values\n",
    "            \n",
    "        rnn_input.append(temp)\n",
    "        rnn_output.append(output_data.iloc[idx].values)\n",
    "        \n",
    "rnn_input = np.array(rnn_input)\n",
    "rnn_output = np.array(rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8abb4e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:00:28.285383Z",
     "start_time": "2023-01-21T05:00:28.275670Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def stratified_split(data_x, data_y, test_size=0.2, n_splits=1, random_state=1, dtype='arr'):\n",
    "\n",
    "    split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    train_x, train_y, test_x, test_y  = [], [], [], []\n",
    "    if dtype=='df':\n",
    "        data_x = data_x.values\n",
    "        data_y = data_y.values\n",
    "    \n",
    "    for train_index, test_index in split.split(data_x, data_y):\n",
    "        train_x.append(data_x[train_index])\n",
    "        train_y.append(data_y[train_index])\n",
    "\n",
    "        test_x.append(data_x[test_index])\n",
    "        test_y.append(data_y[test_index])\n",
    "        \n",
    "    print(\"train/test dataset\")\n",
    "    print(\"train:\", train_x[0].shape, train_y[0].shape)\n",
    "    print(\"test:\", test_x[0].shape, test_y[0].shape)\n",
    "    \n",
    "    if n_splits == 1:\n",
    "        return train_x[0],train_y[0], test_x[0], test_y[0]\n",
    "    else:\n",
    "        return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1df0358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:04:39.108405Z",
     "start_time": "2023-01-21T05:04:36.370406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "2th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.001001001001001\n",
      "3th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "4th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "5th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0006653359946773\n",
      "6th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "7th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "8th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "9th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "10th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "### split data into train/test dataset ###\n",
    "test_size = 0.2\n",
    "n_splits = 10\n",
    "random_state = 1\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "arr_x, arr_y = rnn_input, rnn_output # convert dataframe to nd-array \n",
    "\n",
    "i=1\n",
    "train_idx_list, train_x_list, train_y_list, test_idx_list, test_x_list, test_y_list = [], [], [], [], [], []\n",
    "for train_index, test_index in split.split(arr_x, arr_y):\n",
    "    train_x = arr_x[train_index]\n",
    "    train_y = arr_y[train_index]\n",
    "    test_x = arr_x[test_index]\n",
    "    test_y = arr_y[test_index]\n",
    "    \n",
    "    train_cts = train_x[:,:,:len(x_cts)]\n",
    "    test_cts  = test_x[:,:,:len(x_cts)]\n",
    "    \n",
    "    x_min = train_cts.min(0).min(0)\n",
    "    x_max = train_cts.max(0).max(0)\n",
    "    \n",
    "    train_x[:,:,:len(x_cts)] = (train_cts-x_min)/(x_max-x_min)\n",
    "    test_x[:,:,:len(x_cts)] = (test_cts-x_min)/(x_max-x_min)\n",
    "    \n",
    "    print(f\"{i}th iteration\")\n",
    "    print(\"train:\", train_x.shape, train_y.shape, \"check scale:\", train_x.min(), train_x.max())\n",
    "    print(\"test: \", test_x.shape, test_y.shape, \"check scale:\", test_x.min(), test_x.max())\n",
    "    \n",
    "    train_idx_list.append(train_index)\n",
    "    train_x_list.append(train_x)\n",
    "    train_y_list.append(train_y)\n",
    "    \n",
    "    test_idx_list.append(test_index)\n",
    "    test_x_list.append(test_x)\n",
    "    test_y_list.append(test_y)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52d731b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:04:54.069081Z",
     "start_time": "2023-01-21T05:04:50.100081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up-sampled train dataset: (65188, 21, 34) (65188, 1)\n",
      "test dataset: (8253, 21, 34) (8253, 1)\n"
     ]
    }
   ],
   "source": [
    "## upsampling dataset \n",
    "import random\n",
    "random_state = random_state\n",
    "\n",
    "upsample_x_list, upsample_y_list = [], []\n",
    "for train_x, train_y in zip(train_x_list, train_y_list):\n",
    "    index_pos = np.where(train_y == 1)[0]\n",
    "    index_neg = np.where(train_y == 0)[0]\n",
    "\n",
    "    random.seed(random_state)\n",
    "    up_index = [random.choice(index_pos) for _ in range(len(index_neg))] # get samples from positive sites as much as the number of negative sites\n",
    "\n",
    "    upsample_pos_x = train_x[up_index]\n",
    "    upsample_pos_y = train_y[up_index]\n",
    "    sample_neg_x = train_x[index_neg]\n",
    "    sample_neg_y = train_y[index_neg]\n",
    "\n",
    "    sample_x = np.concatenate([upsample_pos_x, sample_neg_x], axis=0)\n",
    "    sample_y = np.concatenate([upsample_pos_y, sample_neg_y], axis=0)\n",
    "\n",
    "    shuffle_index = np.arange(len(sample_x))\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    sample_x = sample_x[shuffle_index]\n",
    "    sample_y = sample_y[shuffle_index]\n",
    "    \n",
    "    upsample_x_list.append(sample_x)\n",
    "    upsample_y_list.append(sample_y)\n",
    "\n",
    "print(\"up-sampled train dataset:\", sample_x.shape, sample_y.shape)\n",
    "print(\"test dataset:\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74b1ea4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:19:59.721436Z",
     "start_time": "2023-01-21T05:19:59.703081Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def ClassifierLSTM(\n",
    "    history_size, history_dim, class_num,\n",
    "    rnn_layers = 1, rnn_neurons = 100,\n",
    "    dense_layers = 1, dense_neurons = 100,\n",
    "    optimizer = Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.999), loss=\"binary_crossentropy\", metrics = ['accuracy']\n",
    "):\n",
    "    encoder_input = Input(shape=(history_size, history_dim), name='input_encoder')\n",
    "    \n",
    "    # encoder module\n",
    "    if rnn_layers == 1:\n",
    "        encoder_output, state_h, state_c = LSTM(rnn_neurons, return_state=True, name='encoder_last')(encoder_input)\n",
    "        # encoder_states = [state_h, state_c]\n",
    "        \n",
    "    else:\n",
    "        for i in range(rnn_layers):\n",
    "            #first encoder layer\n",
    "            if i==0: \n",
    "                encoder_output = LSTM(rnn_neurons, return_sequences=True, name=\"encoder_1\")(encoder_input)\n",
    "            #mediate encoder layer\n",
    "            elif i < rnn_layers-1: \n",
    "                encoder_output = LSTM(rnn_neurons, return_sequences=True, name=f\"encoder_{i+1}\")(encoder_output)\n",
    "            #last encoder layer\n",
    "            else: \n",
    "                encoder_output, state_h, state_c  = LSTM(rnn_neurons, return_state=True, name=f\"encoder_last\")(encoder_output)\n",
    "                # encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # dense module\n",
    "    if dense_layers == 1:\n",
    "        dense_output = Dense(dense_neurons, name='dense_1')(encoder_output)\n",
    "    else:\n",
    "        for i in range(dense_layers):\n",
    "            #first dense layer\n",
    "            \n",
    "            if i==0:\n",
    "                dense_output = Dense(dense_neurons, name='dense_1')(encoder_output)\n",
    "            #mediate encoder layer\n",
    "            else:\n",
    "                dense_output = Dense(dense_neurons, name=f'dense_{i+1}')(dense_output)\n",
    "    dense_output = Dense(class_num, activation='sigmoid', name=f'dense_last')(dense_output)  \n",
    "    \n",
    "    # model compile\n",
    "    model = Model(encoder_input, dense_output)\n",
    "    model.compile(loss=loss,optimizer = optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9784712e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:20:01.727125Z",
     "start_time": "2023-01-21T05:20:00.360747Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from seaborn import heatmap\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def scores(y_real, y_pred, rounding=4):\n",
    "    \n",
    "    accuracy  = 100*np.array(accuracy_score(y_real, y_pred)).round(rounding)\n",
    "    precision = 100*np.array(precision_score(y_real, y_pred, average=None)).round(rounding)[1]\n",
    "    recall    = 100*np.array(recall_score(y_real, y_pred, average=None)).round(rounding)[1]\n",
    "    f1        = 100*np.array(f1_score(y_real, y_pred, average=None)).round(rounding)[1]\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5fedc0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:37:50.973971Z",
     "start_time": "2023-01-21T05:27:14.625110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random, 1 of 5: 2 layers, 73 neurons, 2 layers, 33 neurons\n",
      "Epoch 1/10000\n",
      "1528/1528 - 35s - loss: 0.4496 - accuracy: 0.7830 - val_loss: 0.2238 - val_accuracy: 0.9129 - 35s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "1528/1528 - 30s - loss: 0.1407 - accuracy: 0.9523 - val_loss: 0.1548 - val_accuracy: 0.9440 - 30s/epoch - 20ms/step\n",
      "Epoch 3/10000\n",
      "1528/1528 - 31s - loss: 0.0699 - accuracy: 0.9791 - val_loss: 0.0596 - val_accuracy: 0.9809 - 31s/epoch - 20ms/step\n",
      "Epoch 4/10000\n",
      "1528/1528 - 30s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0428 - val_accuracy: 0.9874 - 30s/epoch - 20ms/step\n",
      "Epoch 5/10000\n",
      "1528/1528 - 32s - loss: 0.0344 - accuracy: 0.9901 - val_loss: 0.0589 - val_accuracy: 0.9853 - 32s/epoch - 21ms/step\n",
      "Epoch 6/10000\n",
      "1528/1528 - 30s - loss: 0.0273 - accuracy: 0.9926 - val_loss: 0.0297 - val_accuracy: 0.9890 - 30s/epoch - 20ms/step\n",
      "Epoch 7/10000\n",
      "1528/1528 - 30s - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0372 - val_accuracy: 0.9887 - 30s/epoch - 19ms/step\n",
      "Epoch 8/10000\n",
      "1528/1528 - 31s - loss: 0.0189 - accuracy: 0.9944 - val_loss: 0.0327 - val_accuracy: 0.9912 - 31s/epoch - 20ms/step\n",
      "Epoch 9/10000\n",
      "1528/1528 - 31s - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0310 - val_accuracy: 0.9920 - 31s/epoch - 20ms/step\n",
      "Epoch 10/10000\n",
      "1528/1528 - 30s - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.0992 - val_accuracy: 0.9761 - 30s/epoch - 19ms/step\n",
      "Epoch 11/10000\n",
      "1528/1528 - 30s - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0391 - val_accuracy: 0.9923 - 30s/epoch - 20ms/step\n",
      "Epoch 12/10000\n",
      "1528/1528 - 30s - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0294 - val_accuracy: 0.9925 - 30s/epoch - 19ms/step\n",
      "Epoch 13/10000\n",
      "1528/1528 - 31s - loss: 0.0110 - accuracy: 0.9972 - val_loss: 0.0247 - val_accuracy: 0.9943 - 31s/epoch - 20ms/step\n",
      "Epoch 14/10000\n",
      "1528/1528 - 31s - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0258 - val_accuracy: 0.9937 - 31s/epoch - 20ms/step\n",
      "Epoch 15/10000\n",
      "1528/1528 - 30s - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9868 - 30s/epoch - 20ms/step\n",
      "Epoch 16/10000\n",
      "1528/1528 - 32s - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 0.9894 - 32s/epoch - 21ms/step\n",
      "Epoch 17/10000\n",
      "1528/1528 - 31s - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0295 - val_accuracy: 0.9912 - 31s/epoch - 20ms/step\n",
      "Epoch 18/10000\n",
      "1528/1528 - 33s - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0266 - val_accuracy: 0.9925 - 33s/epoch - 21ms/step\n",
      "Epoch 19/10000\n",
      "1528/1528 - 32s - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0319 - val_accuracy: 0.9923 - 32s/epoch - 21ms/step\n",
      "Epoch 20/10000\n",
      "1528/1528 - 32s - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0287 - val_accuracy: 0.9933 - 32s/epoch - 21ms/step\n",
      "Epoch 21/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 53\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     56\u001b[0m time_elapse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((time_end \u001b[38;5;241m-\u001b[39m time_start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## hyper-parameter optimization\n",
    "model_type = 'RNN_without_window'\n",
    "\n",
    "valid_size = test_size/(1-test_size)\n",
    "patience = 30\n",
    "monitor = 'val_loss'\n",
    "random_state = random_state\n",
    "early_stopping_cb = EarlyStopping(patience=patience, restore_best_weights=True, monitor=monitor)\n",
    "\n",
    "parameter_config = {\n",
    "    \"rnn_layers\" : range(1,5),\n",
    "    \"rnn_neurons\" : range(1, 128),\n",
    "    \"dnn_layers\" : range(1,11),\n",
    "    \"dnn_neurons\" : range(1, 128),\n",
    "}\n",
    "\n",
    "method = \"random\"\n",
    "counts = 5\n",
    "metrics = ['time', 'rnn_layers', 'rnn_neurons', 'dnn_layers', 'dnn_neurons', \n",
    "           'loss', 'val_loss', 'test_loss', 'accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "\n",
    "train_x = upsample_x_list[0]\n",
    "train_y = upsample_y_list[0]\n",
    "test_x = test_x_list[0]\n",
    "test_y = test_y_list[0]\n",
    "\n",
    "hpo_result = pd.DataFrame([], columns=metrics)\n",
    "for i in range(counts):\n",
    "    random.seed(i+1)\n",
    "    rnn_layers = random.choice(parameter_config[\"rnn_layers\"])\n",
    "    rnn_neurons = random.choice(parameter_config[\"rnn_neurons\"])\n",
    "    dnn_layers = random.choice(parameter_config[\"dnn_layers\"])\n",
    "    dnn_neurons = random.choice(parameter_config[\"dnn_neurons\"])\n",
    "    print(f\"random, {i+1} of {counts}: {rnn_layers} layers, {rnn_neurons} neurons, {dnn_layers} layers, {dnn_neurons} neurons\")\n",
    "    \n",
    "    model_name = f'{version}_{model_type}_hpo_{i+1}of{counts}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    score_path = f\"./score/{model_name}.csv\"\n",
    "    \n",
    "    history_size = train_x.shape[1]\n",
    "    history_dim = train_x.shape[2]\n",
    "    y_dim = train_y.shape[1]\n",
    "    \n",
    "    model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                           rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                           dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "\n",
    "    if not exists(save_path) or update:\n",
    "        tf.random.set_seed(i+1)\n",
    "        \n",
    "        time_start = time.time()\n",
    "        history = model.fit(train_x, train_y, verbose=2,\n",
    "                            epochs=10000, callbacks=[early_stopping_cb], validation_split= valid_size)\n",
    "        time_end = time.time()\n",
    "        time_elapse = round((time_end - time_start)/60, 3)\n",
    "        \n",
    "        model.save_weights(save_path)\n",
    "        print(f\"model is saved to: {save_path}\")\n",
    "        \n",
    "        idx = np.array(history.history[monitor]).argmin()\n",
    "        val_loss = history.history['val_loss'][idx]\n",
    "        loss = history.history['loss'][idx]\n",
    "        test_loss = model.evaluate(test_x, test_y, verbose=0)[0]\n",
    "        prediction = model.predict(test_x, verbose=0)\n",
    "        prediction = prediction.round(0).astype(int)\n",
    "        y_real = test_y\n",
    "        y_pred = prediction\n",
    "        accuracy, precision, recall, f1 = scores(y_real, y_pred)\n",
    "        scores_df = pd.DataFrame([[time_elapse, rnn_layers, rnn_neurons, dnn_layers, dnn_neurons, \n",
    "                                   loss, val_loss, test_loss, accuracy, precision, recall, f1]], \n",
    "                                  columns=metrics)\n",
    "\n",
    "        scores_df.to_csv(score_path)\n",
    "        print(f\"history is saved to: {score_path}\")\n",
    "\n",
    "    else:\n",
    "        scores_df = pd.read_csv(score_path, index_col=0, header=0)\n",
    "        print(f\"history is loaded from: {score_path}\")\n",
    "        \n",
    "    hpo_result = pd.concat([hpo_result, scores_df], axis=0)\n",
    "\n",
    "hpo_result = hpo_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88082fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:37:50.975928Z",
     "start_time": "2023-01-21T05:37:50.975928Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the HPO result\n",
    "target_metric = 'f1'\n",
    "best_idx = hpo_result[target_metric].argmax()\n",
    "best_parameters = hpo_result.iloc[best_idx]\n",
    "print(f'best hyperparamerter: index {best_idx}')\n",
    "display(best_parameters)\n",
    "\n",
    "display(hpo_result.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "44c5c1b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T23:15:35.329643Z",
     "start_time": "2023-01-19T22:50:24.243664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_1of10.h5\n",
      "2th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_2of10.h5\n",
      "3th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_3of10.h5\n",
      "4th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_4of10.h5\n",
      "5th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_5of10.h5\n",
      "6th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_6of10.h5\n",
      "7th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_7of10.h5\n",
      "8th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_8of10.h5\n",
      "9th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_9of10.h5\n",
      "10th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_10of10.h5\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_input (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               1700      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,001\n",
      "Trainable params: 22,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bulid model\n",
    "rnn_layers = best_parameters['rnn_layers']\n",
    "rnn_neurons = best_parameters['rnn_neurons']\n",
    "dnn_layers = best_parameters['dnn_layers']\n",
    "dnn_neurons = best_parameters['dnn_neurons']\n",
    "\n",
    "i=1\n",
    "for train_x, train_y in zip(upsample_x_list, upsample_y_list):\n",
    "    print(f\"{i}th iteration\")\n",
    "    model_name = f'{version}_{model_type}_{rnn_layers}_{rnn_neurons}_{dnn_layers}_{dnn_neurons}_cv_{i}of{n_splits}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    \n",
    "    if not exists(save_path) or update:\n",
    "        history_size = train_x.shape[1]\n",
    "        history_dim = train_x.shape[2]\n",
    "        y_dim = train_y.shape[1]\n",
    "        model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                               rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                               dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "\n",
    "        if not exists(save_path) or update:\n",
    "            tf.random.set_seed(random_state)\n",
    "            history = model.fit(train_x, train_y, verbose=0,\n",
    "                                epochs=10000, callbacks=[early_stopping_cb], validation_split= valid_size)\n",
    "            model.save_weights(save_path)\n",
    "            print(f\"model is saved to: {save_path}\")\n",
    "    else:\n",
    "        print(f\"model already exists at: {save_path}\")\n",
    "    i += 1\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "04ff491b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T23:49:03.444703Z",
     "start_time": "2023-01-19T23:49:01.044595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>59.403000</td>\n",
       "      <td>1.831000</td>\n",
       "      <td>59.232000</td>\n",
       "      <td>3.54800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.337037</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>4.033889</td>\n",
       "      <td>0.25516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>57.620000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>53.850000</td>\n",
       "      <td>3.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58.360000</td>\n",
       "      <td>1.712500</td>\n",
       "      <td>56.970000</td>\n",
       "      <td>3.31500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>59.465000</td>\n",
       "      <td>1.825000</td>\n",
       "      <td>58.655000</td>\n",
       "      <td>3.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.885000</td>\n",
       "      <td>1.907500</td>\n",
       "      <td>60.340000</td>\n",
       "      <td>3.69500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.200000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>68.270000</td>\n",
       "      <td>4.01000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy  precision     recall  f1-score\n",
       "count  10.000000  10.000000  10.000000  10.00000\n",
       "mean   59.403000   1.831000  59.232000   3.54800\n",
       "std     1.337037   0.131356   4.033889   0.25516\n",
       "min    57.620000   1.680000  53.850000   3.25000\n",
       "25%    58.360000   1.712500  56.970000   3.31500\n",
       "50%    59.465000   1.825000  58.655000   3.54000\n",
       "75%    59.885000   1.907500  60.340000   3.69500\n",
       "max    62.200000   2.070000  68.270000   4.01000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the trained model\n",
    "i=1\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "for test_x, test_y in zip(test_x_list, test_y_list):\n",
    "    model_name = f'{version}_{model_type}_{rnn_layers}_{rnn_neurons}_{dnn_layers}_{dnn_neurons}_cv_{i}of{n_splits}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    \n",
    "    model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                               rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                               dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "    model.load_weights(save_path)\n",
    "    \n",
    "    prediction = model.predict(test_x, verbose=0)\n",
    "    prediction = prediction.round(0).astype(int)\n",
    "\n",
    "    y_real = test_y\n",
    "    y_pred = prediction\n",
    "    \n",
    "    accuracy  = 100*np.array(accuracy_score(y_real, y_pred)).round(4)\n",
    "    precision = 100*np.array(precision_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    recall    = 100*np.array(recall_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    f1        = 100*np.array(f1_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    \n",
    "accuracies = np.array(accuracy_list)\n",
    "precisions = np.array(precision_list)\n",
    "recalls = np.array(recall_list)\n",
    "f1s = np.array(f1_list)\n",
    "\n",
    "results = pd.DataFrame(np.array([accuracies, precisions, recalls, f1s]).T, columns=['accuracy', 'precision', 'recall', 'f1-score'])\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a98239",
   "metadata": {},
   "source": [
    "## case 1: with window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11b304ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:37:55.908351Z",
     "start_time": "2023-01-21T05:37:55.634191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257578, 106)\n",
      "(257578, 1)\n",
      "\n",
      "x columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                Proline\n",
       "1            flexibility\n",
       "2                  SEQ_A\n",
       "3                  SEQ_C\n",
       "4                  SEQ_D\n",
       "             ...        \n",
       "101          side_5_long\n",
       "102        side_5_normal\n",
       "103           side_5_pro\n",
       "104         side_5_small\n",
       "105    side_5_very_small\n",
       "Length: 106, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cat = ['SEQ', 'nS/nT', 'nAli', 'nPos', 'phi_psi', 'SS', \n",
    "         'side_-1', 'side_1', 'side_2', 'side_3','side_4', 'side_5']\n",
    "x_cts = ['Proline', 'flexibility']\n",
    "y_label = ['positivity']\n",
    "\n",
    "data_x = pd.get_dummies(dataset[x_cts+x_cat], columns=x_cat)\n",
    "data_y = dataset[y_label]\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)\n",
    "\n",
    "print(\"\\nx columns:\")\n",
    "display(pd.Series(data_x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8421038e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:38:17.698527Z",
     "start_time": "2023-01-21T05:37:58.850251Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = data_x\n",
    "output_data = data_y\n",
    "\n",
    "protein_list = list(dataset.protein.unique())\n",
    "rnn_input = []\n",
    "rnn_output = []\n",
    "for name in protein_list:\n",
    "    data = dataset[dataset['protein']==name]\n",
    "    low_bound = data.index[0]\n",
    "    up_bound  = data.index[-1]\n",
    "    ST_idx = np.where((data['SEQ']=='S')|(data['SEQ']=='T'))[0] + low_bound\n",
    "    \n",
    "    for idx in ST_idx:\n",
    "        start_idx = idx - window_size\n",
    "        end_idx   = idx + window_size + 1\n",
    "        \n",
    "#         print(f\"{name}, {low_bound}, {up_bound}, {start_idx}, {end_idx}\")\n",
    "        if start_idx < low_bound:\n",
    "            zeros = np.zeros((low_bound-start_idx,input_data.shape[1]))\n",
    "            temp  = input_data.iloc[low_bound:end_idx].values\n",
    "            temp  = np.concatenate([zeros, temp], axis=0)\n",
    "            \n",
    "        elif end_idx > up_bound + 1:\n",
    "            zeros = np.zeros((end_idx-up_bound-1,input_data.shape[1]))\n",
    "            temp  = input_data.iloc[start_idx:up_bound+1].values\n",
    "            temp  = np.concatenate([temp, zeros], axis=0)\n",
    "            \n",
    "        else:\n",
    "            temp  = input_data.iloc[start_idx:end_idx].values\n",
    "            \n",
    "        rnn_input.append(temp)\n",
    "        rnn_output.append(output_data.iloc[idx].values)\n",
    "        \n",
    "rnn_input = np.array(rnn_input)\n",
    "rnn_output = np.array(rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15c71bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:04:39.108405Z",
     "start_time": "2023-01-21T05:04:36.370406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "2th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.001001001001001\n",
      "3th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "4th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "5th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0006653359946773\n",
      "6th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "7th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "8th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0090634441087611\n",
      "9th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n",
      "10th iteration\n",
      "train: (33011, 21, 34) (33011, 1) check scale: 0.0 1.0\n",
      "test:  (8253, 21, 34) (8253, 1) check scale: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "### split data into train/test dataset ###\n",
    "split = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "arr_x, arr_y = rnn_input, rnn_output # convert dataframe to nd-array \n",
    "\n",
    "i=1\n",
    "train_idx_list, train_x_list, train_y_list, test_idx_list, test_x_list, test_y_list = [], [], [], [], [], []\n",
    "for train_index, test_index in split.split(arr_x, arr_y):\n",
    "    train_x = arr_x[train_index]\n",
    "    train_y = arr_y[train_index]\n",
    "    test_x = arr_x[test_index]\n",
    "    test_y = arr_y[test_index]\n",
    "    \n",
    "    train_cts = train_x[:,:,:len(x_cts)]\n",
    "    test_cts  = test_x[:,:,:len(x_cts)]\n",
    "    \n",
    "    x_min = train_cts.min(0).min(0)\n",
    "    x_max = train_cts.max(0).max(0)\n",
    "    \n",
    "    train_x[:,:,:len(x_cts)] = (train_cts-x_min)/(x_max-x_min)\n",
    "    test_x[:,:,:len(x_cts)] = (test_cts-x_min)/(x_max-x_min)\n",
    "    \n",
    "    print(f\"{i}th iteration\")\n",
    "    print(\"train:\", train_x.shape, train_y.shape, \"check scale:\", train_x.min(), train_x.max())\n",
    "    print(\"test: \", test_x.shape, test_y.shape, \"check scale:\", test_x.min(), test_x.max())\n",
    "    \n",
    "    train_idx_list.append(train_index)\n",
    "    train_x_list.append(train_x)\n",
    "    train_y_list.append(train_y)\n",
    "    \n",
    "    test_idx_list.append(test_index)\n",
    "    test_x_list.append(test_x)\n",
    "    test_y_list.append(test_y)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c2e3498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-21T05:04:54.069081Z",
     "start_time": "2023-01-21T05:04:50.100081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up-sampled train dataset: (65188, 21, 34) (65188, 1)\n",
      "test dataset: (8253, 21, 34) (8253, 1)\n"
     ]
    }
   ],
   "source": [
    "## upsampling dataset \n",
    "random_state = random_state\n",
    "\n",
    "upsample_x_list, upsample_y_list = [], []\n",
    "for train_x, train_y in zip(train_x_list, train_y_list):\n",
    "    index_pos = np.where(train_y == 1)[0]\n",
    "    index_neg = np.where(train_y == 0)[0]\n",
    "\n",
    "    random.seed(random_state)\n",
    "    up_index = [random.choice(index_pos) for _ in range(len(index_neg))] # get samples from positive sites as much as the number of negative sites\n",
    "\n",
    "    upsample_pos_x = train_x[up_index]\n",
    "    upsample_pos_y = train_y[up_index]\n",
    "    sample_neg_x = train_x[index_neg]\n",
    "    sample_neg_y = train_y[index_neg]\n",
    "\n",
    "    sample_x = np.concatenate([upsample_pos_x, sample_neg_x], axis=0)\n",
    "    sample_y = np.concatenate([upsample_pos_y, sample_neg_y], axis=0)\n",
    "\n",
    "    shuffle_index = np.arange(len(sample_x))\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    sample_x = sample_x[shuffle_index]\n",
    "    sample_y = sample_y[shuffle_index]\n",
    "    \n",
    "    upsample_x_list.append(sample_x)\n",
    "    upsample_y_list.append(sample_y)\n",
    "\n",
    "print(\"up-sampled train dataset:\", sample_x.shape, sample_y.shape)\n",
    "print(\"test dataset:\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b215712",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-21T05:27:14.622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random, 1 of 5: 2 layers, 73 neurons, 2 layers, 33 neurons\n",
      "Epoch 1/10000\n",
      "1528/1528 - 35s - loss: 0.4496 - accuracy: 0.7830 - val_loss: 0.2238 - val_accuracy: 0.9129 - 35s/epoch - 23ms/step\n",
      "Epoch 2/10000\n",
      "1528/1528 - 30s - loss: 0.1407 - accuracy: 0.9523 - val_loss: 0.1548 - val_accuracy: 0.9440 - 30s/epoch - 20ms/step\n",
      "Epoch 3/10000\n",
      "1528/1528 - 31s - loss: 0.0699 - accuracy: 0.9791 - val_loss: 0.0596 - val_accuracy: 0.9809 - 31s/epoch - 20ms/step\n",
      "Epoch 4/10000\n",
      "1528/1528 - 30s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0428 - val_accuracy: 0.9874 - 30s/epoch - 20ms/step\n",
      "Epoch 5/10000\n",
      "1528/1528 - 32s - loss: 0.0344 - accuracy: 0.9901 - val_loss: 0.0589 - val_accuracy: 0.9853 - 32s/epoch - 21ms/step\n",
      "Epoch 6/10000\n",
      "1528/1528 - 30s - loss: 0.0273 - accuracy: 0.9926 - val_loss: 0.0297 - val_accuracy: 0.9890 - 30s/epoch - 20ms/step\n",
      "Epoch 7/10000\n",
      "1528/1528 - 30s - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.0372 - val_accuracy: 0.9887 - 30s/epoch - 19ms/step\n",
      "Epoch 8/10000\n",
      "1528/1528 - 31s - loss: 0.0189 - accuracy: 0.9944 - val_loss: 0.0327 - val_accuracy: 0.9912 - 31s/epoch - 20ms/step\n",
      "Epoch 9/10000\n",
      "1528/1528 - 31s - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0310 - val_accuracy: 0.9920 - 31s/epoch - 20ms/step\n",
      "Epoch 10/10000\n",
      "1528/1528 - 30s - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.0992 - val_accuracy: 0.9761 - 30s/epoch - 19ms/step\n",
      "Epoch 11/10000\n",
      "1528/1528 - 30s - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0391 - val_accuracy: 0.9923 - 30s/epoch - 20ms/step\n",
      "Epoch 12/10000\n",
      "1528/1528 - 30s - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0294 - val_accuracy: 0.9925 - 30s/epoch - 19ms/step\n",
      "Epoch 13/10000\n",
      "1528/1528 - 31s - loss: 0.0110 - accuracy: 0.9972 - val_loss: 0.0247 - val_accuracy: 0.9943 - 31s/epoch - 20ms/step\n",
      "Epoch 14/10000\n",
      "1528/1528 - 31s - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0258 - val_accuracy: 0.9937 - 31s/epoch - 20ms/step\n",
      "Epoch 15/10000\n",
      "1528/1528 - 30s - loss: 0.0070 - accuracy: 0.9981 - val_loss: 0.0496 - val_accuracy: 0.9868 - 30s/epoch - 20ms/step\n",
      "Epoch 16/10000\n",
      "1528/1528 - 32s - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 0.9894 - 32s/epoch - 21ms/step\n",
      "Epoch 17/10000\n"
     ]
    }
   ],
   "source": [
    "## hyper-parameter optimization\n",
    "model_type = 'RNN_without_window'\n",
    "\n",
    "train_x = upsample_x_list[0]\n",
    "train_y = upsample_y_list[0]\n",
    "test_x = test_x_list[0]\n",
    "test_y = test_y_list[0]\n",
    "\n",
    "hpo_result = pd.DataFrame([], columns=metrics)\n",
    "for i in range(counts):\n",
    "    random.seed(i+1)\n",
    "    rnn_layers = random.choice(parameter_config[\"rnn_layers\"])\n",
    "    rnn_neurons = random.choice(parameter_config[\"rnn_neurons\"])\n",
    "    dnn_layers = random.choice(parameter_config[\"dnn_layers\"])\n",
    "    dnn_neurons = random.choice(parameter_config[\"dnn_neurons\"])\n",
    "    print(f\"random, {i+1} of {counts}: {rnn_layers} layers, {rnn_neurons} neurons, {dnn_layers} layers, {dnn_neurons} neurons\")\n",
    "    \n",
    "    model_name = f'{version}_{model_type}_hpo_{i+1}of{counts}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    score_path = f\"./score/{model_name}.csv\"\n",
    "    \n",
    "    history_size = train_x.shape[1]\n",
    "    history_dim = train_x.shape[2]\n",
    "    y_dim = train_y.shape[1]\n",
    "    \n",
    "    model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                           rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                           dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "\n",
    "    if not exists(save_path) or update:\n",
    "        tf.random.set_seed(i+1)\n",
    "        \n",
    "        time_start = time.time()\n",
    "        history = model.fit(train_x, train_y, verbose=2,\n",
    "                            epochs=10000, callbacks=[early_stopping_cb], validation_split= valid_size)\n",
    "        time_end = time.time()\n",
    "        time_elapse = round((time_end - time_start)/60, 3)\n",
    "        \n",
    "        model.save_weights(save_path)\n",
    "        print(f\"model is saved to: {save_path}\")\n",
    "        \n",
    "        idx = np.array(history.history[monitor]).argmin()\n",
    "        val_loss = history.history['val_loss'][idx]\n",
    "        loss = history.history['loss'][idx]\n",
    "        test_loss = model.evaluate(test_x, test_y, verbose=0)[0]\n",
    "        prediction = model.predict(test_x, verbose=0)\n",
    "        prediction = prediction.round(0).astype(int)\n",
    "        y_real = test_y\n",
    "        y_pred = prediction\n",
    "        accuracy, precision, recall, f1 = scores(y_real, y_pred)\n",
    "        scores_df = pd.DataFrame([[time_elapse, rnn_layers, rnn_neurons, dnn_layers, dnn_neurons, \n",
    "                                   loss, val_loss, test_loss, accuracy, precision, recall, f1]], \n",
    "                                  columns=metrics)\n",
    "\n",
    "        scores_df.to_csv(score_path)\n",
    "        print(f\"history is saved to: {score_path}\")\n",
    "\n",
    "    else:\n",
    "        scores_df = pd.read_csv(score_path, index_col=0, header=0)\n",
    "        print(f\"history is loaded from: {score_path}\")\n",
    "        \n",
    "    hpo_result = pd.concat([hpo_result, scores_df], axis=0)\n",
    "\n",
    "hpo_result = hpo_result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c571e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-21T05:27:17.746Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the HPO result\n",
    "target_metric = 'f1'\n",
    "best_idx = hpo_result[target_metric].argmax()\n",
    "best_parameters = hpo_result.iloc[best_idx]\n",
    "print(f'best hyperparamerter: index {best_idx}')\n",
    "display(best_parameters)\n",
    "\n",
    "display(hpo_result.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d31965ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T23:15:35.329643Z",
     "start_time": "2023-01-19T22:50:24.243664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_1of10.h5\n",
      "2th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_2of10.h5\n",
      "3th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_3of10.h5\n",
      "4th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_4of10.h5\n",
      "5th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_5of10.h5\n",
      "6th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_6of10.h5\n",
      "7th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_7of10.h5\n",
      "8th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_8of10.h5\n",
      "9th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_9of10.h5\n",
      "10th iteration\n",
      "model is saved to: ./model/v4_upsampled_MLP_without_window_3_100_cv_10of10.h5\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_input (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               1700      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,001\n",
      "Trainable params: 22,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bulid model\n",
    "rnn_layers = best_parameters['rnn_layers']\n",
    "rnn_neurons = best_parameters['rnn_neurons']\n",
    "dnn_layers = best_parameters['dnn_layers']\n",
    "dnn_neurons = best_parameters['dnn_neurons']\n",
    "\n",
    "i=1\n",
    "for train_x, train_y in zip(upsample_x_list, upsample_y_list):\n",
    "    print(f\"{i}th iteration\")\n",
    "    model_name = f'{version}_{model_type}_{rnn_layers}_{rnn_neurons}_{dnn_layers}_{dnn_neurons}_cv_{i}of{n_splits}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    \n",
    "    if not exists(save_path) or update:\n",
    "        history_size = train_x.shape[1]\n",
    "        history_dim = train_x.shape[2]\n",
    "        y_dim = train_y.shape[1]\n",
    "        model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                               rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                               dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "\n",
    "        if not exists(save_path) or update:\n",
    "            tf.random.set_seed(random_state)\n",
    "            history = model.fit(train_x, train_y, verbose=0,\n",
    "                                epochs=10000, callbacks=[early_stopping_cb], validation_split= valid_size)\n",
    "            model.save_weights(save_path)\n",
    "            print(f\"model is saved to: {save_path}\")\n",
    "    else:\n",
    "        print(f\"model already exists at: {save_path}\")\n",
    "    i += 1\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "473d2f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-19T23:49:03.444703Z",
     "start_time": "2023-01-19T23:49:01.044595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>59.403000</td>\n",
       "      <td>1.831000</td>\n",
       "      <td>59.232000</td>\n",
       "      <td>3.54800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.337037</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>4.033889</td>\n",
       "      <td>0.25516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>57.620000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>53.850000</td>\n",
       "      <td>3.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58.360000</td>\n",
       "      <td>1.712500</td>\n",
       "      <td>56.970000</td>\n",
       "      <td>3.31500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>59.465000</td>\n",
       "      <td>1.825000</td>\n",
       "      <td>58.655000</td>\n",
       "      <td>3.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.885000</td>\n",
       "      <td>1.907500</td>\n",
       "      <td>60.340000</td>\n",
       "      <td>3.69500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.200000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>68.270000</td>\n",
       "      <td>4.01000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy  precision     recall  f1-score\n",
       "count  10.000000  10.000000  10.000000  10.00000\n",
       "mean   59.403000   1.831000  59.232000   3.54800\n",
       "std     1.337037   0.131356   4.033889   0.25516\n",
       "min    57.620000   1.680000  53.850000   3.25000\n",
       "25%    58.360000   1.712500  56.970000   3.31500\n",
       "50%    59.465000   1.825000  58.655000   3.54000\n",
       "75%    59.885000   1.907500  60.340000   3.69500\n",
       "max    62.200000   2.070000  68.270000   4.01000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the trained model\n",
    "i=1\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "for test_x, test_y in zip(test_x_list, test_y_list):\n",
    "    model_name = f'{version}_{model_type}_{rnn_layers}_{rnn_neurons}_{dnn_layers}_{dnn_neurons}_cv_{i}of{n_splits}'\n",
    "    save_path  = f'./model/{model_name}.h5'\n",
    "    \n",
    "    model = ClassifierLSTM(history_size, history_dim, y_dim,\n",
    "                               rnn_layers = rnn_layers, rnn_neurons = rnn_neurons,\n",
    "                               dense_layers = dnn_layers, dense_neurons = dnn_neurons) \n",
    "    model.load_weights(save_path)\n",
    "    \n",
    "    prediction = model.predict(test_x, verbose=0)\n",
    "    prediction = prediction.round(0).astype(int)\n",
    "\n",
    "    y_real = test_y\n",
    "    y_pred = prediction\n",
    "    \n",
    "    accuracy  = 100*np.array(accuracy_score(y_real, y_pred)).round(4)\n",
    "    precision = 100*np.array(precision_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    recall    = 100*np.array(recall_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    f1        = 100*np.array(f1_score(y_real, y_pred, average=None)).round(4)[1]\n",
    "    \n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    \n",
    "accuracies = np.array(accuracy_list)\n",
    "precisions = np.array(precision_list)\n",
    "recalls = np.array(recall_list)\n",
    "f1s = np.array(f1_list)\n",
    "\n",
    "results = pd.DataFrame(np.array([accuracies, precisions, recalls, f1s]).T, columns=['accuracy', 'precision', 'recall', 'f1-score'])\n",
    "results.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
